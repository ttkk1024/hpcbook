# 第7章 高性能网络

## 7.1 InfiniBand技术

### InfiniBand基础概念

#### InfiniBand架构概述

**InfiniBand技术特点**
```
┌─────────────────────────────────────────────────────────────┐
│                      InfiniBand架构                         │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐        │
│  │  Host   │  │   Switch│  │  Host   │  │   Switch│        │
│  │ Channel │  │         │  │ Channel │  │         │        │
│  │ Adapter│  │         │  │ Adapter│  │         │        │
│  │  (HCA)  │  │         │  │  (HCA)  │  │         │        │
│  └─────────┘  └─────────┘  └─────────┘  └─────────┘        │
│         │            │            │            │           │
│         └────────────┼────────────┘            │           │
│                      │                         │           │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐        │
│  │   Switch│  │  Host   │  │   Switch│  │  Host   │        │
│  │         │  │ Channel │  │         │  │ Channel │        │
│  │         │  │ Adapter│  │         │  │ Adapter│        │
│  │         │  │  (HCA)  │  │         │  │  (HCA)  │        │
│  └─────────┘  └─────────┘  └─────────┘  └─────────┘        │
└─────────────────────────────────────────────────────────────┘
```

**InfiniBand核心组件**
- **HCA (Host Channel Adapter)**：主机通道适配器，连接主机和InfiniBand网络
- **Switch**：交换机，提供多端口连接和数据转发
- **Subnet Manager**：子网管理器，负责网络配置和管理
- **Cabling**：连接线缆，支持不同速率和距离

#### InfiniBand协议栈

**协议层次结构**
```
┌─────────────────────────────────────┐
│           Application Layer         │
│  MPI, SHMEM, OpenSHMEM, etc.        │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│           Transport Layer           │
│  Reliable Connection (RC)           │
│  Unreliable Connection (UC)         │
│  Unreliable Datagram (UD)           │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│           Network Layer             │
│  Global Routing Header (GRH)        │
│  Local Routing Header (LRH)         │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│           Link Layer                │
│  Flow Control                       │
│  Error Detection & Correction       │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│           Physical Layer            │
│  Electrical Signaling               │
│  Optical Signaling                  │
└─────────────────────────────────────┘
```

### InfiniBand版本与性能

#### 传输速率演进

**InfiniBand速率标准**
```bash
# 传输速率对照表 (Lane rate x Lanes)
FDR  (Fourteen Data Rate):  56.0 Gbps (14G x 4)
EDR  (Enhanced Data Rate): 100.0 Gbps (25G x 4)
HDR  (High Data Rate):     200.0 Gbps (50G x 4)
NDR  (Next Data Rate):     400.0 Gbps (100G x 4)
XDR  (eXtreme Data Rate):  800.0 Gbps (200G x 4)
GDR  (Global Data Rate):   1.6 Tbps   (200G x 8 / 400G x 4) [Future]
```

**实际带宽计算**
```bash
# 编码效率: EDR及以后采用 64b/66b 编码 (效率 ~97%)
HDR: 200 Gbps * 0.97 = 194 Gbps (~24.2 GB/s)
NDR: 400 Gbps * 0.97 = 388 Gbps (~48.5 GB/s)
XDR: 800 Gbps * 0.97 = 776 Gbps (~97.0 GB/s)
```

#### 延迟特性

**InfiniBand延迟优势**
```bash
# 典型延迟对比 (Host-to-Host MPC)
Ethernet RoCEv2 (400GbE): ~1.5 - 2.0 μs
InfiniBand HDR:           ~0.5 - 0.6 μs
InfiniBand NDR:           ~0.4 - 0.5 μs (Switch Latency < 100ns)

# AI集群关键指标
- 尾延迟 (Tail Latency): 99th percentile latency 极其关键
- In-Network Computing (SHARP): 减少全局集合通信延迟
```

### InfiniBand配置与管理

#### OFED驱动安装

**安装Mellanox OFED**
```bash
# 下载OFED驱动
wget https://www.mellanox.com/downloads/ofed/MLNX_OFED-5.8-3.0.7.0/MLNX_OFED_LINUX-5.8-3.0.7.0-rhel9.0-x86_64.tgz
tar -xzf MLNX_OFED_LINUX-5.8-3.0.7.0-rhel9.0-x86_64.tgz
cd MLNX_OFED_LINUX-5.8-3.0.7.0-rhel9.0-x86_64

# 安装OFED
./mlnxofedinstall --all
./install.pl --force

# 重启系统
reboot
```

**验证驱动安装**
```bash
# 检查InfiniBand设备
ibstat
ibv_devinfo

# 检查驱动版本
modinfo mlx5_core
modinfo ib_ipoib

# 检查设备状态
lspci | grep -i mellanox
lsmod | grep ib
```

#### 网络配置

**IPoIB配置**
```bash
# 1. 配置网络接口
# /etc/sysconfig/network-scripts/ifcfg-ib0
DEVICE=ib0
TYPE=InfiniBand
ONBOOT=yes
BOOTPROTO=static
IPADDR=10.1.1.10
NETMASK=255.255.255.0
MTU=65520
NM_CONTROLLED=no

# 2. 启动网络服务
systemctl restart network
systemctl enable rdma

# 3. 验证连接
ibstat
ping -c 3 10.1.1.11
```

**子网管理器配置**
```bash
# 启动子网管理器
opensm -g 0x8000

# 配置子网管理器
opensm -g 0x8000 -p 1 -D 1

# 查看子网状态
opensm -V

# 配置文件示例
# /etc/opensm/opensm.conf
force_guid_collisions=0
policy_file=/etc/opensm/partitions.conf
```

#### 性能调优

**NVIDIA/Mellanox 常用工具**
```bash
# 1. 查询 HCA 信息
ibv_devinfo -v
mlxconfig -d /dev/mst/mt4129_pciconf0 q   # 查询固件配置

# 2. 链路层调优 (Link Level)
# 开启自适应路由 (Adaptive Routing) - 需子网管理器配合
# 调整 Service Level (SL) 和 Virtual Lane (VL) 映射

# 3. 传输层调优 (QP/Context)
# 增加 Queue Pair 大小
export MLX5_QP_SIZE=4096

# 4. 开启锐利计算 (SHARP)
# 确认 OpenMPI/NCCL 已编译 SHARP 支持
mpirun -x MELLANOX_SHARP_HOST_LIST=...
```

### InfiniBand性能测试

#### 基准测试工具

**IB测试工具集**
```bash
# 1. 基本连通性测试
ibping -c 10 -S 1 remote-host

# 2. 带宽测试
ib_send_bw -d ib0 -F -x 1 remote-host
ib_write_bw -d ib0 -F -x 1 remote-host
ib_read_bw -d ib0 -F -x 1 remote-host

# 3. 延迟测试
ib_send_lat -d ib0 -F -x 1 remote-host
ib_write_lat -d ib0 -F -x 1 remote-host
ib_read_lat -d ib0 -F -x 1 remote-host

# 4. 多队列测试
ib_send_bw -d ib0 -F -x 1 -q 16 remote-host
```

**性能分析工具**
```bash
# 1. 网络统计
cat /sys/class/infiniband/ib0/ports/1/counters/port_rcv_packets
cat /sys/class/infiniband/ib0/ports/1/counters/port_xmit_packets

# 2. 错误统计
cat /sys/class/infiniband/ib0/ports/1/counters/port_rcv_errors
cat /sys/class/infiniband/ib0/ports/1/counters/port_xmit_constraint_errors

# 3. 性能监控
sar -n DEV 1
iftop -i ib0
```

#### MPI性能测试

**OSU微基准测试**
```bash
# 安装OSU Micro-Benchmarks
wget http://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-5.8.tar.gz
tar -xzf osu-micro-benchmarks-5.8.tar.gz
cd osu-micro-benchmarks-5.8
./configure --prefix=/opt/osu --with-mpi=/opt/openmpi
make && make install

# 运行延迟测试
mpirun -np 2 -host node1,node2 /opt/osu/libexec/osu-micro-benchmarks/mpi/pt2pt/osu_latency

# 运行带宽测试
mpirun -np 2 -host node1,node2 /opt/osu/libexec/osu-micro-benchmarks/mpi/pt2pt/osu_bw

# 运行单向延迟测试
mpirun -np 2 -host node1,node2 /opt/osu/libexec/osu-micro-benchmarks/mpi/pt2pt/osu_bibw
```

**HPL性能测试**
```bash
# 编译HPL with InfiniBand support
cd hpl-2.3
cp setup/Make.Linux_INTEL64 ~/HPL-2.3/Make.intel
make arch=intel

# 配置HPL.dat
# 使用InfiniBand优化参数
cat > HPL.dat << EOF
HPLinpack benchmark input file
InfiniBand optimized settings
1                        # This line: NBs (number of problems)
24                       # This line: NB (block size)
0                        # This line: P (number of process rows)
0                        # This line: Q (number of process columns)
16.0                     # This line: threshold
1                        # This line: number of panel fact
0                        # This line: PFACT (0=left, 1=Crout, 2=Right)
0                        # This line: NBMIN (minimum block size)
1                        # This line: NDIV (number of recursive steps)
2                        # This line: RFACT (recursive panel factorization)
0                        # This line: BCAST (broadcast method)
0                        # This line: DEPTH (depth of recursion)
0                        # This line: SWAP (0=bin-exchange, 1=long, 2=mix)
64                       # This line: L1 (0=transposed, 1=no-transposed)
0                        # This line: U (0=transposed, 1=no-transposed)
1                        # This line: EQUIL (0=no, 1=yes)
8                        # This line: ALIGN (memory alignment)
EOF

# 运行HPL测试
mpirun -np 64 -hostfile hosts ./xhpl
```

## 7.2 以太网优化

### 高速以太网技术

#### 以太网演进

**以太网速率发展**
```bash
# 以太网速率标准 (单通道/多通道)
10G/25G:     接入层基础速率
100G (4x25G): 广泛使用
200G (4x56G PAM4): 高性能
400G (4x100G PAM4 / 8x56G): AI/HPC 主流
800G (8x100G PAM4): 下一代 AI 集群标准 (2024-2025)
1.6T: 早期部署阶段
```

**UEC (Ultra Ethernet Consortium)**
鉴于传统 TCP/IP 在 HPC/AI 场景的局限性，UEC 致力于定义下一代以太网传输层，改进 RoCEv2 的拥塞控制和多路径传输能力。

**物理层技术**
```bash
# 光模块类型
QSFP56:  200G (HDR/200GbE)
OSFP / QSFP-DD: 400G/800G (NDR/800GbE)
- DR4/DR8: 500m (数据中心内)
- SR4/SR8: 100m (AOC/多模)
```

#### RDMA over Converged Ethernet (RoCE)

**RoCE技术概述**
```bash
# RoCE版本对比
RoCE v1: 基于以太网链路层，需要无损以太网
RoCE v2: 基于UDP/IP层，支持路由，更灵活

# RoCE优势
- 利用现有以太网基础设施
- 支持RDMA零拷贝技术
- 低延迟、高带宽
- 与InfiniBand兼容
```

**RoCE 配置 (基于 NVIDIA/Mellanox Cards)**
```bash
# 1. 优先级流控制 (PFC) - 必须在交换机和网卡同时配置
# 确保无损网络 (Lossless Network)
mlnx_qos -i eth0 --pfc 0,0,0,1,0,0,0,0  # 在优先级3开启PFC

# 2. ECN (Explicit Congestion Notification)
# 拥塞控制，配合 DCQCN
sysctl -w net.ipv4.tcp_ecn=1

# 3. 验证 RoCE 状态
show_gids
ibv_devinfo -v
```

### 以太网性能优化

#### 网络参数调优

**TCP参数优化**
```bash
# 1. 网络缓冲区优化
echo 'net.core.rmem_max = 134217728' >> /etc/sysctl.conf
echo 'net.core.wmem_max = 134217728' >> /etc/sysctl.conf
echo 'net.ipv4.tcp_rmem = 4096 87380 134217728' >> /etc/sysctl.conf
echo 'net.ipv4.tcp_wmem = 4096 65536 134217728' >> /etc/sysctl.conf

# 2. 连接队列优化
echo 'net.core.somaxconn = 65535' >> /etc/sysctl.conf
echo 'net.core.netdev_max_backlog = 5000' >> /etc/sysctl.conf
echo 'net.ipv4.tcp_max_syn_backlog = 65535' >> /etc/sysctl.conf

# 3. TCP窗口缩放
echo 'net.ipv4.tcp_window_scaling = 1' >> /etc/sysctl.conf
echo 'net.ipv4.tcp_adv_win_scale = 2' >> /etc/sysctl.conf

# 4. 拥塞控制算法
echo 'net.ipv4.tcp_congestion_control = htcp' >> /etc/sysctl.conf

# 5. 应用配置
sysctl -p
```

**网卡参数优化**
```bash
# 1. 禁用不必要的功能
ethtool -K eth0 gro off    # 禁用GRO
ethtool -K eth0 tso off   # 禁用TSO
ethtool -K eth0 lro off   # 禁用LRO
ethtool -K eth0 gso off   # 禁用GSO

# 2. 调整队列数量
ethtool -L eth0 combined 16

# 3. 配置RSS (Receive Side Scaling)
ethtool -x eth0 equal 16

# 4. 调整中断亲和性
for i in {0..15}; do
    irq=$(cat /proc/interrupts | grep eth0-TxRx-$i | awk '{print $1}' | tr -d ':')
    echo $(printf "%x" $((1 << i))) > /proc/irq/$irq/smp_affinity
done
```

#### 多路径网络配置

**链路聚合 (LACP)**
```bash
# 1. 配置网卡绑定
# /etc/sysconfig/network-scripts/ifcfg-bond0
DEVICE=bond0
TYPE=Bond
BONDING_MASTER=yes
BOOTPROTO=static
IPADDR=192.168.1.10
NETMASK=255.255.255.0
GATEWAY=192.168.1.1
ONBOOT=yes
BONDING_OPTS="mode=4 miimon=100 lacp_rate=fast"

# /etc/sysconfig/network-scripts/ifcfg-eth0
DEVICE=eth0
BOOTPROTO=none
ONBOOT=yes
MASTER=bond0
SLAVE=yes

# /etc/sysconfig/network-scripts/ifcfg-eth1
DEVICE=eth1
BOOTPROTO=none
ONBOOT=yes
MASTER=bond0
SLAVE=yes

# 2. 重启网络服务
systemctl restart network
```

**多网卡负载均衡**
```bash
# 1. 配置多个默认网关
ip route add default via 192.168.1.1 dev eth0 table 1
ip route add default via 192.168.2.1 dev eth1 table 2

# 2. 配置策略路由
ip rule add from 192.168.1.0/24 table 1
ip rule add from 192.168.2.0/24 table 2

# 3. 配置ECMP (Equal Cost Multi-Path)
ip route add 10.0.0.0/8 nexthop via 192.168.1.1 nexthop via 192.168.2.1
```

### 网络监控与故障排除

#### 网络性能监控

**带宽测试工具**
```bash
# 1. iperf3测试
# 服务端
iperf3 -s -p 5201

# 客户端
iperf3 -c server-ip -t 60 -P 16 -w 2M

# 2. ntttcp测试
# 发送端
ntttcp -s -t 60 -P 16

# 接收端
ntttcp -r -t 60 -P 16

# 3. qperf测试
qperf server-ip tcp_bw tcp_lat udp_bw udp_lat
```

**延迟测试工具**
```bash
# 1. ping测试
ping -c 100 -i 0.1 server-ip

# 2. hping3测试
hping3 -S -p 80 -c 100 server-ip

# 3. 网络质量测试
nping --tcp -p 22 --delay 100ms server-ip
```

#### 故障诊断工具

**网络连通性诊断**
```bash
# 1. 基本连通性检查
ping server-ip
traceroute server-ip
mtr server-ip

# 2. 端口状态检查
nmap -p 22,80,443 server-ip
telnet server-ip 22

# 3. 网络接口状态
ethtool eth0
ip link show
ip addr show
```

**性能瓶颈诊断**
```bash
# 1. 网络接口统计
cat /proc/net/dev
ethtool -S eth0

# 2. 系统网络统计
cat /proc/net/netstat
cat /proc/net/snmp

# 3. 连接状态分析
netstat -tuln
ss -tuln
lsof -i
```

## 7.3 网络监控与故障排除

### 网络监控系统

#### 监控指标体系

**关键性能指标 (KPIs)**
```bash
# 带宽相关
- 吞吐量 (Throughput): GB/s, MB/s
- 利用率 (Utilization): % of bandwidth used
- 饱和度 (Saturation): Queue depth, buffer usage

# 延迟相关
- 端到端延迟 (Latency): μs, ms
- 往返时间 (RTT): Round Trip Time
- 队列延迟 (Queue Delay): Buffer queuing time

# 可靠性相关
- 丢包率 (Packet Loss): %
- 重传率 (Retransmission): %
- 错误率 (Error Rate): %

# 连接相关
- 连接数 (Connections): Active connections
- 并发连接 (Concurrent): Simultaneous connections
- 连接建立时间 (Connection Setup): Time to establish
```

**监控工具配置**
```bash
# 1. 安装监控工具
yum install collectl nmon sysstat

# 2. 配置collectl监控网络
collectl -sN -i 10 -P

# 3. 配置nmon监控
nmon -F network.nmon -s 10 -c 360

# 4. 配置sar监控
sar -n DEV 10 360
```

#### 实时监控仪表板

**Grafana + Prometheus配置**
```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['localhost:9100']

rule_files:
  - "network_alerts.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093
```

**监控面板配置**
```json
{
  "dashboard": {
    "title": "Network Performance",
    "panels": [
      {
        "title": "Network Bandwidth",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(node_network_receive_bytes_total[5m])",
            "legendFormat": "RX"
          },
          {
            "expr": "rate(node_network_transmit_bytes_total[5m])",
            "legendFormat": "TX"
          }
        ]
      },
      {
        "title": "Network Packets",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(node_network_receive_packets_total[5m])",
            "legendFormat": "RX Packets"
          },
          {
            "expr": "rate(node_network_transmit_packets_total[5m])",
            "legendFormat": "TX Packets"
          }
        ]
      }
    ]
  }
}
```

### 故障排除流程

#### 系统化故障排除

**故障排除框架**
```bash
# 1. 问题识别
- 确定故障现象
- 评估影响范围
- 收集初步信息

# 2. 信息收集
- 系统日志分析
- 网络状态检查
- 性能指标收集

# 3. 问题定位
- 分层诊断 (物理层→链路层→网络层→传输层→应用层)
- 对比分析 (正常vs异常)
- 模式识别 (时间、频率、条件)

# 4. 解决方案实施
- 制定修复计划
- 实施修复措施
- 验证修复效果

# 5. 预防措施
- 根因分析
- 改进监控
- 更新文档
```

**常见故障类型及处理**
```bash
# 1. 连接失败
现象: ping不通，端口无法访问
检查步骤:
- 物理连接: 网线、光纤、端口状态
- IP配置: IP地址、子网掩码、网关
- 路由配置: 路由表、默认网关
- 防火墙: iptables、firewalld规则

# 2. 性能下降
现象: 带宽不足、延迟高、丢包
检查步骤:
- 网络拥塞: 带宽利用率、队列深度
- 硬件故障: 网卡、交换机、线缆
- 配置问题: MTU、缓冲区、QoS
- 软件问题: 驱动、协议栈参数

# 3. 不稳定连接
现象: 间歇性断连、重连
检查步骤:
- 硬件状态: 温度、错误计数
- 网络环境: 电磁干扰、物理损坏
- 配置一致性: 端口协商、双工模式
- 协议问题: ARP、STP、路由震荡
```

#### 高级诊断技术

**网络抓包分析**
```bash
# 1. 基本抓包
tcpdump -i eth0 -w capture.pcap
tcpdump -i eth0 port 22

# 2. 过滤特定流量
tcpdump -i eth0 host 192.168.1.10
tcpdump -i eth0 tcp[tcpflags] & tcp-syn != 0

# 3. 分析抓包文件
wireshark capture.pcap
tshark -r capture.pcap -q -z conv,tcp

# 4. 实时分析
tcpdump -i eth0 -A port 80 | grep "GET\|POST"
```

**性能分析工具**
```bash
# 1. 网络性能分析
iftop -i eth0
nload -i eth0
bmon

# 2. 连接状态分析
ss -tuln
netstat -tuln
lsof -i

# 3. 协议分析
nstat
snmpnetstat
ip -s link
```

## 本章小结

高性能网络是HPC系统的关键组件。本章详细介绍了：

1. **InfiniBand技术**：架构原理、性能特点、配置管理和性能测试
2. **以太网优化**：高速以太网技术、RoCE配置、参数优化和故障排除
3. **网络监控与故障排除**：监控指标体系、实时监控仪表板、系统化故障排除流程

掌握这些知识和技术有助于：
- 设计高性能网络架构
- 优化网络性能参数
- 快速诊断和解决网络问题
- 建立完善的网络监控体系

在实际工作中，需要根据具体的网络环境和应用需求，选择合适的网络技术，并进行针对性的优化和监控。