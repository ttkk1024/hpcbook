# 第3章 计算机体系结构

## 3.1 CPU架构与性能

### CPU基本原理

#### CPU组成结构

**CPU核心组件**
```
┌─────────────────────────────────────────────────────────────┐
│                          CPU Core                           │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐        │
│  │  Control│  │ Arithmetic│ │  Cache  │  │  Registers│       │
│  │  Unit   │  │ Logic    │ │  (L1/L2)│  │          │       │
│  │  (CU)   │  │ Unit     │ │         │  │          │       │
│  │         │  │ (ALU)    │ │         │  │          │       │
│  └─────────┘  └─────────┘  └─────────┘  └─────────┘        │
└─────────────────────────────────────────────────────────────┘
                       │
┌─────────────────────────────────────────────────────────────┐
│                      Cache Hierarchy                      │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐        │
│  │  L1 Cache│  │  L2 Cache│  │  L3 Cache│  │  Memory │        │
│  │  (32KB) │  │  (256KB)│  │  (32MB) │  │         │        │
│  └─────────┘  └─────────┘  └─────────┘  └─────────┘        │
└─────────────────────────────────────────────────────────────┘
```

#### CPU工作原理

**指令执行周期**
```assembly
# 典型的指令执行过程
1. 取指 (Fetch): 从内存读取指令到指令寄存器
2. 译码 (Decode): 指令译码器解析指令含义
3. 执行 (Execute): ALU执行算术逻辑运算
4. 写回 (Write-back): 将结果写回寄存器或内存

# 流水线执行示例
Cycle 1: Fetch Instruction 1
Cycle 2: Decode Instruction 1, Fetch Instruction 2
Cycle 3: Execute Instruction 1, Decode Instruction 2, Fetch Instruction 3
Cycle 4: Write-back Instruction 1, Execute Instruction 2, Decode Instruction 3
```

### 现代CPU架构

#### Intel x86-64架构

**Intel Xeon Scalable架构特点**
```bash
# 基本规格
核心数量: 8-64核
制程工艺: 10nm/7nm
缓存结构: L1(32KB)+L2(1MB)+L3(可变)
内存控制器: 6通道DDR4
PCIe通道: 64条PCIe 5.0
TDP范围: 165-350W

# 架构特性
- 超线程技术 (Hyper-Threading)
- 智能缓存分配 (Intel UPI)
- 内存带宽优化 (MCDRAM)
- 安全特性 (SGX, TSX)
```

**性能指标**
```bash
# 计算性能
单核性能: 2.5-3.5 GHz基础频率
加速频率: 3.5-4.5 GHz
IPC提升: 相比上代提升10-20%
TDP: 165-350W

# 内存性能
内存带宽: 204.8 GB/s (DDR4-3200, 8通道)
内存容量: 最大6TB
延迟: ~70-100ns
```

#### AMD Zen架构

**AMD EPYC架构特点**
```bash
# 基本规格
核心数量: 16-64核
制程工艺: 7nm/5nm
缓存结构: L1(32KB)+L2(512KB)+L3(可变)
内存控制器: 8通道DDR4
PCIe通道: 128条PCIe 4.0/5.0
TDP范围: 225-280W

# 架构特性
- Chiplet设计
- Infinity Fabric互连
- 大容量L3缓存
- 安全加密 (SEV)
```

**性能对比**
```bash
# 与Intel对比优势
- 核心密度更高
- 内存带宽更大
- PCIe通道更多
- 能效比更好

# 适用场景
- 高密度计算
- 内存密集型应用
- I/O密集型应用
```

### CPU性能评估

#### 理论峰值性能计算

**FLOPS计算公式**
```bash
# 基本公式
FLOPS = 核心数 × 每周期浮点运算数 × 频率

# Intel Xeon Gold 6248示例
核心数: 20核
频率: 2.5 GHz (基础), 3.9 GHz (加速)
每周期运算数: 16 FLOP (AVX-512, 双精度)

理论峰值:
单精度: 20 × 16 × 3.9GHz = 1,248 GFLOPS
双精度: 20 × 8 × 3.9GHz = 624 GFLOPS
```

**实际性能影响因素**
```bash
# 影响实际性能的因素
1. 内存带宽限制
2. 缓存命中率
3. 并行度限制
4. 热设计功耗
5. 电源管理策略

# 性能优化策略
- 提高数据局部性
- 优化内存访问模式
- 利用向量化指令
- 合理分配线程
```

#### 基准测试工具

**CPU基准测试**
```bash
# LINPACK (HPL)
./xhpl
# 测试双精度浮点性能
# 结果: GFLOPS

# SPEC CPU2017
./runspec --config=myconfig cpu2017
# 综合性能测试
# 包含整数和浮点测试

# STREAM
./stream_c.exe
# 内存带宽测试
# 结果: MB/s

# Geekbench
./geekbench5
# 跨平台性能测试
# 包含单核和多核测试
```

**性能分析工具**
```bash
# perf性能分析
perf top                    # 实时性能热点
perf record ./program       # 记录性能数据
perf report                 # 生成性能报告

# Intel VTune Profiler
vtune -collect hotspots ./program
# 详细的性能分析
# 包含热点分析、内存分析等

# AMD uProf
uprof --analyze ./program
# AMD平台性能分析
# 专门针对Zen架构优化
```

### 并行计算架构

#### 多核架构

**NUMA架构**
```bash
# NUMA拓扑查看
numactl --hardware
numactl --show

# NUMA拓扑示例
node 0 cpus: 0 1 2 3 4 5 6 7
node 0 size: 128 GB
node 0 free: 100 GB
node 1 cpus: 8 9 10 11 12 13 14 15
node 1 size: 128 GB
node 1 free: 110 GB

# NUMA感知编程
numactl --cpunodebind=0 --membind=0 ./program
# 绑定到特定NUMA节点
```

**缓存一致性**
```bash
# MESI协议
# Modified: 修改状态
# Exclusive: 独占状态
# Shared: 共享状态
# Invalid: 无效状态

# 缓存行大小
getconf LEVEL1_DCACHE_LINESIZE  # 通常64字节
getconf LEVEL2_CACHE_LINESIZE
getconf LEVEL3_CACHE_LINESIZE

# 缓存优化策略
# 避免false sharing
# 对齐数据结构
# 优化访问模式
```

#### SIMD架构

**向量化指令集**
```bash
# x86指令集演进
MMX     (1997)  - 64位SIMD
SSE     (1999)  - 128位SIMD
SSE2    (2001)  - 双精度支持
AVX     (2011)  - 256位SIMD
AVX2    (2013)  - 整数SIMD扩展
AVX-512 (2017)  - 512位SIMD

# ARM NEON
# 128位SIMD指令集
# 适用于移动和服务器平台

# 向量化示例
# C代码
for (int i = 0; i < n; i++) {
    c[i] = a[i] + b[i];
}

# 编译器自动向量化
gcc -O3 -mavx2 -mfma vector_code.c

# 手动向量化 (使用intrinsics)
#include <immintrin.h>
__m256 va = _mm256_loadu_ps(a);
__m256 vb = _mm256_loadu_ps(b);
__m256 vc = _mm256_add_ps(va, vb);
_mm256_storeu_ps(c, vc);
```

## 3.2 内存系统

### 内存层次结构

#### 存储器金字塔
```
┌─────────────────────────────────────┐
│           寄存器 (Registers)         │
│  容量: 几十个字节                    │
│  延迟: 1-2周期                       │
│  带宽: 最高                          │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│            L1缓存 (32KB)            │
│  延迟: 3-4周期                       │
│  命中率: 80-90%                      │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│            L2缓存 (256KB)           │
│  延迟: 10-15周期                     │
│  命中率: 90-95%                      │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│            L3缓存 (32MB)            │
│  延迟: 40-50周期                     │
│  命中率: 95-98%                      │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│           主内存 (DDR4)             │
│  容量: 64GB-2TB                      │
│  延迟: 60-100ns                      │
│  带宽: 50-200 GB/s                   │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│           SSD存储                   │
│  容量: 1TB-10TB                      │
│  延迟: 0.1ms                         │
│  带宽: 3-7 GB/s                      │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│           HDD存储                   │
│  容量: 1TB-100TB                     │
│  延迟: 5-10ms                        │
│  带宽: 100-200 MB/s                  │
└─────────────────────────────────────┘
```

### 内存类型与规格

#### DDR内存技术

**DDR4内存规格**
```bash
# 基本参数
电压: 1.2V
频率: 1600-3200 MHz
带宽: 12.8-25.6 GB/s (单通道)
容量: 4-64GB per module
ECC: 支持错误校正
```

**内存通道配置**
```bash
# 单通道配置
内存带宽 = 基础带宽 × 1

# 双通道配置
内存带宽 = 基础带宽 × 2
# 需要成对安装内存条

# 四通道配置
内存带宽 = 基础带宽 × 4
# 高端服务器平台支持

# 八通道配置
内存带宽 = 基础带宽 × 8
# 最新一代服务器平台
```

**内存性能计算**
```bash
# 理论带宽计算
带宽 = 内存频率 × 总线宽度 × 通道数 ÷ 8

# DDR4-3200双通道示例
带宽 = 3200 MHz × 64位 × 2 ÷ 8 = 51.2 GB/s

# 实际可用带宽
通常只有理论值的70-80%
受内存控制器、延迟等因素影响
```

#### 高速内存技术

**Intel Optane DC Persistent Memory**
```bash
# 规格特点
容量: 128GB-512GB per module
延迟: ~350ns (介于DRAM和SSD之间)
带宽: 2-4 GB/s
持久性: 断电数据不丢失
模式: Memory Mode / App Direct Mode
```

**HBM (High Bandwidth Memory)**
```bash
# 规格特点
容量: 4-16GB per stack
带宽: 1000+ GB/s
延迟: 低延迟设计
功耗: 相对较低
应用: GPU、AI加速器
```

### 内存性能优化

#### 内存访问模式

**访问模式分类**
```bash
# 顺序访问
for (int i = 0; i < n; i++) {
    data[i] = i;  // 顺序访问，缓存友好
}

# 随机访问
for (int i = 0; i < n; i++) {
    data[random_index[i]] = i;  // 随机访问，缓存不友好
}

# 步长访问
for (int i = 0; i < n; i += stride) {
    data[i] = i;  // 步长访问，取决于步长大小
}
```

**内存局部性原则**
```bash
# 时间局部性
# 最近访问的数据很可能再次被访问
# 利用: 循环展开、缓存预取

# 空间局部性
# 访问某个数据时，其附近的数据也可能被访问
# 利用: 缓存行预取、数据对齐

# 优化示例
// 不好的访问模式
for (int j = 0; j < n; j++) {
    for (int i = 0; i < m; i++) {
        matrix[i][j] = value;  // 列优先访问，缓存不友好
    }
}

// 好的访问模式
for (int i = 0; i < m; i++) {
    for (int j = 0; j < n; j++) {
        matrix[i][j] = value;  // 行优先访问，缓存友好
    }
}
```

#### 内存分配策略

**NUMA感知分配**
```bash
# 查看NUMA拓扑
numactl --hardware

# NUMA绑定分配
numactl --membind=0 --cpunodebind=0 ./program
# 将进程绑定到特定NUMA节点

# 透明大页 (THP)
echo always > /sys/kernel/mm/transparent_hugepage/enabled
# 启用透明大页，减少TLB miss

# 内存绑定策略
numactl --interleave=all ./program    # 交错分配
numactl --preferred=0 ./program      # 优先分配
```

**内存池技术**
```c
// 内存池实现示例
typedef struct {
    void *pool;
    size_t size;
    size_t used;
} memory_pool_t;

memory_pool_t *create_pool(size_t size) {
    memory_pool_t *pool = malloc(sizeof(memory_pool_t));
    pool->pool = malloc(size);
    pool->size = size;
    pool->used = 0;
    return pool;
}

void *pool_alloc(memory_pool_t *pool, size_t size) {
    if (pool->used + size > pool->size) {
        return NULL;  // 内存不足
    }
    void *ptr = (char *)pool->pool + pool->used;
    pool->used += size;
    return ptr;
}
```

### 内存性能监控

#### 内存使用监控

**系统级监控**
```bash
# 查看内存使用
free -h                    # 总体内存使用
cat /proc/meminfo         # 详细内存信息
vmstat 1                  # 内存和swap统计
sar -r 1                  # 内存使用历史

# 查看进程内存
ps aux --sort=-%mem | head -10    # 高内存使用进程
pmap PID                         # 进程内存映射
smem -r                          # 按RSS排序的内存使用
```

**缓存性能监控**
```bash
# 查看缓存统计
cat /proc/vmstat | grep -E "(cache|swap)"  # 缓存相关统计
sar -B 1                                  # 分页统计

# 缓存命中率监控
perf stat -e cache-misses,cache-references ./program
# 计算缓存命中率 = (references - misses) / references

# TLB性能监控
perf stat -e dTLB-load-misses,iTLB-load-misses ./program
# 监控TLB miss率
```

#### 内存性能分析

**内存分析工具**
```bash
# Valgrind内存分析
valgrind --tool=memcheck --leak-check=full ./program
# 检测内存泄漏、越界访问等问题

# Massif内存使用分析
valgrind --tool=massif ./program
ms_print massif.out.PID
# 分析内存使用模式

# Intel Inspector
inspxe-cl -collect mi1 ./program
# Intel内存和线程检查工具

# Linux perf内存分析
perf record -e mem-loads,mem-stores ./program
perf report
# 分析内存访问模式
```

**内存优化建议**
```bash
# 1. 减少内存分配次数
# 使用对象池、内存池技术

# 2. 提高缓存命中率
# 优化数据结构布局
# 使用合适的数据访问模式

# 3. 减少内存碎片
# 使用大页内存
# 合理的内存分配策略

# 4. NUMA优化
# 数据和计算绑定到同一NUMA节点
# 使用NUMA感知的内存分配
```

## 3.3 存储系统

### 存储层次结构

#### 存储金字塔
```
┌─────────────────────────────────────┐
│           寄存器文件                 │
│  容量: 几KB                          │
│  延迟: 1-2周期                       │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│             CPU缓存                  │
│  L1: 32-64KB                         │
│  L2: 256KB-1MB                       │
│  L3: 2-64MB                          │
│  延迟: 3-50周期                      │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│            主内存                    │
│  DDR4/DDR5                           │
│  容量: 64GB-2TB                      │
│  延迟: 60-100ns                      │
│  带宽: 50-200 GB/s                   │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│           NVMe SSD                   │
│  容量: 1TB-30TB                      │
│  延迟: 0.1ms                         │
│  带宽: 3-14 GB/s                     │
│  IOPS: 100K-1M                       │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│           SATA SSD                   │
│  容量: 250GB-15TB                    │
│  延迟: 0.1ms                         │
│  带宽: 500-5500 MB/s                 │
│  IOPS: 10K-100K                      │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│            HDD                       │
│  容量: 1TB-20TB                      │
│  延迟: 5-10ms                        │
│  带宽: 100-250 MB/s                  │
│  IOPS: 100-200                       │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│           磁带存储                   │
│  容量: 10TB-50TB                     │
│  延迟: 秒级                          │
│  带宽: 100-300 MB/s                  │
│  用途: 长期归档                      │
└─────────────────────────────────────┘
```

### 存储设备类型

#### SSD技术

**NAND Flash类型**
```bash
# SLC (Single-Level Cell)
- 每单元存储1位
- 寿命长: 50,000-100,000次擦写
- 成本高
- 企业级应用

# MLC (Multi-Level Cell)
- 每单元存储2位
- 寿命中等: 3,000-10,000次擦写
- 成本中等
- 消费级和企业级

# TLC (Triple-Level Cell)
- 每单元存储3位
- 寿命较短: 500-3,000次擦写
- 成本低
- 消费级主流

# QLC (Quad-Level Cell)
- 每单元存储4位
- 寿命短: 100-1,000次擦写
- 成本最低
- 大容量存储
```

**接口类型对比**
```bash
# SATA III
- 带宽: 6 Gbps (约550 MB/s)
- 延迟: ~0.1ms
- 成本: 低
- 兼容性: 好

# NVMe PCIe 3.0 x4
- 带宽: 4 GB/s
- 延迟: ~0.05ms
- 成本: 中等
- 性能: 优秀

# NVMe PCIe 4.0 x4
- 带宽: 7 GB/s
- 延迟: ~0.03ms
- 成本: 高
- 性能: 顶级

# NVMe PCIe 5.0 x4
- 带宽: 14 GB/s
- 延迟: ~0.02ms
- 成本: 很高
- 性能: 未来标准
```

#### HDD技术

**机械硬盘规格**
```bash
# 转速
5400 RPM: 笔记本硬盘，安静低功耗
7200 RPM: 台式机主流，性能平衡
10000 RPM: 企业级，高性能
15000 RPM: 高端企业级，极致性能

# 缓存
32MB-256MB: 缓冲数据，提升性能
DRAM + 闪存: 混合缓存技术

# 接口
SATA III: 6 Gbps，主流接口
SAS: 12 Gbps，企业级接口
NVMe: 新兴企业级接口
```

**企业级HDD特点**
```bash
# 可靠性
MTBF: 200万小时
年故障率: <0.5%
工作负载: 550TB/年

# 性能
缓存: 256MB-1GB
旋转缓存: 提升随机读写性能
振动补偿: 多盘位优化

# 管理功能
自加密: 硬件级数据加密
SMART监控: 健康状态监控
TLER: 限时错误恢复
```

### 存储性能指标

#### 性能参数

**IOPS (Input/Output Operations Per Second)**
```bash
# 定义
# 每秒能处理的I/O操作数量
# 随机读写的性能指标

# 计算公式
IOPS = 1000ms / (平均寻道时间 + 平均延迟时间)

# 典型值
HDD: 100-200 IOPS
SATA SSD: 10,000-100,000 IOPS
NVMe SSD: 100,000-1,000,000+ IOPS
```

**吞吐量 (Throughput)**
```bash
# 定义
# 单位时间内传输的数据量
# 顺序读写的性能指标

# 单位
MB/s 或 GB/s

# 典型值
HDD: 100-250 MB/s
SATA SSD: 500-5,500 MB/s
NVMe SSD: 3,000-14,000 MB/s
```

**延迟 (Latency)**
```bash
# 定义
# I/O请求从发出到完成的时间
# 响应速度的指标

# 组成部分
寻道时间 + 旋转延迟 + 传输时间

# 典型值
HDD: 5-10ms
SATA SSD: 0.1ms
NVMe SSD: 0.02-0.05ms
```

#### 性能测试工具

**FIO (Flexible I/O Tester)**
```bash
# 安装
yum install fio
apt-get install fio

# 基本测试
# 顺序读取测试
fio --name=seq_read --ioengine=libaio --rw=read --bs=1m --size=1G --numjobs=4 --runtime=60 --time_based --filename=/dev/sdb

# 随机读取测试
fio --name=rand_read --ioengine=libaio --rw=randread --bs=4k --size=1G --numjobs=4 --runtime=60 --time_based --filename=/dev/sdb

# 混合读写测试
fio --name=mixed --ioengine=libaio --rw=randrw --rwmixread=70 --bs=4k --size=1G --numjobs=4 --runtime=60 --time_based --filename=/dev/sdb

# 结果解读
# read: IOPS=xxx, BW=xxx (xxx/s)(xxx/xxx)
# write: IOPS=xxx, BW=xxx (xxx/s)(xxx/xxx)
# lat (usec): min=xxx, max=xxx, avg=xxx
```

**其他测试工具**
```bash
# dd命令 (简单测试)
dd if=/dev/zero of=testfile bs=1G count=1 oflag=direct
dd if=testfile of=/dev/null bs=1G iflag=direct

# ioping (延迟测试)
ioping -c 10 /dev/sdb
ioping -R /dev/sdb 4k 1m

# hdparm (基础测试)
hdparm -Tt /dev/sdb
hdparm -I /dev/sdb

# CrystalDiskMark (Windows)
# GUI工具，适合快速测试
```

### 存储系统优化

#### RAID配置

**RAID级别对比**
```bash
# RAID 0 (条带化)
- 容量: 100% 利用率
- 性能: 读写性能提升
- 冗余: 无
- 适用: 性能要求高，数据可丢失

# RAID 1 (镜像)
- 容量: 50% 利用率
- 性能: 读性能提升，写性能略降
- 冗余: 高
- 适用: 重要数据，容量要求不高

# RAID 5 (分布式奇偶校验)
- 容量: (n-1)/n 利用率
- 性能: 读性能好，写性能一般
- 冗余: 可容忍1块盘故障
- 适用: 通用存储，性价比高

# RAID 6 (双重奇偶校验)
- 容量: (n-2)/n 利用率
- 性能: 读性能好，写性能较差
- 冗余: 可容忍2块盘故障
- 适用: 大容量存储，高可靠性要求

# RAID 10 (1+0)
- 容量: 50% 利用率
- 性能: 读写性能都好
- 冗余: 高
- 适用: 高性能，高可靠性
```

**RAID配置示例**
```bash
# 软件RAID配置
# 创建RAID 5
mdadm --create /dev/md0 --level=5 --raid-devices=4 /dev/sdb /dev/sdc /dev/sdd /dev/sde

# 创建RAID 10
mdadm --create /dev/md0 --level=10 --raid-devices=4 /dev/sdb /dev/sdc /dev/sdd /dev/sde

# 查看RAID状态
mdadm --detail /dev/md0
cat /proc/mdstat

# 硬件RAID配置
# 使用厂商管理工具
# MegaCLI, storcli, hpssacli
```

#### 文件系统优化

**文件系统选择**
```bash
# XFS
- 优势: 大文件性能好，扩展性强
- 适用: 大型存储，视频处理
- 配置: mkfs.xfs -f -l size=128m /dev/sdb

# ext4
- 优势: 稳定性好，兼容性强
- 适用: 通用存储，系统盘
- 配置: mkfs.ext4 -E stride=16,stripe-width=64 /dev/sdb

# ZFS
- 优势: 高级功能，数据完整性
- 适用: 企业级存储，数据保护
- 配置: zpool create poolname raidz /dev/sdb /dev/sdc /dev/sdd

# Btrfs
- 优势: 现代文件系统，快照功能
- 适用: 需要快照和压缩
- 配置: mkfs.btrfs /dev/sdb
```

**挂载选项优化**
```bash
# XFS优化选项
mount -t xfs -o noatime,logbsize=256k,largeio /dev/sdb1 /data

# ext4优化选项
mount -t ext4 -o noatime,data=writeback,barrier=0 /dev/sdb1 /data

# 通用优化选项
noatime         # 不更新访问时间
nodiratime      # 不更新目录访问时间
data=writeback  # 延迟写入
barrier=0       # 禁用写入屏障
```

#### 存储缓存策略

**缓存层次**
```bash
# L1缓存 (SSD控制器)
- 容量: 512MB-4GB
- 类型: DRAM + SLC缓存
- 作用: 加速随机读写

# L2缓存 (系统内存)
- 容量: 系统可用内存
- 类型: Page Cache
- 作用: 文件系统缓存

# L3缓存 (存储网络)
- 容量: 存储控制器缓存
- 类型: 电池备份DRAM
- 作用: 写入缓冲，读取缓存
```

**缓存配置优化**
```bash
# 文件系统缓存
echo 1 > /proc/sys/vm/dirty_ratio        # 脏页比例
echo 5 > /proc/sys/vm/dirty_background_ratio  # 后台写入比例

# I/O调度器
# noop: 适合SSD
# deadline: 通用，低延迟
# cfq: 传统HDD，公平调度
echo noop > /sys/block/sdb/queue/scheduler

# 预读设置
blockdev --setra 1024 /dev/sdb    # 设置预读大小
```

## 3.4 网络互连技术

### 网络拓扑结构

#### 常见拓扑类型

**总线型拓扑**
```bash
特点:
- 所有节点连接到一条总线
- 简单，成本低
- 冲突域大，性能受限
- 适用于小型网络

缺点:
- 单点故障
- 冲突检测复杂
- 扩展性差
```

**星型拓扑**
```bash
特点:
- 所有节点连接到中心交换机
- 结构清晰，易于管理
- 故障隔离性好
- 当前主流拓扑

配置示例:
交换机1 (核心)
├── 交换机2 (接入)
│   ├── 节点1
│   ├── 节点2
│   └── 节点3
└── 交换机3 (接入)
    ├── 节点4
    ├── 节点5
    └── 节点6
```

**环型拓扑**
```bash
特点:
- 节点形成环状连接
- 数据单向传输
- 故障影响范围大
- 适用于特定场景

应用:
- FDDI网络
- Token Ring
- 某些存储网络
```

**网状拓扑**
```bash
特点:
- 节点间多路径连接
- 高可靠性，高冗余
- 复杂度高，成本高
- 适用于关键业务

类型:
- 全连接网状
- 部分连接网状
- 分层网状
```

### 高性能网络技术

#### InfiniBand技术

**InfiniBand架构**
```bash
# 基本组件
- HCA (Host Channel Adapter): 主机通道适配器
- Switch: 交换机
- Subnet Manager: 子网管理器
- Cabling: 连接线缆

# 协议栈
Application Layer
    ↓
MPI Library
    ↓
IB Verbs API
    ↓
IB Transport Layer
    ↓
IB Network Layer
    ↓
IB Link Layer
    ↓
Physical Layer
```

**InfiniBand版本演进**
```bash
# 传输速率
SDR (Single Data Rate): 2.5 Gbps
DDR (Double Data Rate): 5 Gbps
QDR (Quad Data Rate): 10 Gbps
FDR (Fourteen Data Rate): 14 Gbps
EDR (Enhanced Data Rate): 25 Gbps
HDR (High Data Rate): 50 Gbps
NDR (Next Data Rate): 100 Gbps
XDR (eXtreme Data Rate): 200 Gbps
```

**InfiniBand配置**
```bash
# 安装OFED驱动
./MLNX_OFED_LINUX-5.8-3.0.7.0-rhel9.0-x86_64.tgz

# 配置IPoIB
# /etc/sysconfig/network-scripts/ifcfg-ib0
DEVICE=ib0
TYPE=InfiniBand
ONBOOT=yes
BOOTPROTO=static
IPADDR=10.1.1.10
NETMASK=255.255.255.0
MTU=65520

# 启动服务
systemctl enable rdma
systemctl start rdma
```

#### 高速以太网

**Ethernet演进**
```bash
# 传输速率
10MbE: 10 Mbps
100MbE: 100 Mbps
1GbE: 1000 Mbps
10GbE: 10 Gbps
25GbE: 25 Gbps
40GbE: 40 Gbps
50GbE: 50 Gbps
100GbE: 100 Gbps
400GbE: 400 Gbps
800GbE: 800 Gbps
1.6TbE: 1.6 Tbps
```

**RoCE技术**
```bash
# RoCE版本
RoCE v1: 基于以太网链路层
RoCE v2: 基于UDP/IP层

# 配置RoCE
# 启用DCB
dcbtool sc eth0 dcb on
dcbtool sc eth0 pfc e:1

# 配置RoCE
echo 1 > /sys/class/infiniband/roce0/ports/1/enable

# 测试连接
ibping -c 10 -S 1 remote-node
```

### 网络性能优化

#### 网络参数调优

**TCP参数优化**
```bash
# 网络缓冲区优化
echo 'net.core.rmem_max = 134217728' >> /etc/sysctl.conf
echo 'net.core.wmem_max = 134217728' >> /etc/sysctl.conf
echo 'net.ipv4.tcp_rmem = 4096 87380 134217728' >> /etc/sysctl.conf
echo 'net.ipv4.tcp_wmem = 4096 65536 134217728' >> /etc/sysctl.conf

# 连接队列优化
echo 'net.core.somaxconn = 65535' >> /etc/sysctl.conf
echo 'net.core.netdev_max_backlog = 5000' >> /etc/sysctl.conf

# TCP窗口缩放
echo 'net.ipv4.tcp_window_scaling = 1' >> /etc/sysctl.conf
echo 'net.ipv4.tcp_adv_win_scale = 2' >> /etc/sysctl.conf

# 应用配置
sysctl -p
```

**网络接口优化**
```bash
# 禁用不必要的功能
ethtool -K eth0 gro off    # 禁用GRO
ethtool -K eth0 tso off   # 禁用TSO
ethtool -K eth0 lro off   # 禁用LRO

# 调整中断亲和性
echo 2 > /proc/irq/16/smp_affinity
echo 4 > /proc/irq/17/smp_affinity

# 配置RSS
ethtool -L eth0 combined 8
ethtool -x eth0 equal 8
```

#### 网络监控工具

**带宽测试**
```bash
# iperf3测试
# 服务端
iperf3 -s

# 客户端
iperf3 -c server-ip -t 60 -P 4

# ntttcp测试
# 发送端
ntttcp -s -t 60

# 接收端
ntttcp -r -t 60
```

**延迟测试**
```bash
# ping测试
ping -c 100 -i 0.1 server-ip

# hping3测试
hping3 -S -p 80 -c 100 server-ip

# 网络质量测试
qperf server-ip tcp_lat tcp_bw udp_lat udp_bw
```

**网络分析**
```bash
# 流量监控
iftop -i eth0
nload -i eth0
bmon

# 连接状态
netstat -tuln
ss -tuln
lsof -i

# 网络诊断
tcpdump -i eth0 -w capture.pcap
wireshark capture.pcap
```

### 网络故障排除

#### 常见网络问题

**连接问题**
```bash
# 检查物理连接
ethtool eth0
mii-tool eth0

# 检查IP配置
ip addr show
ip route show

# 检查DNS配置
cat /etc/resolv.conf
nslookup google.com
```

**性能问题**
```bash
# 检查网络拥塞
sar -n DEV 1
iftop

# 检查错误统计
cat /proc/net/dev
ethtool -S eth0

# 检查丢包
ping -i 0.2 -c 1000 server-ip | grep "packet loss"
```

**配置问题**
```bash
# 检查防火墙
iptables -L
firewall-cmd --list-all

# 检查路由
route -n
ip route show

# 检查MTU
ping -M do -s 1472 server-ip
```

#### 网络调试技巧

**分层诊断**
```bash
# 物理层检查
ethtool eth0
dmesg | grep eth0

# 数据链路层检查
arp -a
brctl show

# 网络层检查
ping server-ip
traceroute server-ip

# 传输层检查
netstat -tuln
ss -tuln

# 应用层检查
telnet server-ip port
curl http://server-ip
```

**性能分析**
```bash
# 网络瓶颈识别
sar -n ALL 1
iftop -i eth0
nload

# 协议分析
tcpdump -i eth0 -n -s 0 -w capture.pcap
tshark -r capture.pcap -q -z conv,tcp

# 应用分析
iftop -i eth0 -P
nethogs eth0
```

## 本章小结

计算机体系结构是HPC运维工程师必须掌握的基础知识。本章详细介绍了：

1. **CPU架构与性能**：现代CPU的内部结构、性能指标和优化方法
2. **内存系统**：内存层次结构、性能优化和监控技术
3. **存储系统**：存储设备类型、性能指标和优化策略
4. **网络互连技术**：网络拓扑、高性能网络技术和故障排除

掌握这些知识有助于：
- 理解HPC系统的性能瓶颈
- 优化应用程序性能
- 选择合适的硬件配置
- 解决系统性能问题

在实际工作中，需要根据具体的应用需求和预算限制，合理选择和配置硬件资源，以达到最佳的性能价格比。