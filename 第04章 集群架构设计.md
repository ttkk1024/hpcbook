# 第4章 集群架构设计

## 4.1 集群类型与选择

### 集群分类

#### 按用途分类

**计算集群（Compute Cluster）**
- **特点**：专注于数值计算和科学仿真
- **配置**：高CPU核心数、大内存、高速网络
- **应用**：CFD、分子动力学、量子化学

```
典型配置：
CPU: 64核，2.5GHz
内存: 256GB DDR4
网络: 100GbE InfiniBand
存储: 本地SSD + 并行文件系统
```

**存储集群（Storage Cluster）**
- **特点**：提供高性能存储服务
- **配置**：大容量硬盘、高速网络、RAID配置
- **应用**：并行文件系统、数据库、备份系统

```
典型配置：
CPU: 16核，3.0GHz
内存: 128GB DDR4
存储: 100TB+ HDD RAID 6
网络: 100GbE
```

**GPU集群（GPU Cluster）**
- **特点**：配备GPU加速器
- **配置**：多GPU卡、高速互联、优化散热
- **应用**：深度学习、图像处理、科学计算

```
典型配置：
CPU: 32核，3.2GHz
GPU: 4-8块V100/A100
内存: 512GB DDR4
网络: NVLink + InfiniBand
```

**混合集群（Hybrid Cluster）**
- **特点**：包含多种类型节点
- **配置**：计算节点+存储节点+GPU节点
- **优势**：资源灵活调度、适应多种应用

### 节点类型设计

#### 计算节点分类

**通用计算节点**
```bash
# 典型配置
CPU: 2×Intel Xeon Gold 6248 (20核/2.5GHz)
内存: 384GB DDR4 2933MHz
存储: 1TB NVMe SSD
网络: 2×100GbE + 1×1GbE
```

**大内存节点**
```bash
# 典型配置
CPU: 2×Intel Xeon Platinum 8280 (28核/2.7GHz)
内存: 1.5TB DDR4 2933MHz
存储: 2TB NVMe SSD
网络: 2×100GbE
```

**GPU加速节点**
```bash
# 典型配置
CPU: 2×AMD EPYC 7742 (64核/2.25GHz)
GPU: 8×NVIDIA A100 40GB
内存: 1TB DDR4 3200MHz
存储: 4TB NVMe SSD RAID 0
网络: 2×200GbE HDR InfiniBand
```

**高主频节点**
```bash
# 典型配置
CPU: 2×Intel Xeon Platinum 8380 (40核/2.3GHz)
内存: 512GB DDR4 3200MHz
存储: 2TB NVMe SSD
网络: 2×100GbE
```

#### 存储节点设计

**元数据服务器（MDS）**
```bash
# 典型配置
CPU: 2×Intel Xeon Gold 6248 (20核/2.5GHz)
内存: 768GB DDR4 2933MHz
存储: 4TB NVMe SSD RAID 10
网络: 2×100GbE
```

**对象存储服务器（OSS）**
```bash
# 典型配置
CPU: 2×Intel Xeon Silver 4216 (16核/2.1GHz)
内存: 128GB DDR4 2666MHz
存储: 12×16TB HDD RAID 6 + 2TB NVMe SSD缓存
网络: 2×25GbE
```

### 集群规模规划

#### 小型集群（<50节点）
- **适用场景**：部门级、研究组
- **特点**：成本低、管理简单
- **配置**：标准服务器、千兆/万兆网络

#### 中型集群（50-500节点）
- **适用场景**：院所级、企业
- **特点**：平衡性能与成本
- **配置**：高性能服务器、高速网络

#### 大型集群（>500节点）
- **适用场景**：国家级、超算中心
- **特点**：极致性能、复杂管理
- **配置**：定制化硬件、顶级网络

### 成本效益分析

#### 硬件成本估算

```bash
# 小型集群（20节点）成本估算
计算节点: 20 × $15,000 = $300,000
存储节点: 4 × $25,000 = $100,000
网络设备: $50,000
管理节点: 2 × $10,000 = $20,000
总计: $470,000

# 大型集群（1000节点）成本估算
计算节点: 1000 × $20,000 = $20,000,000
存储系统: $5,000,000
网络设备: $3,000,000
管理节点: 10 × $15,000 = $150,000
总计: $28,150,000
```

#### 运维成本考虑
- **电力消耗**：每千瓦时成本 × 功率 × 运行时间
- **冷却成本**：PUE值 × 电力成本
- **人力成本**：运维团队规模 × 平均薪资
- **维护成本**：硬件保修、备件库存

## 4.2 节点配置策略

### 硬件选型原则

#### CPU选择策略

**Intel Xeon系列**
```bash
# 适用场景
- 通用计算：Xeon Gold/Silver
- 高性能计算：Xeon Platinum
- GPU加速：Xeon Scalable

# 选择要点
- 核心数量 vs 主频
- 内存通道数
- PCIe通道数
- 功耗与散热
```

**AMD EPYC系列**
```bash
# 适用场景
- 大内存应用：EPYC 7003系列
- 高性价比：EPYC 7002系列
- 新架构：EPYC 9004系列

# 优势特点
- 更多核心数
- 更多内存通道
- 更多PCIe通道
- 更好性价比
```

#### 内存配置策略

**容量规划**
```bash
# 计算密集型应用
每核 2-4GB内存
典型配置：256-512GB

# 内存密集型应用
每核 8-16GB内存
典型配置：512GB-2TB

# 平衡型配置
每核 4-8GB内存
典型配置：384-1TB
```

**类型选择**
```bash
# DDR4 vs DDR5
DDR4: 成熟稳定，成本较低
DDR5: 更高速度，更高容量

# ECC vs 非ECC
ECC: 错误校正，稳定性高
非ECC: 成本低，性能略高
```

#### 存储配置策略

**本地存储**
```bash
# SSD选择
NVMe SSD: 高性能，高成本
SATA SSD: 平衡性能与成本
Optane: 超低延迟，极高成本

# 容量规划
系统盘: 500GB-1TB
数据盘: 1-4TB
缓存盘: 200-500GB
```

**RAID配置**
```bash
# RAID级别选择
RAID 0: 性能最优，无冗余
RAID 1: 100%冗余，容量减半
RAID 5: 平衡性能与冗余
RAID 10: 高性能，高冗余

# 配置示例
# 系统盘RAID 1
mdadm --create /dev/md0 --level=1 --raid-devices=2 /dev/sda /dev/sdb

# 数据盘RAID 10
mdadm --create /dev/md1 --level=10 --raid-devices=4 /dev/sdc /dev/sdd /dev/sde /dev/sdf
```

### 网络配置策略

#### 网络拓扑设计

**Fat-Tree拓扑**
```bash
# 特点
- 高带宽
- 低延迟
- 良好扩展性
- 成本较高

# 适用场景
- 大型集群
- 高通信密集型应用
```

**Dragonfly拓扑**
```bash
# 特点
- 低直径
- 高容错性
- 成本效益好
- 设计复杂

# 适用场景
- 超大规模集群
- 科研计算中心
```

**Torus拓扑**
```bash
# 特点
- 规则结构
- 低延迟
- 扩展性好
- 通信模式受限

# 适用场景
- 规则网格计算
- 物理仿真应用
```

#### 网络设备选型

**交换机选择**
```bash
# 核心交换机
端口数: 32-64端口
带宽: 100GbE/200GbE
延迟: <1μs
背板带宽: 10+ Tbps

# 接入交换机
端口数: 16-48端口
带宽: 25GbE/100GbE
延迟: <2μs
```

**网卡选择**
```bash
# InfiniBand网卡
厂商: Mellanox/Intel
速率: HDR 200Gb/s
延迟: <0.5μs
协议: RoCEv2

# 以太网网卡
厂商: Intel/Mellanox
速率: 25/100/200GbE
延迟: <1μs
协议: TCP/IP, RoCE
```

### 软件环境配置

#### 操作系统选择
```bash
# 企业级选择
CentOS Stream 9: 稳定性好，支持周期长
RHEL 9: 企业支持，功能完善
Ubuntu 22.04 LTS: 用户友好，更新及时

# HPC专用
Rocky Linux 9: RHEL兼容
AlmaLinux 9: 社区驱动
OpenHPC: HPC优化
```

#### 中间件配置
```bash
# MPI库选择
OpenMPI: 开源，功能丰富
Intel MPI: 性能优化，Intel平台
MVAPICH2: InfiniBand优化
MPICH: 标准兼容，稳定

# 编译器选择
GCC: 开源，通用性好
Intel OneAPI: Intel平台优化
AOCC: AMD平台优化
NVHPC: NVIDIA GPU优化
```

## 4.3 网络拓扑设计

### 网络架构原则

#### 分层设计
```bash
# 三层架构
接入层: 节点接入，端口密度
汇聚层: 流量汇聚，策略实施
核心层: 高速转发，可靠性
```

#### 带宽规划
```bash
# 计算网络
节点带宽: 25/100/200GbE
聚合比: 1:1 - 4:1
总带宽: 节点数 × 节点带宽

# 管理网络
带宽: 1/10GbE
隔离: 独立VLAN
冗余: 双网卡绑定
```

### 典型拓扑结构

#### 二层网络拓扑

**Leaf-Spine架构**
```
┌─────────────────────────────────────┐
│              Spine Layer            │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐
│  │ Switch  │  │ Switch  │  │ Switch  │
│  │   S1    │  │   S2    │  │   S3    │
│  └─────────┘  └─────────┘  └─────────┘
│       │              │              │
│       └──────────────┼──────────────┘
│                      │
└──────────────────────┼───────────────────
                       │
┌──────────────────────┼───────────────────┐
│        Leaf Layer    │                   │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  │
│  │ Switch  │  │ Switch  │  │ Switch  │  │
│  │   L1    │  │   L2    │  │   L3    │  │
│  └─────────┘  └─────────┘  └─────────┘  │
│       │              │              │  │
│  ┌────┼────┐    ┌────┼────┐    ┌────┼──┴──┐
│  │    │    │    │    │    │    │    │     │
│ Node Node Node Node Node Node Node Node Node
└───────────────────────────────────────────┘
```

**优势特点**：
- �零阻塞设计
- 良好的扩展性
- 低延迟通信
- 负载均衡

#### 三层网络拓扑

**核心-汇聚-接入架构**
```
┌─────────────────────────────────────┐
│            Core Layer               │
│  ┌─────────┐  ┌─────────┐           │
│  │ Switch  │  │ Switch  │           │
│  │   C1    │  │   C2    │           │
│  └─────────┘  └─────────┘           │
└─────────────────────────────────────┘
                       │ │
              ┌────────┘ └────────┐
              │                    │
┌─────────────────────────────────────┐
│           Aggregation Layer         │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐
│  │ Switch  │  │ Switch  │  │ Switch  │
│  │   A1    │  │   A2    │  │   A3    │
│  └─────────┘  └─────────┘  └─────────┘
└─────────────────────────────────────┘
        │ │              │ │
        │ └──────────────┘ │
        │                  │
┌─────────────────────────────────────┐
│             Access Layer            │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐
│  │ Switch  │  │ Switch  │  │ Switch  │
│  │   X1    │  │   X2    │  │   X3    │
│  └─────────┘  └─────────┘  └─────────┘
│       │              │              │
│  ┌────┼────┐    ┌────┼────┐    ┌────┼──┴──┐
│  │    │    │    │    │    │    │    │     │
│ Node Node Node Node Node Node Node Node Node
└───────────────────────────────────────────┘
```

### 网络协议选择

#### InfiniBand网络

**协议栈结构**
```
Application Layer
    ↓
MPI Library (OpenMPI, Intel MPI)
    ↓
IB Verbs API
    ↓
IB Transport Layer (RC, UC, UD)
    ↓
IB Network Layer
    ↓
IB Link Layer
    ↓
Physical Layer (HDR, EDR, FDR)
```

**配置示例**
```bash
# 1. 安装OFED驱动
./MLNX_OFED_LINUX-5.8-3.0.7.0-rhel9.0-x86_64.tgz

# 2. 配置IPoIB
# /etc/sysconfig/network-scripts/ifcfg-ib0
DEVICE=ib0
TYPE=InfiniBand
ONBOOT=yes
BOOTPROTO=static
IPADDR=10.1.1.10
NETMASK=255.255.255.0
MTU=65520
```

#### Ethernet网络

**RDMA over Converged Ethernet (RoCE)**
```bash
# 1. 启用DCB
dcbtool sc eth0 dcb on
dcbtool sc eth0 pfc e:1

# 2. 配置RoCE
echo 1 > /sys/class/infiniband/roce0/ports/1/enable

# 3. 测试连接
ibping -c 10 -S 1 remote-node
```

### 网络性能优化

#### QoS配置
```bash
# 1. 配置流量分类
tc qdisc add dev eth0 root handle 1: htb default 12
tc class add dev eth0 parent 1: classid 1:1 htb rate 100gbit
tc class add dev eth0 parent 1:1 classid 1:10 htb rate 80gbit prio 0
tc class add dev eth0 parent 1:1 classid 1:12 htb rate 20gbit prio 1

# 2. 配置队列规则
tc filter add dev eth0 protocol ip parent 1: prio 0 u32 match ip dport 47556 0xffff flowid 1:10
```

#### 负载均衡
```bash
# 1. 配置ECMP
ip route add 10.1.0.0/16 nexthop via 192.168.1.1 nexthop via 192.168.1.2

# 2. 配置LACP
# /etc/sysconfig/network-scripts/ifcfg-bond0
DEVICE=bond0
TYPE=Bond
BONDING_MASTER=yes
BOOTPROTO=static
IPADDR=192.168.1.10
NETMASK=255.255.255.0
ONBOOT=yes
BONDING_OPTS="mode=4 miimon=100 lacp_rate=fast"
```

## 4.4 存储架构规划

### 存储层次设计

#### 存储金字塔
```
┌─────────────────────────────────────┐
│           Application Layer         │
│  (Compute Nodes, User Applications) │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│            Storage Access           │
│  (Lustre Clients, GPFS Clients)     │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│           Parallel File System      │
│  (Lustre MDS, GPFS NSD)             │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│             Storage Servers         │
│  (MDS Servers, OSS Servers)         │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│              Storage Devices        │
│  (HDD, SSD, NVMe)                   │
└─────────────────────────────────────┘
```

### 并行文件系统选择

#### Lustre文件系统

**架构组件**
```bash
# 元数据服务器 (MDS)
- 功能: 管理文件系统元数据
- 配置: 高内存，快速存储
- 数量: 1-2台（主备）

# 对象存储服务器 (OSS)
- 功能: 存储文件数据
- 配置: 多磁盘，高吞吐
- 数量: 根据容量需求

# 客户端
- 功能: 访问文件系统
- 配置: Lustre客户端模块
- 数量: 所有计算节点
```

**部署配置**
```bash
# 1. 安装Lustre服务端
yum install lustre-osd-ldiskfs
yum install lustre

# 2. 配置MDS
mkfs.lustre --fsname=hpc --mgs --mdt /dev/sdb1
mount -t lustre /dev/sdb1 /mnt/mdt

# 3. 配置OSS
mkfs.lustre --fsname=hpc --mgsnode=mds-ip@tcp --ost /dev/sdb1
mount -t lustre /dev/sdb1 /mnt/ost

# 4. 客户端挂载
mount -t lustre mds-ip@tcp:/hpc /mnt/lustre
```

#### GPFS文件系统

**架构特点**
```bash
# NSD服务器
- 功能: 提供存储设备访问
- 配置: 多路径，高可用
- 管理: NSD管理服务器

# GPFS集群
- 功能: 文件系统管理
- 配置: 集群管理节点
- 通信: 高速网络
```

**配置示例**
```bash
# 1. 创建NSD
mmcrnsd -F nsd.config

# 2. 创建文件系统
mmcrfs hpcfs -F fs.config

# 3. 挂载文件系统
mcmount hpcfs
```

### 存储性能优化

#### 缓存策略
```bash
# 1. 客户端缓存
mount -o localflock,readahead=16m,async /mnt/lustre

# 2. 服务端缓存
echo 1 > /proc/sys/vm/dirty_ratio
echo 5 > /proc/sys/vm/dirty_background_ratio

# 3. SSD缓存
mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/nvme0n1 /dev/nvme1n1
```

#### 条带化配置
```bash
# Lustre条带化
lfs setstripe -c 4 -S 1m /mnt/lustre/file

# GPFS条带化
mmchfs hpcfs -B 1m
```

### 备份与容灾

#### 备份策略设计

**分级备份**
```bash
# 热数据备份
- 频率: 实时/小时级
- 方式: 同步复制
- 目标: 本地备份

# 温数据备份
- 频率: 每日
- 方式: 增量备份
- 目标: 异地备份

# 冷数据备份
- 频率: 每周/月
- 方式: 完整备份
- 目标: 磁带/云存储
```

#### 容灾方案
```bash
# 1. 数据复制
rsync -avz --delete /data/ backup-server:/backup/

# 2. 快照备份
lvcreate --size 100G --snapshot --name snap /dev/vg/data

# 3. 异地容灾
# 配置DRBD
drbdadm create-md r0
drbdadm up r0
```

## 本章小结

集群架构设计是HPC系统建设的关键环节。本章详细介绍了：

1. **集群类型与选择**：不同用途的集群特点和适用场景
2. **节点配置策略**：硬件选型原则和软件环境配置
3. **网络拓扑设计**：网络架构原则和典型拓扑结构
4. **存储架构规划**：存储层次设计和并行文件系统选择

合理的架构设计能够确保HPC系统的高性能、高可用性和良好的扩展性。在实际规划中，需要根据具体的应用需求、预算限制和未来发展规划，制定最适合的技术方案。