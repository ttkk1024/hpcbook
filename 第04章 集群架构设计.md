# 第4章 集群架构设计

## 4.1 集群类型与选择

### 集群分类

#### 按用途分类

**计算集群（Compute Cluster）**
- **特点**：专注于数值计算和科学仿真
- **配置**：高密度多核CPU、高带宽内存
- **应用**：CFD、分子动力学、天气预报

```
典型配置 (2025):
CPU: 双路 96-128核 (AMD EPYC 9004/Turin 或 Intel Xeon 6)
内存: 768GB-1.5TB DDR5-6000+
网络: 200/400Gbps NDR InfiniBand
存储: 本地 NVMe Gen5 SSD
```

**AI 超算集群（AI Supercomputing Cluster / SuperPOD）**
- **特点**：专为大规模 AI 模型训练设计
- **配置**：紧密耦合的 GPU 节点、Rail-optimized 网络
- **应用**：LLM 训练、Generative AI、AI4Science

```
典型配置:
GPU: 8x NVIDIA H100/H200/Blackwell (HGX 平台)
互连: NVSwitch / NVLink Switch (900GB/s+ 双向)
网络: 8x 400Gbps NDR IB (计算网) + 2x 200GbE (存储/管理网)
散热: 冷板式液冷 (DLC)
```

**存储集群（Storage Cluster）**
- **特点**：提供高性能并行存储服务
- **配置**：全闪存 (All-Flash) 或 混闪架构
- **应用**：并行文件系统 (Lustre/DAOS)、对象存储

```
典型配置:
控制器: 双路 32-64核
缓存: DDR5 + Optane/CXL Memory
存储介质: NVMe Gen5 SSD (热数据) + HDD (冷数据)
网络: 400Gbps NDR / 400GbE
```

**混合/异构集群（Heterogeneous Cluster）**
- **特点**：CPU+GPU+DPU 多算力融合
- **优势**：适应多样化工作负载 (HPC + AI + BigData)

### 节点类型设计

#### 计算节点分类

**通用计算节点**
```bash
# 典型配置 (2025 Standard)
CPU: 2×AMD EPYC 9654 (96核/2.4GHz) 或 Intel Xeon 6900P
内存: 768GB DDR5 4800/5600MHz (24GB/core 配比)
存储: 3.84TB NVMe Gen4/5 SSD
网络: 1× NDR 200/400Gbps IB
```

**大内存节点**
```bash
# 典型配置
CPU: 2×Intel Xeon Platinum 8592+ (64核/1.9GHz)
内存: 4TB-8TB DDR5 (支持 CXL 内存扩展)
存储: 8TB NVMe SSD
网络: 2× NDR 400Gbps IB
```

**AI/GPU 加速节点**
```bash
# 典型配置 (HGX H100/H200)
CPU: 2×Intel Xeon Platinum 8480+
GPU: 8×NVIDIA H100 SXM5 80GB (或 H200 141GB)
内存: 2TB DDR5 4800MHz
存储: 30TB NVMe Gen4/5 SSD (高速本地缓存)
网络: 4× NDR 400Gbps IB (计算平面) + 2× 200GbE (存储平面)
```

#### 存储节点设计

**元数据服务器（MDS）**
```bash
# 典型配置
CPU: 2×Intel Xeon Gold 6248 (20核/2.5GHz)
内存: 768GB DDR4 2933MHz
存储: 4TB NVMe SSD RAID 10
网络: 2×100GbE
```

**对象存储服务器（OSS）**
```bash
# 典型配置
CPU: 2×Intel Xeon Silver 4216 (16核/2.1GHz)
内存: 128GB DDR4 2666MHz
存储: 12×16TB HDD RAID 6 + 2TB NVMe SSD缓存
网络: 2×25GbE
```

### 集群规模规划

#### 小型集群（<50节点）
- **适用场景**：部门级、研究组
- **特点**：成本低、管理简单
- **配置**：标准服务器、千兆/万兆网络

#### 中型集群（50-500节点）
- **适用场景**：院所级、企业
- **特点**：平衡性能与成本
- **配置**：高性能服务器、高速网络

#### 大型集群（>500节点）
- **适用场景**：国家级、超算中心
- **特点**：极致性能、复杂管理
- **配置**：定制化硬件、顶级网络

### 成本效益分析

#### 硬件成本估算

```bash
# 小型集群（20节点）成本估算
计算节点: 20 × $15,000 = $300,000
存储节点: 4 × $25,000 = $100,000
网络设备: $50,000
管理节点: 2 × $10,000 = $20,000
总计: $470,000

# 大型集群（1000节点）成本估算
计算节点: 1000 × $20,000 = $20,000,000
存储系统: $5,000,000
网络设备: $3,000,000
管理节点: 10 × $15,000 = $150,000
总计: $28,150,000
```

#### 运维成本考虑
- **电力消耗**：每千瓦时成本 × 功率 × 运行时间
- **冷却成本**：PUE值 × 电力成本
- **人力成本**：运维团队规模 × 平均薪资
- **维护成本**：硬件保修、备件库存

## 4.2 节点配置策略

### 硬件选型原则

#### CPU选择策略

**Intel Xeon 系列 (Xeon Scalable Gen4/5/6)**
```bash
# 适用场景
- 通用计算：Xeon Gold / Platinum 8400/8500
- 高性能计算/AI：Xeon Max (集成 HBM)
- 能效优化：Xeon 6 (Sierra Forest - E-cores)

# 选择要点
- 支持 AMX 指令集 (AI 加速)
- CXL 1.1/2.0 支持
- DDR5 内存带宽
```

**AMD EPYC 系列 (Genoa/Bergamo/Turin)**
```bash
# 适用场景
- 通用/HPC：EPYC 9004 (Genoa)
- 云原生/高吞吐：EPYC 9754 (Bergamo, 128核)
- 极致性能：EPYC Turin (Zen 5)

# 优势特点
- 单路/双路核心数极高 (96-192核)
- 12通道 DDR5 内存
- 128条 PCIe 5.0 通道
```

#### 内存配置策略

**容量规划**
```bash
# 计算密集型
每核 3-4GB (DDR5 高带宽优势)
典型: 512GB - 768GB

# AI 训练/大模型
每 GPU 对应系统内存 1TB+
典型: 1.5TB - 2TB (配合 HBM 使用)
```

**类型选择**
```bash
# DDR4 vs DDR5
DDR5: 2025年主流，带宽翻倍 (4800-6400MT/s)
HBM (High Bandwidth Memory): 集成在 CPU/GPU 内，超高带宽

#MRDIMM / MCR DIMM
新一代高带宽内存模组，用于极致性能场景
```

#### 存储配置策略

**本地存储**
```bash
# SSD选择
NVMe SSD: 高性能，高成本
SATA SSD: 平衡性能与成本
Optane: 超低延迟，极高成本

# 容量规划
系统盘: 500GB-1TB
数据盘: 1-4TB
缓存盘: 200-500GB
```

**RAID配置**
```bash
# RAID级别选择
RAID 0: 性能最优，无冗余
RAID 1: 100%冗余，容量减半
RAID 5: 平衡性能与冗余
RAID 10: 高性能，高冗余

# 配置示例
# 系统盘RAID 1
mdadm --create /dev/md0 --level=1 --raid-devices=2 /dev/sda /dev/sdb

# 数据盘RAID 10
mdadm --create /dev/md1 --level=10 --raid-devices=4 /dev/sdc /dev/sdd /dev/sde /dev/sdf
```

### 网络配置策略

#### 网络拓扑设计

**Fat-Tree 拓扑**
```bash
# 特点
- 无阻塞，全对全带宽
- 易于理解和管理
- 成本随规模指数增长

# 适用场景
- 中小规模集群 (<500节点)
- 对长尾延迟敏感的应用
```

**Dragonfly+ 拓扑**
```bash
# 特点
- 极低的网络直径 (通常 <= 3跳)
- 成本效益极高 (减少长光缆使用)
- 依赖自适应路由 (Adaptive Routing) 避免拥塞

# 适用场景
- 超大规模集群 (Exascale, >1000节点)
- AI SuperPOD (部分采用类似 Rail-optimized 结构)
```

#### 网络设备选型

**交换机选择**
```bash
# IB交换机
规格: 64端口 NDR (400Gbps) / 32端口 XDR (800Gbps)
技术: 锋利计算 (Sharp) - 网内计算加速
延迟: <130ns

# 以太网交换机
规格: 32/64端口 400GbE/800GbE (QSFP-DD/OSFP)
特性: RoCEv2, PFC, ECN (无损网络支持)
```

**网卡选择**
```bash
# InfiniBand HCA
型号: NVIDIA ConnectX-7 / ConnectX-8
速率: 400Gb/s (NDR) / 800Gb/s (XDR)
功能: MPI Tag Matching 硬件卸载

# AI 专用 DPU
型号: NVIDIA BlueField-3
功能: 卸载基础设施任务 (存储、安全、管控)
```

### 软件环境配置

#### 操作系统选择
```bash
# 企业级选择
CentOS Stream 9: 稳定性好，支持周期长
RHEL 9: 企业支持，功能完善
Ubuntu 22.04 LTS: 用户友好，更新及时

# HPC专用
Rocky Linux 9: RHEL兼容
AlmaLinux 9: 社区驱动
OpenHPC: HPC优化
```

#### 中间件配置
```bash
# MPI库选择
OpenMPI: 开源，功能丰富
Intel MPI: 性能优化，Intel平台
MVAPICH2: InfiniBand优化
MPICH: 标准兼容，稳定

# 编译器选择
GCC: 开源，通用性好
Intel OneAPI: Intel平台优化
AOCC: AMD平台优化
NVHPC: NVIDIA GPU优化
```

## 4.3 网络拓扑设计

### 网络架构原则

#### 分层设计
```bash
# 三层架构
接入层: 节点接入，端口密度
汇聚层: 流量汇聚，策略实施
核心层: 高速转发，可靠性
```

#### 带宽规划
```bash
# 计算网络
节点带宽: 25/100/200GbE
聚合比: 1:1 - 4:1
总带宽: 节点数 × 节点带宽

# 管理网络
带宽: 1/10GbE
隔离: 独立VLAN
冗余: 双网卡绑定
```

### 典型拓扑结构

#### 二层网络拓扑

**Leaf-Spine架构**
```
┌─────────────────────────────────────┐
│              Spine Layer            │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐
│  │ Switch  │  │ Switch  │  │ Switch  │
│  │   S1    │  │   S2    │  │   S3    │
│  └─────────┘  └─────────┘  └─────────┘
│       │              │              │
│       └──────────────┼──────────────┘
│                      │
└──────────────────────┼───────────────────
                       │
┌──────────────────────┼───────────────────┐
│        Leaf Layer    │                   │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  │
│  │ Switch  │  │ Switch  │  │ Switch  │  │
│  │   L1    │  │   L2    │  │   L3    │  │
│  └─────────┘  └─────────┘  └─────────┘  │
│       │              │              │  │
│  ┌────┼────┐    ┌────┼────┐    ┌────┼──┴──┐
│  │    │    │    │    │    │    │    │     │
│ Node Node Node Node Node Node Node Node Node
└───────────────────────────────────────────┘
```

**优势特点**：
- �零阻塞设计
- 良好的扩展性
- 低延迟通信
- 负载均衡

#### 三层网络拓扑

**核心-汇聚-接入架构**
```
┌─────────────────────────────────────┐
│            Core Layer               │
│  ┌─────────┐  ┌─────────┐           │
│  │ Switch  │  │ Switch  │           │
│  │   C1    │  │   C2    │           │
│  └─────────┘  └─────────┘           │
└─────────────────────────────────────┘
                       │ │
              ┌────────┘ └────────┐
              │                    │
┌─────────────────────────────────────┐
│           Aggregation Layer         │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐
│  │ Switch  │  │ Switch  │  │ Switch  │
│  │   A1    │  │   A2    │  │   A3    │
│  └─────────┘  └─────────┘  └─────────┘
└─────────────────────────────────────┘
        │ │              │ │
        │ └──────────────┘ │
        │                  │
┌─────────────────────────────────────┐
│             Access Layer            │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐
│  │ Switch  │  │ Switch  │  │ Switch  │
│  │   X1    │  │   X2    │  │   X3    │
│  └─────────┘  └─────────┘  └─────────┘
│       │              │              │
│  ┌────┼────┐    ┌────┼────┐    ┌────┼──┴──┐
│  │    │    │    │    │    │    │    │     │
│ Node Node Node Node Node Node Node Node Node
└───────────────────────────────────────────┘
```

### 网络协议选择

#### InfiniBand网络

**协议栈结构**
```
Application Layer
    ↓
MPI Library (OpenMPI, Intel MPI)
    ↓
IB Verbs API
    ↓
IB Transport Layer (RC, UC, UD)
    ↓
IB Network Layer
    ↓
IB Link Layer
    ↓
Physical Layer (HDR, EDR, FDR)
```

**配置示例**
```bash
# 1. 安装OFED驱动
./MLNX_OFED_LINUX-5.8-3.0.7.0-rhel9.0-x86_64.tgz

# 2. 配置IPoIB
# /etc/sysconfig/network-scripts/ifcfg-ib0
DEVICE=ib0
TYPE=InfiniBand
ONBOOT=yes
BOOTPROTO=static
IPADDR=10.1.1.10
NETMASK=255.255.255.0
MTU=65520
```

#### Ethernet网络

**RDMA over Converged Ethernet (RoCE)**
```bash
# 1. 启用DCB
dcbtool sc eth0 dcb on
dcbtool sc eth0 pfc e:1

# 2. 配置RoCE
echo 1 > /sys/class/infiniband/roce0/ports/1/enable

# 3. 测试连接
ibping -c 10 -S 1 remote-node
```

### 网络性能优化

#### QoS配置
```bash
# 1. 配置流量分类
tc qdisc add dev eth0 root handle 1: htb default 12
tc class add dev eth0 parent 1: classid 1:1 htb rate 100gbit
tc class add dev eth0 parent 1:1 classid 1:10 htb rate 80gbit prio 0
tc class add dev eth0 parent 1:1 classid 1:12 htb rate 20gbit prio 1

# 2. 配置队列规则
tc filter add dev eth0 protocol ip parent 1: prio 0 u32 match ip dport 47556 0xffff flowid 1:10
```

#### 负载均衡
```bash
# 1. 配置ECMP
ip route add 10.1.0.0/16 nexthop via 192.168.1.1 nexthop via 192.168.1.2

# 2. 配置LACP
# /etc/sysconfig/network-scripts/ifcfg-bond0
DEVICE=bond0
TYPE=Bond
BONDING_MASTER=yes
BOOTPROTO=static
IPADDR=192.168.1.10
NETMASK=255.255.255.0
ONBOOT=yes
BONDING_OPTS="mode=4 miimon=100 lacp_rate=fast"
```

## 4.4 存储架构规划

### 存储层次设计

#### 存储金字塔
```
┌─────────────────────────────────────┐
│           Application Layer         │
│  (Compute Nodes, User Applications) │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│            Storage Access           │
│  (Lustre Clients, GPFS Clients)     │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│           Parallel File System      │
│  (Lustre MDS, GPFS NSD)             │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│             Storage Servers         │
│  (MDS Servers, OSS Servers)         │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│              Storage Devices        │
│  (HDD, SSD, NVMe)                   │
└─────────────────────────────────────┘
```

### 并行文件系统选择

#### Lustre文件系统

**架构组件**
```bash
# 元数据服务器 (MDS)
- 功能: 管理文件系统元数据
- 配置: 高内存，快速存储
- 数量: 1-2台（主备）

# 对象存储服务器 (OSS)
- 功能: 存储文件数据
- 配置: 多磁盘，高吞吐
- 数量: 根据容量需求

# 客户端
- 功能: 访问文件系统
- 配置: Lustre客户端模块
- 数量: 所有计算节点
```

**部署配置**
```bash
# 1. 安装Lustre服务端
yum install lustre-osd-ldiskfs
yum install lustre

# 2. 配置MDS
mkfs.lustre --fsname=hpc --mgs --mdt /dev/sdb1
mount -t lustre /dev/sdb1 /mnt/mdt

# 3. 配置OSS
mkfs.lustre --fsname=hpc --mgsnode=mds-ip@tcp --ost /dev/sdb1
mount -t lustre /dev/sdb1 /mnt/ost

# 4. 客户端挂载
mount -t lustre mds-ip@tcp:/hpc /mnt/lustre
```

#### GPFS文件系统

**架构特点**
```bash
# NSD服务器
- 功能: 提供存储设备访问
- 配置: 多路径，高可用
- 管理: NSD管理服务器

# GPFS集群
- 功能: 文件系统管理
- 配置: 集群管理节点
- 通信: 高速网络
```

**配置示例**
```bash
# 1. 创建NSD
mmcrnsd -F nsd.config

# 2. 创建文件系统
mmcrfs hpcfs -F fs.config

# 3. 挂载文件系统
mcmount hpcfs
```

### 存储性能优化

#### 缓存策略
```bash
# 1. 客户端缓存
mount -o localflock,readahead=16m,async /mnt/lustre

# 2. 服务端缓存
echo 1 > /proc/sys/vm/dirty_ratio
echo 5 > /proc/sys/vm/dirty_background_ratio

# 3. SSD缓存
mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/nvme0n1 /dev/nvme1n1
```

#### 条带化配置
```bash
# Lustre条带化
lfs setstripe -c 4 -S 1m /mnt/lustre/file

# GPFS条带化
mmchfs hpcfs -B 1m
```

### 备份与容灾

#### 备份策略设计

**分级备份**
```bash
# 热数据备份
- 频率: 实时/小时级
- 方式: 同步复制
- 目标: 本地备份

# 温数据备份
- 频率: 每日
- 方式: 增量备份
- 目标: 异地备份

# 冷数据备份
- 频率: 每周/月
- 方式: 完整备份
- 目标: 磁带/云存储
```

#### 容灾方案
```bash
# 1. 数据复制
rsync -avz --delete /data/ backup-server:/backup/

# 2. 快照备份
lvcreate --size 100G --snapshot --name snap /dev/vg/data

# 3. 异地容灾
# 配置DRBD
drbdadm create-md r0
drbdadm up r0
```

## 4.5 绿色数据中心与散热设计 (Green HPC)

### 散热挑战与解决方案

随着 AI 芯片功耗激增 (H100 > 700W, Blackwell > 1000W)，单机柜功率密度已突破 40kW-100kW，传统风冷已逼近极限。

#### 液冷技术 (Liquid Cooling)

**冷板式液冷 (Direct-to-Chip / DLC)**
- **原理**：冷却液直接流经贴合在 CPU/GPU/内存 上的冷板。
- **优势**：
  - 散热效率高，PUE 可降至 1.1-1.2。
  - 兼容现有数据中心机架架构。
- **适用**：高密度计算节点，AI 训练集群。

**浸没式液冷 (Immersion Cooling)**
- **原理**：将服务器完全浸泡在绝缘冷却液中。
- **类型**：单相浸没 (无相变)、双相浸没 (蒸发冷凝)。
- **优势**：
  - 极致 PUE (< 1.05)。
  - 静音，无风扇震动。
- **挑战**：维护复杂，需定制硬件。

#### 基础设施规划

**PUE (Power Usage Effectiveness) 优化**
- 目标：新建 HPC 数据中心 PUE < 1.15。
- 措施：
  - 提高进风/进水温度 (暖水冷却)。
  - 利用余热回收 (供暖)。
  - 动态能耗管理 (基于作业调度的降频/休眠)。

**供电系统**
- 高压直流 (HVDC) 供电，减少转换损耗。
- 机柜母线槽 (Busbar) 替代传统 PDU，支持更大电流。

## 本章小结

集群架构设计是HPC系统建设的关键环节。本章详细介绍了：

1. **集群类型与选择**：不同用途的集群特点和适用场景
2. **节点配置策略**：硬件选型原则和软件环境配置
3. **网络拓扑设计**：网络架构原则和典型拓扑结构
4. **存储架构规划**：存储层次设计和并行文件系统选择

合理的架构设计能够确保HPC系统的高性能、高可用性和良好的扩展性。在实际规划中，需要根据具体的应用需求、预算限制和未来发展规划，制定最适合的技术方案。