# 第6章 作业调度系统

## 6.1 SLURM调度器详解

### SLURM基础概念

#### 核心组件

**SLURM架构**
```
┌─────────────────────────────────────────────────────────────┐
│                    用户界面层                                 │
│  ┌────────────┐  ┌────────────┐  ┌────────────┐             │
│  │   sbatch   │  │   srun     │  │   squeue   │             │
│  └────────────┘  └────────────┘  └────────────┘             │
└─────────────────────────────────────────────────────────────┘
┌─────────────────────────────────────────────────────────────┐
│                    控制层                                     │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │              SLURM Controller (slurmctld)               │ │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐  │ │
│  │  │  Scheduler │  │  Database│  │  Plugin │  │  REST   │  │ │
│  │  │  Module │  │  (slurmdbd)│ │  System │  │  API    │  │ │
│  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘  │ │
│  └─────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────┘
┌─────────────────────────────────────────────────────────────┐
│                    计算资源层                                 │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐        │
│  │  Login  │  │  Compute│  │  Login  │  │  Compute│        │
│  │  Node   │  │  Nodes  │  │  Node   │  │  Nodes  │        │
│  │  (1-2)  │  │  (100+) │  │  (1-2)  │  │  (100+) │        │
│  └─────────┘  └─────────┘  └─────────┘  └─────────┘        │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐        │
│  │  GPU    │  │  GPU    │  │  GPU    │  │  GPU    │        │
│  │  Nodes  │  │  Nodes  │  │  Nodes  │  │  Nodes  │        │
│  │  (10+)  │  │  (10+)  │  │  (10+)  │  │  (10+)  │        │
│  └─────────┘  └─────────┘  └─────────┘  └─────────┘        │
└─────────────────────────────────────────────────────────────┘
```

#### 基本概念

**作业（Job）**
- 用户提交的计算任务
- 包含资源需求和执行命令
- 有唯一的作业ID

**分区（Partition）**
- 计算资源的逻辑分组
- 可以有不同的配置策略
- 类似于PBS中的队列

**节点（Node）**
- 物理或虚拟计算资源
- 包含CPU、内存、GPU等资源
- 可以属于多个分区

### SLURM配置文件

#### 主配置文件（slurm.conf）

```bash
# SLURM配置文件示例
# /etc/slurm/slurm.conf

# 基本配置
ClusterName=hpc-cluster
ControlMachine=master-node
ControlAddr=192.168.1.10
SlurmctldPort=6817
SlurmdPort=6818

# 节点配置 (支持 GRES/TRES)
NodeName=compute-[001-100] CPUs=128 RealMemory=512000 State=UNKNOWN
NodeName=gpu-[001-020] CPUs=64 RealMemory=1000000 Gres=gpu:h100:8 State=UNKNOWN
NodeName=ai-node-[01-10] CPUs=96 RealMemory=2000000 Gres=gpu:h100:8,nic:8 State=UNKNOWN

# 分区配置
PartitionName=compute Nodes=compute-[001-100] Default=YES MaxTime=7-00:00:00 State=UP
PartitionName=gpu Nodes=gpu-[001-020] Default=NO MaxTime=3-00:00:00 State=UP OverSubscribe=NO
PartitionName=interactive Nodes=compute-[001-005] Default=NO MaxTime=04:00:00 State=UP

# GPU 资源配置 (gres.conf)
# Name=gpu Type=h100 File=/dev/nvidia[0-7] Cores=0-7,64-71

# 调度配置 (加强 GPU/AI 支持)
SchedulerType=sched/backfill
SelectType=select/cons_tres  # 使用 cons_tres 替代 cons_res
SelectTypeParameters=CR_Core_Memory,CR_CORE_DEFAULT_DIST_BLOCK
SchedulerParameters=bf_continue,bf_interval=30,bf_window=1440,sched_max_job_start=50

# cgroup v2 配置
ProctrackType=proctrack/cgroup
TaskPlugin=task/cgroup,task/affinity
```

#### 数据库配置（slurmdbd.conf）

```bash
# SLURM数据库配置
# /etc/slurm/slurmdbd.conf

# 数据库连接
AuthType=auth/munge
AuthInfo=/var/run/munge/munge.socket.2
DbdHost=localhost
DbdPort=6819
DbdUser=slurm
DbdPass=slurm_password
StorageHost=localhost
StorageLoc=slurm_acct_db
StoragePass=slurm_password
StorageUser=slurm
StorageType=accounting_storage/mysql

# 日志配置
LogFile=/var/log/slurm/slurmdbd.log
DebugLevel=info
```

### 作业提交与管理

#### 基本作业提交

```bash
# 1. 简单作业提交
sbatch job_script.sh

# 2. 交互式作业
salloc -N 1 -n 4 --time=01:00:00
srun --pty /bin/bash

# 3. 直接运行
srun -N 1 -n 4 --time=01:00:00 ./my_program

# 4. 批量作业脚本
#!/bin/bash
#SBATCH --job-name=test_job
#SBATCH --output=output_%j.log
#SBATCH --error=error_%j.log
#SBATCH --partition=compute
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=32
#SBATCH --time=02:00:00
#SBATCH --mail-type=ALL
#SBATCH --mail-user=user@hpc.edu.cn

# 加载模块
module load openmpi/4.1.4
module load gcc/11.2.0

# 运行程序
mpirun -np 64 ./my_mpi_program
```

#### 高级作业选项

```bash
# 1. 资源指定
#SBATCH --cpus-per-task=8        # 每个任务的CPU核心数
#SBATCH --mem=32G                # 内存限制
#SBATCH --gres=gpu:h100:2        # 请求 2 块 H100 GPU
#SBATCH --gpus-per-task=1        # 每个任务绑定 1 块 GPU

# 2. 容器化支持 (Pyxis + Enroot)
#SBATCH --container-image=nvidia/pytorch:24.01-py3
#SBATCH --container-mounts=/data:/data

# 3. 约束与排他
#SBATCH --constraint=ib_ndr    # 网络约束
#SBATCH --exclusive            # 独占节点 (AI 训练常用)

# 4. 依赖关系 (Pipeline)
#SBATCH --dependency=afterok:12345
```

### 作业监控与管理

#### 作业状态查询

```bash
# 1. 查看作业队列
squeue
squeue -u username
squeue -p partition_name

# 2. 查看详细信息
squeue -l
squeue --format="%.18i %.9P %.8j %.8u %.2t %.10M %.6D %R"

# 3. 查看作业详情
scontrol show job 12345
scontrol show jobid -dd 12345

# 4. 查看节点状态
sinfo
sinfo -l
sinfo -N -l

# 5. 查看分区信息
sinfo -p partition_name
```

#### 作业管理操作

```bash
# 1. 取消作业
scancel 12345
scancel -u username
scancel -t pending

# 2. 挂起和恢复
scontrol suspend 12345
scontrol resume 12345

# 3. 重新排队
scontrol requeue 12345

# 4. 修改作业
scontrol update jobid=12345 TimeLimit=04:00:00
scontrol update jobid=12345 Priority=2000

# 5. 查看作业输出
scontrol read_file 12345
scontrol listpids 12345
```

### SLURM高级功能

#### 资源管理

```bash
# 1. 节点管理
scontrol update nodename=compute-001 state=drain reason="maintenance"
scontrol update nodename=compute-001 state=resume

# 2. 分区管理
scontrol update partitionname=compute MaxTime=48:00:00
scontrol create partition name=test nodes=node001,002

# 3. QoS管理
scontrol create qos name=premium
scontrol update qosname=premium Priority=10000

# 4. 账户管理
sacctmgr create account name=project1
sacctmgr create user name=user1 account=project1
```

#### 性能监控

```bash
# 1. 作业性能统计
sacct -j 12345 --format=JobID,JobName,Partition,AllocCPUS,Elapsed,MaxRSS,MaxVMSize
sacct -u username --format=User,JobID,Partition,Elapsed,CPUTime

# 2. 系统利用率
sreport cluster Allocated -t hour
sreport cluster Utilization -t hour

# 3. 资源使用统计
sreport license Utilization -t hour
sreport filesystem Utilization -t hour
```

#### 故障诊断

```bash
# 1. 检查服务状态
systemctl status slurmctld
systemctl status slurmd

# 2. 查看日志
tail -f /var/log/slurm/slurmctld.log
tail -f /var/log/slurm/slurmd.log

# 3. 检查配置
slurmd -C
scontrol show config

# 4. 测试连接
scontrol ping
```

## 6.2 其他调度器简介 (PBS/LSF/Kubernetes)

传统 HPC 环境中 PBS 和 LSF 仍有一席之地，但由 Slurm 主导。新兴的 AI 平台倾向于使用 Kubernetes。

### PBS Pro / OpenPBS
- **特点**: 商业支持完善，历史悠久
- **适用**: 传统工业仿真 (CAE/EDA)

### Kubernetes (云原生 AI 调度)
- **特点**: 容器编排标准，微服务架构
- **调度增强**:
  - **Volcano**: 批处理调度插件，支持 Gang Scheduling (Gang 调度)
  - **Kueue**: 队列管理系统，支持多租户配额
- **适用**: AI 推理服务、云原生训练任务 (Kubeflow)


## 6.4 调度策略优化

### 调度算法

#### 先来先服务（FCFS）
```bash
# 简单公平调度
# 优点：简单、公平
# 缺点：资源利用率低、响应时间长
```

#### 最短作业优先（SJF）
```bash
# 优化响应时间
# 优点：平均等待时间最短
# 缺点：可能造成饥饿
```

#### 优先级调度
```bash
# 基于优先级的调度
# 优点：灵活、可定制
# 缺点：需要合理设置优先级
```

#### 回填调度（Backfilling）
```bash
# SLURM回填配置
SchedulerParameters=bf_continue,bf_window=600

# 优点：提高资源利用率
# 缺点：实现复杂
```

### 资源分配策略

#### 节点分配策略

```bash
# 1. 打包策略（Pack）
# 尽量使用少的节点
#SBATCH --exclusive
#SBATCH --constraint=fast

# 2. 散布策略（Scatter）
# 分散到多个节点
#SBATCH --spread-job

# 3. 平衡策略（Balance）
# 平衡负载
#SBATCH --distribution=cyclic
```

#### CPU分配策略

```bash
# 1. 核心绑定
#SBATCH --cpu-bind=cores
#SBATCH --hint=compute_bound

# 2. 内存绑定
#SBATCH --hint=memory_bound

# 3. NUMA优化
numactl --membind=0 --cpunodebind=0 ./program
```

### 性能优化策略

#### 队列设计

```bash
# 1. 分级队列
# 短作业队列
PartitionName=short Nodes=compute-[001-020] MaxTime=02:00:00 State=UP

# 长作业队列
PartitionName=long Nodes=compute-[021-100] MaxTime=168:00:00 State=UP

# GPU队列
PartitionName=gpu Nodes=gpu-[001-020] MaxTime=24:00:00 State=UP
```

#### 资源预留

```bash
# 1. 维护预留
scontrol update nodename=compute-001 state=maintenance

# 2. 高优先级预留
scontrol create reservation name=urgent starttime=now duration=3600 users=root
```

#### 负载均衡

```bash
# 1. 动态负载均衡
# 配置负载均衡插件
SchedulerParameters=load_balance

# 2. 智能调度
# 基于历史数据的调度
SchedulerParameters=predictive
```

### 故障处理

#### 节点故障处理

```bash
# 1. 自动故障检测
scontrol update nodename=compute-001 state=drain reason="hardware_failure"

# 2. 作业重调度
# 配置自动重调度
RestartWaitTime=300
Resubmit=2
```

#### 网络故障处理

```bash
# 1. 网络分区处理
# 配置网络检测
HealthCheckInterval=60
HealthCheckProgram=/opt/slurm/bin/check_network.sh

# 2. 作业迁移
# 配置作业迁移
MigrateJobs=YES
```

## 本章小结

作业调度系统是HPC集群的核心组件，负责资源管理和作业调度。本章详细介绍了：

1. **SLURM调度器**：现代HPC主流调度器，功能丰富，配置灵活
2. **PBS Pro**：传统调度器，稳定可靠，企业级应用
3. **LSF调度器**：IBM产品，适用于大型企业环境
4. **调度策略优化**：提高资源利用率和作业吞吐量的方法

选择合适的调度器需要考虑集群规模、应用特点、管理复杂度等因素。在实际部署中，需要根据具体需求进行配置优化，确保系统的高效运行。