# ç¬¬13ç«  æ•…éšœè¯Šæ–­ä¸æ’é™¤

> HPCé›†ç¾¤æ•…éšœè¯Šæ–­ä¸åº”æ€¥å“åº”å®æˆ˜æŒ‡å—

## ğŸš¨ æœ¬ç« æ¦‚è¿°

æ•…éšœè¯Šæ–­æ˜¯HPCè¿ç»´å·¥ç¨‹å¸ˆçš„æ ¸å¿ƒæŠ€èƒ½ã€‚æœ¬ç« å°†æ·±å…¥ä»‹ç»HPCé›†ç¾¤ä¸­å¸¸è§æ•…éšœçš„è¯†åˆ«ã€è¯Šæ–­å’Œæ’é™¤æ–¹æ³•ï¼Œå»ºç«‹ç³»ç»ŸåŒ–çš„æ•…éšœå¤„ç†æµç¨‹å’Œåº”æ€¥å“åº”æœºåˆ¶ã€‚

**å­¦ä¹ ç›®æ ‡ï¼š**
- æŒæ¡HPCé›†ç¾¤å¸¸è§æ•…éšœç±»å‹å’Œç‰¹å¾
- å­¦ä¼šç³»ç»ŸåŒ–çš„æ•…éšœè¯Šæ–­æµç¨‹
- ç†Ÿç»ƒä½¿ç”¨å„ç§æ•…éšœè¯Šæ–­å·¥å…·å’ŒæŠ€å·§
- å»ºç«‹å®Œå–„çš„åº”æ€¥å“åº”é¢„æ¡ˆä½“ç³»

## ğŸ“Š æ•…éšœåˆ†ç±»ä½“ç³»

### 13.1 å¸¸è§æ•…éšœç±»å‹

#### 13.1.1 ç¡¬ä»¶æ•…éšœ

**è®¡ç®—èŠ‚ç‚¹æ•…éšœï¼š**
- CPUè¿‡çƒ­æˆ–æ•…éšœ
- å†…å­˜æ¡æŸå
- ç¡¬ç›˜æ•…éšœï¼ˆSSD/HDDï¼‰
- ç½‘å¡æ•…éšœ
- ç”µæºæ•…éšœ

**å­˜å‚¨ç³»ç»Ÿæ•…éšœï¼š**
- RAIDé˜µåˆ—æ•…éšœ
- å­˜å‚¨æ§åˆ¶å™¨æ•…éšœ
- ç½‘ç»œå­˜å‚¨è¿æ¥é—®é¢˜
- å¹¶è¡Œæ–‡ä»¶ç³»ç»Ÿæ•…éšœ

**ç½‘ç»œè®¾å¤‡æ•…éšœï¼š**
- äº¤æ¢æœºæ•…éšœ
- InfiniBandé€‚é…å™¨æ•…éšœ
- å…‰çº¤é€šé“æ•…éšœ
- ç½‘ç»œçº¿ç¼†é—®é¢˜

**æ•…éšœè¯Šæ–­è„šæœ¬ - ç¡¬ä»¶å¥åº·æ£€æŸ¥ï¼š**

```bash
#!/bin/bash
# /opt/troubleshooting/hardware_check.sh

HOSTNAME=$(hostname)
LOG_FILE="/var/log/hardware_health.log"
TIMESTAMP=$(date '+%Y-%m-%d %H:%M:%S')

echo "=== Hardware Health Check: $TIMESTAMP ===" >> $LOG_FILE

# CPU/GPU æ¸©åº¦æ£€æŸ¥
echo "Thermal Status:" >> $LOG_FILE
if command -v sensors &> /dev/null; then
    sensors | grep -E "Package|Core" >> $LOG_FILE
fi
if command -v nvidia-smi &> /dev/null; then
    nvidia-smi --query-gpu=index,temperature.gpu,utilization.gpu,power.draw --format=csv,noheader >> $LOG_FILE
fi

# ç¡¬ä»¶é”™è¯¯æ£€æŸ¥ (BMC/IPMI)
echo "BMC System Event Log:" >> $LOG_FILE
if command -v ipmitool &> /dev/null; then
    ipmitool sel list | tail -20 >> $LOG_FILE
fi
```

**å­˜å‚¨æ•…éšœè¯Šæ–­ï¼š**

```bash
#!/bin/bash
# /opt/troubleshooting/storage_check.sh

echo "=== Storage System Check ==="

# æ£€æŸ¥æ–‡ä»¶ç³»ç»ŸæŒ‚è½½çŠ¶æ€
echo "Mounted filesystems:"
mount | grep -E "lustre|gpfs|nfs" || echo "No parallel filesystems mounted"

# æ£€æŸ¥LUSTREçŠ¶æ€
if mount | grep -q lustre; then
    echo "LUSTRE filesystem status:"
    lfs df -h
    lfs check servers
fi

# æ£€æŸ¥RAIDçŠ¶æ€
echo "RAID status:"
if command -v mdadm &> /dev/null; then
    mdadm --detail /dev/md* 2>/dev/null || echo "No RAID arrays found"
fi

# æ£€æŸ¥ç£ç›˜I/Oé”™è¯¯
echo "Disk I/O errors:"
for disk in /dev/sd* /dev/nvme*; do
    if [ -b "$disk" ]; then
        echo "Checking $disk errors:"
        dmesg | grep -i "$disk.*error" | tail -5
    fi
done

# å­˜å‚¨æ€§èƒ½æ£€æŸ¥
echo "Storage performance:"
iostat -x 1 1 | grep -E "Device|sd|nvme"
```

#### 13.1.2 è½¯ä»¶æ•…éšœ

**æ“ä½œç³»ç»Ÿæ•…éšœï¼š**
- å†…æ ¸å´©æºƒï¼ˆKernel Panicï¼‰
- ç³»ç»Ÿå¯åŠ¨å¤±è´¥
- æœåŠ¡è¿›ç¨‹å¼‚å¸¸ç»ˆæ­¢
- ç³»ç»Ÿèµ„æºè€—å°½

**æ··åˆè°ƒåº¦ç³»ç»Ÿæ•…éšœ (Slurm + K8s):**
- Slurm: `sbatch: error: Batch job submission failed` (æ•°æ®åº“/é…ç½®)
- Kubernetes: `CrashLoopBackOff`, `ImagePullBackOff` (å®¹å™¨/ç½‘ç»œ)
- èµ„æºç«äº‰: K8s Pod å ç”¨ Slurm é¢„ç•™ CPU æ ¸å¿ƒ

**ç½‘ç»œæœåŠ¡æ•…éšœï¼š**
- DNSè§£æå¤±è´¥
- NFSæŒ‚è½½å¤±è´¥
- SSHè¿æ¥é—®é¢˜
- ç½‘ç»œé…ç½®é”™è¯¯

**æ•…éšœè¯Šæ–­è„šæœ¬ - è½¯ä»¶æœåŠ¡æ£€æŸ¥ï¼š**

```bash
#!/bin/bash
# /opt/troubleshooting/service_check.sh

echo "=== Service Status Check ==="

# æ£€æŸ¥å…³é”®ç³»ç»ŸæœåŠ¡
SERVICES=("sshd" "slurmctld" "slurmd" "nfs-server" "named" "ntpd")

for service in "${SERVICES[@]}"; do
    if systemctl is-active --quiet $service 2>/dev/null; then
        echo "âœ“ $service: Running"
    else
        echo "âœ— $service: Not running"
        # å°è¯•é‡å¯æœåŠ¡
        if systemctl restart $service 2>/dev/null; then
            echo "  Attempted restart: Success"
        else
            echo "  Attempted restart: Failed"
        fi
    fi
done

# æ£€æŸ¥SLURMçŠ¶æ€
echo ""
echo "SLURM cluster status:"
if command -v sinfo &> /dev/null; then
    sinfo -h -o "%P %a %l %n %D" | head -10
else
    echo "SLURM not available"
fi

# æ£€æŸ¥ä½œä¸šçŠ¶æ€
echo ""
echo "Recent job submissions:"
if command -v squeue &> /dev/null; then
    squeue -u $(whoami) -h -o "%j %T %M" | head -5
else
    echo "SLURM queue not available"
fi

# æ£€æŸ¥ç³»ç»Ÿèµ„æº
echo ""
echo "System resources:"
echo "Memory usage: $(free -h | awk 'NR==2{printf "%.2f%%", $3*100/$2}')"
echo "Disk usage: $(df -h / | awk 'NR==2{print $5}')"
echo "Load average: $(uptime | awk -F'load average:' '{print $2}')"
```

#### 13.1.3 ç½‘ç»œæ•…éšœ

**ç½‘ç»œè¿æ¥é—®é¢˜ï¼š**
- èŠ‚ç‚¹é—´é€šä¿¡å¤±è´¥
- ç½‘ç»œå»¶è¿Ÿè¿‡é«˜
- å¸¦å®½å—é™
- ç½‘ç»œåˆ†åŒº

**DNSå’ŒåŸŸåè§£æï¼š**
- åŸŸåè§£æå¤±è´¥
- DNSæœåŠ¡å™¨ä¸å¯ç”¨
- ä¸»æœºåé…ç½®é”™è¯¯

**InfiniBandæ•…éšœï¼š**
- IBé“¾è·¯æ•…éšœ
- å­ç½‘ç®¡ç†å™¨é—®é¢˜
- QoSé…ç½®é”™è¯¯

**ç½‘ç»œæ•…éšœè¯Šæ–­å·¥å…·ï¼š**

```bash
#!/bin/bash
# /opt/troubleshooting/network_check.sh

echo "=== Network Diagnostics ==="

# åŸºæœ¬è¿é€šæ€§æµ‹è¯•
echo "Testing connectivity to cluster nodes:"
for node in compute-node-{01..10}; do
    if ping -c 1 -W 1 $node &> /dev/null; then
        echo "âœ“ $node: OK"
    else
        echo "âœ— $node: Failed"
    fi
done

# DNSè§£ææµ‹è¯•
echo ""
echo "DNS resolution test:"
nslookup google.com
if [ $? -eq 0 ]; then
    echo "âœ“ DNS resolution: Working"
else
    echo "âœ— DNS resolution: Failed"
fi

# InfiniBand æ·±åº¦è¯Šæ–­
echo ""
echo "InfiniBand Deep Diagnostic:"
if command -v ibdiagnet &> /dev/null; then
    ibdiagnet -r --skip all --check_dugs  # å¿«é€Ÿæ£€æŸ¥æ‹“æ‰‘å’Œé”™è¯¯è®¡æ•°
    grep -E "Error|SymbolError" /var/tmp/ibdiagnet2/ibdiagnet2.log
else
    echo "ibdiagnet tool not found"
fi

# RDMA è¿é€šæ€§æµ‹è¯•
echo ""
echo "RDMA Connectivity (ib_write_bw):"
if command -v ib_write_bw &> /dev/null; then
    # éœ€é…åˆæœåŠ¡ç«¯è¿è¡Œï¼Œæ­¤å¤„ä»…æ£€æŸ¥è®¾å¤‡çŠ¶æ€
    ibv_devinfo | grep -E "hca_id|transport|state"
fi
```

### 13.2 æ•…éšœè¯Šæ–­æµç¨‹

#### 13.2.1 æ•…éšœè¯Šæ–­åŸåˆ™

**5W1Håˆ†ææ³•ï¼š**
- **What**: å‘ç”Ÿäº†ä»€ä¹ˆæ•…éšœï¼Ÿ
- **When**: ä»€ä¹ˆæ—¶å€™å‘ç”Ÿçš„ï¼Ÿ
- **Where**: åœ¨å“ªé‡Œå‘ç”Ÿçš„ï¼Ÿ
- **Who**: è°å‘ç°çš„ï¼Ÿå½±å“è°ï¼Ÿ
- **Why**: å¯èƒ½çš„åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ
- **How**: å¦‚ä½•å‘ç”Ÿçš„ï¼Ÿå¦‚ä½•è§£å†³ï¼Ÿ

**æ•…éšœè¯Šæ–­æµç¨‹å›¾ï¼š**

```
æ•…éšœå‘ç”Ÿ
    â†“
ä¿¡æ¯æ”¶é›†
    â†“
åˆæ­¥åˆ¤æ–­
    â†“
è¯¦ç»†è¯Šæ–­
    â†“
å®šä½æ ¹æº
    â†“
å®æ–½ä¿®å¤
    â†“
éªŒè¯æ•ˆæœ
    â†“
è®°å½•æ€»ç»“
```

#### 13.2.2 ä¿¡æ¯æ”¶é›†ç­–ç•¥

**æ•…éšœä¿¡æ¯æ”¶é›†è„šæœ¬ï¼š**

```bash
#!/bin/bash
# /opt/troubleshooting/incident_collect.sh

INCIDENT_ID="INC-$(date +%Y%m%d-%H%M%S)"
COLLECT_DIR="/tmp/$INCIDENT_ID"
mkdir -p $COLLECT_DIR

echo "=== Incident Collection: $INCIDENT_ID ===" > $COLLECT_DIR/incident_info.txt
echo "Timestamp: $(date)" >> $COLLECT_DIR/incident_info.txt
echo "Hostname: $(hostname)" >> $COLLECT_DIR/incident_info.txt
echo "User: $(whoami)" >> $COLLECT_DIR/incident_info.txt
echo "Description: $1" >> $COLLECT_DIR/incident_info.txt

# æ”¶é›†ç³»ç»Ÿä¿¡æ¯
echo "=== System Information ===" > $COLLECT_DIR/system_info.txt
uname -a >> $COLLECT_DIR/system_info.txt
cat /etc/os-release >> $COLLECT_DIR/system_info.txt
cat /proc/meminfo | head -10 >> $COLLECT_DIR/system_info.txt

# æ”¶é›†è¿›ç¨‹ä¿¡æ¯
echo "=== Process Information ===" > $COLLECT_DIR/process_info.txt
ps aux | head -20 >> $COLLECT_DIR/process_info.txt
top -bn1 | head -20 >> $COLLECT_DIR/process_info.txt

# æ”¶é›†ç½‘ç»œä¿¡æ¯
echo "=== Network Information ===" > $COLLECT_DIR/network_info.txt
ip addr show >> $COLLECT_DIR/network_info.txt
ip route show >> $COLLECT_DIR/network_info.txt
ss -tuln >> $COLLECT_DIR/network_info.txt

# æ”¶é›†å­˜å‚¨ä¿¡æ¯
echo "=== Storage Information ===" > $COLLECT_DIR/storage_info.txt
df -h >> $COLLECT_DIR/storage_info.txt
mount | grep -E "lustre|gpfs|nfs" >> $COLLECT_DIR/storage_info.txt

# æ”¶é›†æ—¥å¿—ä¿¡æ¯
echo "=== Log Information ===" > $COLLECT_DIR/log_summary.txt
echo "Recent system messages:" >> $COLLECT_DIR/log_summary.txt
tail -50 /var/log/messages >> $COLLECT_DIR/log_summary.txt

echo "Recent kernel messages:" >> $COLLECT_DIR/log_summary.txt
dmesg | tail -50 >> $COLLECT_DIR/log_summary.txt

# æ”¶é›†SLURMä¿¡æ¯
if command -v sinfo &> /dev/null; then
    echo "=== SLURM Information ===" > $COLLECT_DIR/slurm_info.txt
    sinfo >> $COLLECT_DIR/slurm_info.txt
    squeue -h >> $COLLECT_DIR/slurm_info.txt
fi

# åˆ›å»ºè¯Šæ–­æŠ¥å‘Š
echo "=== Initial Diagnosis ===" > $COLLECT_DIR/diagnosis.txt
echo "Collected information for incident: $INCIDENT_ID" >> $COLLECT_DIR/diagnosis.txt
echo "Collection completed at: $(date)" >> $COLLECT_DIR/diagnosis.txt
echo "Files collected:" >> $COLLECT_DIR/diagnosis.txt
ls -la $COLLECT_DIR >> $COLLECT_DIR/diagnosis.txt

echo "Incident data collected in: $COLLECT_DIR"
echo "Use this directory for further analysis."
```

#### 13.2.3 æ•…éšœæ ‘åˆ†æï¼ˆFTAï¼‰

**å»ºç«‹æ•…éšœæ ‘çš„æ­¥éª¤ï¼š**

1. **å®šä¹‰é¡¶äº‹ä»¶**ï¼šæ˜ç¡®è¦åˆ†æçš„æ•…éšœç°è±¡
2. **è¯†åˆ«ä¸­é—´äº‹ä»¶**ï¼šå¯¼è‡´é¡¶äº‹ä»¶çš„ç›´æ¥åŸå› 
3. **ç¡®å®šåŸºæœ¬äº‹ä»¶**ï¼šä¸å¯å†åˆ†è§£çš„æ ¹æœ¬åŸå› 
4. **æ„å»ºé€»è¾‘å…³ç³»**ï¼šä½¿ç”¨AND/ORé—¨è¿æ¥äº‹ä»¶
5. **åˆ†ææœ€å°å‰²é›†**ï¼šæ‰¾å‡ºå¯¼è‡´æ•…éšœçš„æœ€å°äº‹ä»¶ç»„åˆ

**å¸¸è§æ•…éšœçš„æ•…éšœæ ‘ç¤ºä¾‹ï¼š**

```
é¡¶äº‹ä»¶ï¼šè®¡ç®—èŠ‚ç‚¹æ— æ³•æäº¤ä½œä¸š

    â”œâ”€â”€ AND â”€â”€ èŠ‚ç‚¹æ— æ³•è®¿é—®
    â”‚         â”œâ”€â”€ OR â”€â”€ ç½‘ç»œè¿æ¥æ•…éšœ
    â”‚         â”‚         â”œâ”€â”€ ç½‘çº¿æ¾åŠ¨
    â”‚         â”‚         â”œâ”€â”€ äº¤æ¢æœºæ•…éšœ
    â”‚         â”‚         â””â”€â”€ ç½‘å¡æ•…éšœ
    â”‚         â””â”€â”€ OR â”€â”€ ç³»ç»Ÿå¯åŠ¨æ•…éšœ
    â”‚                   â”œâ”€â”€ BIOSè®¾ç½®é”™è¯¯
    â”‚                   â”œâ”€â”€ ç¡¬ç›˜æ•…éšœ
    â”‚                   â””â”€â”€ å†…å­˜æ•…éšœ
    â”‚
    â””â”€â”€ OR â”€â”€ SLURMæœåŠ¡å¼‚å¸¸
              â”œâ”€â”€ slurmctldè¿›ç¨‹å¼‚å¸¸
              â”œâ”€â”€ é…ç½®æ–‡ä»¶é”™è¯¯
              â””â”€â”€ æ•°æ®åº“è¿æ¥å¤±è´¥
```

### 13.3 æ—¥å¿—åˆ†ææŠ€å·§

#### 13.3.1 æ—¥å¿—åˆ†ç±»ä¸ä½ç½®

**ç³»ç»Ÿæ—¥å¿—ï¼š**
- `/var/log/messages` - ç³»ç»Ÿæ¶ˆæ¯
- `/var/log/syslog` - ç³»ç»Ÿæ—¥å¿—
- `/var/log/dmesg` - å†…æ ¸æ¶ˆæ¯
- `/var/log/boot.log` - å¯åŠ¨æ—¥å¿—

**åº”ç”¨æ—¥å¿—ï¼š**
- `/var/log/slurm/` - SLURMæ—¥å¿—
- `/var/log/httpd/` - WebæœåŠ¡å™¨æ—¥å¿—
- `/var/log/mysql/` - æ•°æ®åº“æ—¥å¿—
- `/var/log/named/` - DNSæœåŠ¡å™¨æ—¥å¿—

**HPCç‰¹å®šæ—¥å¿—ï¼š**
- `/var/log/lustre/` - LUSTREæ–‡ä»¶ç³»ç»Ÿæ—¥å¿—
- `/var/log/mpi/` - MPIåº“æ—¥å¿—
- `/var/log/infiniband/` - InfiniBandæ—¥å¿—

#### 13.3.2 æ—¥å¿—åˆ†æå·¥å…·

**æ—¥å¿—åˆ†æè„šæœ¬ï¼š**

```bash
#!/bin/bash
# /opt/troubleshooting/log_analyzer.sh

LOG_FILE=$1
TIME_WINDOW=${2:-"1 hour ago"}

if [ -z "$LOG_FILE" ]; then
    echo "Usage: $0 <log_file> [time_window]"
    echo "Example: $0 /var/log/messages '2 hours ago'"
    exit 1
fi

echo "=== Log Analysis for $LOG_FILE ==="
echo "Time window: $TIME_WINDOW to now"
echo ""

# é”™è¯¯å…³é”®è¯æœç´¢
echo "=== Error Keywords ==="
grep -i "error\|fail\|critical\|panic" "$LOG_FILE" | \
    grep "$(date --date="$TIME_WINDOW" '+%b %d')" | \
    head -20

# ç³»ç»Ÿå¯åŠ¨ç›¸å…³æ—¥å¿—
echo ""
echo "=== Boot-related Messages ==="
grep -i "boot\|startup\|init" "$LOG_FILE" | \
    grep "$(date --date="$TIME_WINDOW" '+%b %d')" | \
    head -10

# ç½‘ç»œç›¸å…³æ—¥å¿—
echo ""
echo "=== Network-related Messages ==="
grep -i "network\|connect\|disconnect\|timeout" "$LOG_FILE" | \
    grep "$(date --date="$TIME_WINDOW" '+%b %d')" | \
    head -10

# SLURMç›¸å…³æ—¥å¿—
if grep -q "slurm" "$LOG_FILE"; then
    echo ""
    echo "=== SLURM Messages ==="
    grep -i "slurm\|job\|queue" "$LOG_FILE" | \
        grep "$(date --date="$TIME_WINDOW" '+%b %d')" | \
        head -10
fi

# ç»Ÿè®¡åˆ†æ
echo ""
echo "=== Log Statistics ==="
TOTAL_LINES=$(wc -l < "$LOG_FILE")
ERROR_LINES=$(grep -c -i "error\|fail\|critical" "$LOG_FILE")
WARNING_LINES=$(grep -c -i "warning\|warn" "$LOG_FILE")

echo "Total lines: $TOTAL_LINES"
echo "Error lines: $ERROR_LINES"
echo "Warning lines: $WARNING_LINES"
echo "Error rate: $(echo "scale=2; $ERROR_LINES * 100 / $TOTAL_LINES" | bc)%"

# æ—¶é—´åˆ†å¸ƒåˆ†æ
echo ""
echo "=== Time Distribution (Last 24 Hours) ==="
for hour in {0..23}; do
    count=$(grep "$(date '+%b %d') $(printf "%02d" $hour)" "$LOG_FILE" | wc -l)
    echo "$(printf "%02d" $hour):00 - $count entries"
done
```

**é«˜çº§æ—¥å¿—åˆ†æå·¥å…·ï¼š**

```bash
#!/bin/bash
# /opt/troubleshooting/advanced_log_analysis.sh

# å®æ—¶æ—¥å¿—ç›‘æ§
tail -f /var/log/messages | grep -E "(error|fail|critical)" &
TAIL_PID=$!

# å¤šæ—¥å¿—æ–‡ä»¶å…³è”åˆ†æ
echo "=== Multi-log Correlation Analysis ==="
for log_file in /var/log/messages /var/log/syslog /var/log/dmesg; do
    if [ -f "$log_file" ]; then
        echo "--- $log_file ---"
        # æŸ¥æ‰¾ç‰¹å®šæ—¶é—´æ®µçš„é”™è¯¯
        grep -E "$(date '+%b %d %H')" "$log_file" | \
            grep -i error | \
            head -5
    fi
done

# æ—¥å¿—æ¨¡å¼è¯†åˆ«
echo ""
echo "=== Pattern Recognition ==="
# æŸ¥æ‰¾é‡å¤å‡ºç°çš„é”™è¯¯æ¨¡å¼
awk '/error|fail/ {print $0}' /var/log/messages | \
    sort | uniq -c | sort -nr | head -10

# ç­‰å¾…ç”¨æˆ·ä¸­æ–­
echo "Press Ctrl+C to stop real-time monitoring"
wait $TAIL_PID
```

#### 13.3.3 æ—¥å¿—è½®è½¬ç®¡ç†

**æ—¥å¿—è½®è½¬é…ç½®ï¼š**

```bash
# /etc/logrotate.d/hpc-cluster
/var/log/slurm/*.log {
    daily
    rotate 30
    compress
    delaycompress
    missingok
    notifempty
    create 644 slurm slurm
    postrotate
        /bin/systemctl reload slurmctld
    endscript
}

/var/log/lustre/*.log {
    weekly
    rotate 12
    compress
    delaycompress
    missingok
    notifempty
    create 644 root root
}
```

### 13.4 åº”æ€¥å“åº”é¢„æ¡ˆ

#### 13.4.1 åº”æ€¥å“åº”çº§åˆ«

**å“åº”çº§åˆ«å®šä¹‰ï¼š**

| çº§åˆ« | æè¿° | å“åº”æ—¶é—´ | è´Ÿè´£äºº |
|------|------|----------|--------|
| **P1 - ç´§æ€¥** | æ•´ä¸ªé›†ç¾¤ä¸å¯ç”¨ | 15åˆ†é’Ÿ | å€¼ç­ä¸»ç®¡ |
| **P2 - é«˜** | éƒ¨åˆ†èŠ‚ç‚¹æ•…éšœ | 1å°æ—¶ | é«˜çº§å·¥ç¨‹å¸ˆ |
| **P3 - ä¸­** | æ€§èƒ½ä¸‹é™ | 4å°æ—¶ | å·¥ç¨‹å¸ˆ |
| **P4 - ä½** | åŠŸèƒ½å¼‚å¸¸ | 24å°æ—¶ | æŠ€æœ¯æ”¯æŒ |

#### 13.4.2 åº”æ€¥å“åº”æµç¨‹

**P1çº§åˆ«åº”æ€¥å“åº”æµç¨‹ï¼š**

```bash
#!/bin/bash
# /opt/troubleshooting/emergency_response_p1.sh

echo "=== P1 Emergency Response Initiated ==="
echo "Time: $(date)"
echo "Incident: Cluster-wide outage"
echo ""

# 1. ç«‹å³é€šçŸ¥
echo "1. Notifying emergency team..."
mail -s "P1 Emergency: Cluster Outage" admin@hpc.example.com << EOF
EMERGENCY ALERT - P1 Level
Time: $(date)
Issue: Complete cluster outage
Action: Immediate response required
EOF

# 2. å¿«é€Ÿè¯Šæ–­
echo "2. Performing rapid diagnosis..."
/opt/troubleshooting/incident_collect.sh "Cluster outage"

# 3. åŸºç¡€è®¾æ–½æ£€æŸ¥
echo "3. Checking infrastructure..."
# æ£€æŸ¥ç”µæº
if [ -f /sys/class/power_supply/AC/online ]; then
    POWER_STATUS=$(cat /sys/class/power_supply/AC/online)
    echo "Power status: $POWER_STATUS"
    if [ "$POWER_STATUS" = "0" ]; then
        echo "âš ï¸ Power outage detected - check UPS and generators"
    fi
fi

# æ£€æŸ¥ç½‘ç»œæ ¸å¿ƒè®¾å¤‡
echo "Checking core network devices..."
for switch in core-sw-01 core-sw-02; do
    if ping -c 1 -W 1 $switch &> /dev/null; then
        echo "âœ“ $switch: Online"
    else
        echo "âœ— $switch: Offline"
    fi
done

# 4. æœåŠ¡æ¢å¤
echo "4. Attempting service recovery..."
systemctl restart slurmctld
systemctl restart slurmdbd

# 5. çŠ¶æ€æ›´æ–°
echo "5. Updating incident status..."
echo "Response initiated at $(date)" > /tmp/emergency_status.txt
echo "Team notified" >> /tmp/emergency_status.txt
echo "Initial diagnosis in progress" >> /tmp/emergency_status.txt

echo "=== Initial Response Complete ==="
echo "Monitor /tmp/emergency_status.txt for updates"
```

**P2çº§åˆ«åº”æ€¥å“åº”æµç¨‹ï¼š**

```bash
#!/bin/bash
# /opt/troubleshooting/emergency_response_p2.sh

echo "=== P2 Emergency Response Initiated ==="
echo "Time: $(date)"
echo "Issue: Partial node failure"
echo ""

# 1. é€šçŸ¥ç›¸å…³äººå‘˜
echo "1. Notifying team..."
mail -s "P2 Alert: Partial Node Failure" support@hpc.example.com << EOF
P2 Alert - Partial Node Failure
Time: $(date)
Issue: Some compute nodes are offline
Impact: Reduced cluster capacity
Action: Investigation in progress
EOF

# 2. æ•…éšœèŠ‚ç‚¹è¯†åˆ«
echo "2. Identifying failed nodes..."
FAILED_NODES=()
for node in compute-node-{01..20}; do
    if ! ping -c 1 -W 1 $node &> /dev/null; then
        FAILED_NODES+=($node)
        echo "âœ— $node: Failed"
    else
        echo "âœ“ $node: OK"
    fi
done

# 3. å½±å“è¯„ä¼°
echo "3. Assessing impact..."
TOTAL_NODES=20
FAILED_COUNT=${#FAILED_NODES[@]}
IMPACT_PERCENT=$((FAILED_COUNT * 100 / TOTAL_NODES))

echo "Failed nodes: $FAILED_COUNT/$TOTAL_NODES ($IMPACT_PERCENT%)"

if [ $IMPACT_PERCENT -gt 50 ]; then
    echo "âš ï¸ High impact detected - escalating to P1"
    # å‡çº§åˆ°P1çº§åˆ«
    /opt/troubleshooting/emergency_response_p1.sh
    exit 1
fi

# 4. æ•…éšœéš”ç¦»
echo "4. Isolating failed nodes..."
for node in "${FAILED_NODES[@]}"; do
    scontrol update NodeName=$node State=DOWN Reason="Hardware failure detected"
done

# 5. ç”¨æˆ·é€šçŸ¥
echo "5. Notifying users..."
if [ $FAILED_COUNT -gt 0 ]; then
    echo "Users with jobs on failed nodes will be notified"
    # è·å–å—å½±å“çš„ç”¨æˆ·
    squeue -o "%u %j %N" | grep -E "$(IFS='|'; echo "${FAILED_NODES[*]}")" | \
        awk '{print $1}' | sort | uniq | \
        while read user; do
            mail -s "Job Impact Notice" $user << EOF
Notice: Some compute nodes are currently offline.
Your running jobs may be affected.
New job submissions will be queued normally.
EOF
        done
fi

echo "=== P2 Response Complete ==="
```

#### 13.4.3 åº”æ€¥æ¼”ç»ƒ

**å®šæœŸåº”æ€¥æ¼”ç»ƒè®¡åˆ’ï¼š**

```bash
#!/bin/bash
# /opt/troubleshooting/drill_schedule.sh

echo "=== HPC Cluster Emergency Drill Schedule ==="
echo ""

# æœˆåº¦æ¼”ç»ƒ
echo "Monthly Drills:"
echo "- Power outage simulation (1st Monday)"
echo "- Network failure drill (2nd Tuesday)"
echo "- Storage system failure (3rd Wednesday)"
echo "- SLURM serviceæ•…éšœ (4th Thursday)"
echo ""

# å­£åº¦æ¼”ç»ƒ
echo "Quarterly Drills:"
echo "- Complete cluster failure (Full day)"
echo "- Data center evacuation (Coordination with facilities)"
echo "- Security incident response (Coordination with security team)"
echo ""

# å¹´åº¦æ¼”ç»ƒ
echo "Annual Drills:"
echo "- Disaster recovery (Site failover)"
echo "- Vendor coordination (Hardware replacement)"
echo "- Business continuity (Alternative workflows)"
echo ""

# æ¼”ç»ƒæ£€æŸ¥æ¸…å•
echo "=== Drill Checklist ==="
cat << 'EOF'
â–¡ Pre-drill briefing completed
â–¡ Stakeholders notified
â–¡ Backup systems verified
â–¡ Communication channels tested
â–¡ Documentation updated
â–¡ Participants trained
â–¡ Drill execution completed
â–¡ Post-drill review conducted
â–¡ Lessons learned documented
â–¡ Action items identified and assigned
EOF
```

**åº”æ€¥æ¼”ç»ƒè„šæœ¬ï¼š**

```bash
#!/bin/bash
# /opt/troubleshooting/emergency_drill.sh

DRILL_TYPE=$1
DRILL_DURATION=${2:-30}  # é»˜è®¤30åˆ†é’Ÿ

if [ -z "$DRILL_TYPE" ]; then
    echo "Usage: $0 <drill_type> [duration_minutes]"
    echo "Drill types: power-outage, network-failure, storage-failure, slurm-failure"
    exit 1
fi

echo "=== Emergency Drill: $DRILL_TYPE ==="
echo "Duration: $DRILL_DURATION minutes"
echo "Start time: $(date)"
echo ""

# è®°å½•æ¼”ç»ƒå¼€å§‹
echo "$(date),DRILL_START,$DRILL_TYPE,$DRILL_DURATION" >> /var/log/emergency_drills.log

case $DRILL_TYPE in
    "power-outage")
        echo "Simulating power outage..."
        # æ¨¡æ‹Ÿç”µæºæ•…éšœ
        echo "1. Simulating UPS failure"
        echo "2. Checking generator startup"
        echo "3. Testing graceful shutdown procedures"
        ;;
    "network-failure")
        echo "Simulating network failure..."
        # æ¨¡æ‹Ÿç½‘ç»œæ•…éšœ
        echo "1. Disconnecting primary network"
        echo "2. Testing failover to backup network"
        echo "3. Verifying node isolation procedures"
        ;;
    "storage-failure")
        echo "Simulating storage failure..."
        # æ¨¡æ‹Ÿå­˜å‚¨æ•…éšœ
        echo "1. Simulating LUSTRE server failure"
        echo "2. Testing data redundancy"
        echo "3. Verifying backup restoration"
        ;;
    "slurm-failure")
        echo "Simulating SLURM failure..."
        # æ¨¡æ‹Ÿè°ƒåº¦ç³»ç»Ÿæ•…éšœ
        echo "1. Stopping SLURM controller"
        echo "2. Testing manual job management"
        echo "3. Verifying database recovery"
        ;;
    *)
        echo "Unknown drill type: $DRILL_TYPE"
        exit 1
        ;;
esac

echo ""
echo "Drill execution time: $DRILL_DURATION minutes"
echo "Please perform the following:"
echo "1. Follow standard operating procedures"
echo "2. Document all actions taken"
echo "3. Note any issues or improvements needed"
echo "4. Complete post-drill review"

# è®¾ç½®å®šæ—¶å™¨
sleep ${DRILL_DURATION}m

echo ""
echo "=== Drill Completed ==="
echo "End time: $(date)"
echo "$(date),DRILL_END,$DRILL_TYPE" >> /var/log/emergency_drills.log

# æç¤ºè¿›è¡Œæ¼”ç»ƒæ€»ç»“
echo ""
echo "Please complete the drill summary:"
echo "1. What went well?"
echo "2. What could be improved?"
echo "3. Any new procedures needed?"
echo "4. Training requirements identified?"
```

## ğŸ› ï¸ æ•…éšœå¤„ç†å·¥å…·é›†

### 13.5 è‡ªåŠ¨åŒ–è¯Šæ–­å·¥å…·

#### 13.5.1 ä¸€é”®è¯Šæ–­è„šæœ¬

```bash
#!/bin/bash
# /opt/troubleshooting/one_click_diagnose.sh

echo "=== HPC Cluster One-Click Diagnosis ==="
echo "Started at: $(date)"
echo ""

DIAGNOSIS_DIR="/tmp/diagnosis_$(date +%Y%m%d_%H%M%S)"
mkdir -p $DIAGNOSIS_DIR

# 1. ç³»ç»ŸåŸºæœ¬ä¿¡æ¯
echo "1. Collecting system information..."
cat > $DIAGNOSIS_DIR/system_info.txt << EOF
=== System Information ===
Hostname: $(hostname)
OS: $(cat /etc/os-release | grep PRETTY_NAME | cut -d'"' -f2)
Kernel: $(uname -r)
Uptime: $(uptime)
CPU: $(nproc) cores
Memory: $(free -h | awk 'NR==2{print $2}')
EOF

# 2. ç¡¬ä»¶çŠ¶æ€æ£€æŸ¥
echo "2. Checking hardware status..."
cat > $DIAGNOSIS_DIR/hardware_status.txt << EOF
=== Hardware Status ===
$(date)
EOF

# CPUæ¸©åº¦
if command -v sensors &> /dev/null; then
    echo "CPU Temperature:" >> $DIAGNOSIS_DIR/hardware_status.txt
    sensors | grep -E "Package|Core" >> $DIAGNOSIS_DIR/hardware_status.txt
fi

# å†…å­˜é”™è¯¯
if [ -f /var/log/mcelog ]; then
    echo "Memory Errors:" >> $DIAGNOSIS_DIR/hardware_status.txt
    tail -10 /var/log/mcelog >> $DIAGNOSIS_DIR/hardware_status.txt
fi

# ç£ç›˜çŠ¶æ€
echo "Disk Status:" >> $DIAGNOSIS_DIR/hardware_status.txt
for disk in /dev/sd* /dev/nvme*; do
    if [ -b "$disk" ]; then
        echo "Checking $disk:" >> $DIAGNOSIS_DIR/hardware_status.txt
        smartctl -H $disk 2>/dev/null | grep -E "SMART overall|PASSED|FAILED" >> $DIAGNOSIS_DIR/hardware_status.txt
    fi
done

# 3. ç½‘ç»œçŠ¶æ€æ£€æŸ¥
echo "3. Checking network status..."
cat > $DIAGNOSIS_DIR/network_status.txt << EOF
=== Network Status ===
$(date)
EOF

# æ¥å£çŠ¶æ€
echo "Interface Status:" >> $DIAGNOSIS_DIR/network_status.txt
ip link show | grep -E "state UP|state DOWN" >> $DIAGNOSIS_DIR/network_status.txt

# è¿é€šæ€§æµ‹è¯•
echo "Connectivity Test:" >> $DIAGNOSIS_DIR/network_status.txt
for node in compute-node-{01..05}; do
    if ping -c 1 -W 1 $node &> /dev/null; then
        echo "$node: OK" >> $DIAGNOSIS_DIR/network_status.txt
    else
        echo "$node: FAILED" >> $DIAGNOSIS_DIR/network_status.txt
    fi
done

# 4. æœåŠ¡çŠ¶æ€æ£€æŸ¥
echo "4. Checking service status..."
cat > $DIAGNOSIS_DIR/service_status.txt << EOF
=== Service Status ===
$(date)
EOF

SERVICES=("sshd" "slurmctld" "slurmd" "nfs-server")

for service in "${SERVICES[@]}"; do
    if systemctl is-active --quiet $service 2>/dev/null; then
        echo "$service: RUNNING" >> $DIAGNOSIS_DIR/service_status.txt
    else
        echo "$service: NOT RUNNING" >> $DIAGNOSIS_DIR/service_status.txt
    fi
done

# 5. SLURMçŠ¶æ€æ£€æŸ¥
if command -v sinfo &> /dev/null; then
    echo "5. Checking SLURM status..."
    cat > $DIAGNOSIS_DIR/slurm_status.txt << EOF
=== SLURM Status ===
$(date)
EOF
    sinfo >> $DIAGNOSIS_DIR/slurm_status.txt
    squeue -h | head -10 >> $DIAGNOSIS_DIR/slurm_status.txt
fi

# 6. å­˜å‚¨çŠ¶æ€æ£€æŸ¥
echo "6. Checking storage status..."
cat > $DIAGNOSIS_DIR/storage_status.txt << EOF
=== Storage Status ===
$(date)
EOF

# æ–‡ä»¶ç³»ç»Ÿ
echo "Filesystems:" >> $DIAGNOSIS_DIR/storage_status.txt
df -h >> $DIAGNOSIS_DIR/storage_status.txt

# æŒ‚è½½ç‚¹
echo "Mount Points:" >> $DIAGNOSIS_DIR/storage_status.txt
mount | grep -E "lustre|gpfs|nfs" >> $DIAGNOSIS_DIR/storage_status.txt

# 7. æ—¥å¿—åˆ†æ
echo "7. Analyzing recent logs..."
cat > $DIAGNOSIS_DIR/log_analysis.txt << EOF
=== Log Analysis ===
$(date)
EOF

# æœ€è¿‘é”™è¯¯
echo "Recent Errors:" >> $DIAGNOSIS_DIR/log_analysis.txt
grep -i "error\|fail\|critical" /var/log/messages | tail -20 >> $DIAGNOSIS_DIR/log_analysis.txt

# ç³»ç»Ÿèµ„æºä½¿ç”¨
echo "8. Checking resource usage..."
cat > $DIAGNOSIS_DIR/resource_usage.txt << EOF
=== Resource Usage ===
$(date)
EOF

echo "Memory Usage: $(free -h | awk 'NR==2{printf "%.2f%%", $3*100/$2}')" >> $DIAGNOSIS_DIR/resource_usage.txt
echo "Disk Usage: $(df -h / | awk 'NR==2{print $5}')" >> $DIAGNOSIS_DIR/resource_usage.txt
echo "Load Average: $(uptime | awk -F'load average:' '{print $2}')" >> $DIAGNOSIS_DIR/resource_usage.txt

# 8. ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
echo "8. Generating diagnosis report..."
cat > $DIAGNOSIS_DIR/diagnosis_report.txt << EOF
=== Diagnosis Report ===
Generated: $(date)
Location: $DIAGNOSIS_DIR

Summary:
- System information: âœ“ Collected
- Hardware status: âœ“ Checked
- Network status: âœ“ Checked
- Service status: âœ“ Checked
- SLURM status: âœ“ Checked (if available)
- Storage status: âœ“ Checked
- Log analysis: âœ“ Performed
- Resource usage: âœ“ Checked

Files generated:
$(ls -la $DIAGNOSIS_DIR)

Next steps:
1. Review individual status files
2. Check for any FAILED or ERROR states
3. Investigate issues found
4. Take appropriate corrective actions
EOF

echo ""
echo "=== Diagnosis Complete ==="
echo "Results saved to: $DIAGNOSIS_DIR"
echo "Summary report: $DIAGNOSIS_DIR/diagnosis_report.txt"
echo ""
echo "Quick status check:"
echo "- System info: $(ls $DIAGNOSIS_DIR/system_info.txt 2>/dev/null | wc -l)"
echo "- Hardware: $(ls $DIAGNOSIS_DIR/hardware_status.txt 2>/dev/null | wc -l)"
echo "- Network: $(ls $DIAGNOSIS_DIR/network_status.txt 2>/dev/null | wc -l)"
echo "- Services: $(ls $DIAGNOSIS_DIR/service_status.txt 2>/dev/null | wc -l)"
echo "- SLURM: $(ls $DIAGNOSIS_DIR/slurm_status.txt 2>/dev/null | wc -l)"
echo "- Storage: $(ls $DIAGNOSIS_DIR/storage_status.txt 2>/dev/null | wc -l)"
echo "- Logs: $(ls $DIAGNOSIS_DIR/log_analysis.txt 2>/dev/null | wc -l)"
echo "- Resources: $(ls $DIAGNOSIS_DIR/resource_usage.txt 2>/dev/null | wc -l)"
```

#### 13.5.2 æ™ºèƒ½æ•…éšœé¢„æµ‹

```bash
#!/bin/bash
# /opt/troubleshooting/predictive_monitoring.sh

echo "=== Predictive Failure Monitoring ==="
echo "Started at: $(date)"
echo ""

# æ”¶é›†å†å²æ•°æ®
DATA_FILE="/var/log/failure_prediction.log"
mkdir -p /var/log

# 1. ç¡¬ä»¶å¥åº·è¶‹åŠ¿åˆ†æ
echo "1. Analyzing hardware health trends..."

# CPUæ¸©åº¦è¶‹åŠ¿
if command -v sensors &> /dev/null; then
    CPU_TEMP=$(sensors | grep -E "Package|Core" | awk '{print $4}' | sed 's/+//' | sed 's/Â°C//' | head -1)
    if [ ! -z "$CPU_TEMP" ]; then
        echo "$(date),CPU_TEMP,$CPU_TEMP" >> $DATA_FILE
        # é¢„æµ‹è¿‡çƒ­é£é™©
        if (( $(echo "$CPU_TEMP > 75" | bc -l) )); then
            echo "âš ï¸ Warning: High CPU temperature detected ($CPU_TEMPÂ°C)"
            echo "$(date),HIGH_TEMP,$CPU_TEMP" >> /var/log/predictive_alerts.log
        fi
    fi
fi

# ç£ç›˜SMARTæ•°æ®è¶‹åŠ¿
for disk in /dev/sd* /dev/nvme*; do
    if [ -b "$disk" ]; then
        # æ£€æŸ¥é‡æ–°åˆ†é…æ‰‡åŒºæ•°
        REALLOCATED=$(smartctl -A $disk 2>/dev/null | grep "Reallocated_Sector" | awk '{print $10}')
        if [ ! -z "$REALLOCATED" ] && [ "$REALLOCATED" != "0" ]; then
            echo "$(date),DISK_REALLOCATED,$disk,$REALLOCATED" >> $DATA_FILE
            if [ "$REALLOCATED" -gt 100 ]; then
                echo "âš ï¸ Warning: High reallocated sectors on $disk ($REALLOCATED)"
                echo "$(date),DISK_FAILURE_RISK,$disk,$REALLOCATED" >> /var/log/predictive_alerts.log
            fi
        fi
    fi
done

# 2. ç³»ç»Ÿèµ„æºè¶‹åŠ¿åˆ†æ
echo "2. Analyzing resource usage trends..."

# å†…å­˜ä½¿ç”¨ç‡è¶‹åŠ¿
MEM_USAGE=$(free | awk 'NR==2{printf "%.2f", $3*100/$2}')
echo "$(date),MEMORY_USAGE,$MEM_USAGE" >> $DATA_FILE

# å¦‚æœå†…å­˜ä½¿ç”¨ç‡æŒç»­é«˜äº90%ï¼Œå‘å‡ºè­¦å‘Š
if (( $(echo "$MEM_USAGE > 90" | bc -l) )); then
    echo "âš ï¸ Warning: High memory usage detected ($MEM_USAGE%)"
    echo "$(date),HIGH_MEMORY,$MEM_USAGE" >> /var/log/predictive_alerts.log
fi

# ç£ç›˜ä½¿ç”¨ç‡è¶‹åŠ¿
DISK_USAGE=$(df / | awk 'NR==2{print $5}' | sed 's/%//')
echo "$(date),DISK_USAGE,$DISK_USAGE" >> $DATA_FILE

# å¦‚æœç£ç›˜ä½¿ç”¨ç‡æŒç»­é«˜äº90%ï¼Œå‘å‡ºè­¦å‘Š
if [ "$DISK_USAGE" -gt 90 ]; then
    echo "âš ï¸ Warning: High disk usage detected ($DISK_USAGE%)"
    echo "$(date),HIGH_DISK,$DISK_USAGE" >> /var/log/predictive_alerts.log
fi

# 3. ç½‘ç»œè¿æ¥è¶‹åŠ¿åˆ†æ
echo "3. Analyzing network reliability..."

# æµ‹è¯•åˆ°å…³é”®èŠ‚ç‚¹çš„è¿æ¥
for node in compute-node-{01..05}; do
    if ! ping -c 1 -W 1 $node &> /dev/null; then
        echo "$(date),NETWORK_FAILURE,$node" >> $DATA_FILE
        echo "$(date),NETWORK_FAILURE,$node" >> /var/log/predictive_alerts.log
    fi
done

# 4. æœåŠ¡å¯ç”¨æ€§è¶‹åŠ¿
echo "4. Analyzing service reliability..."

SERVICES=("sshd" "slurmctld" "slurmd")
for service in "${SERVICES[@]}"; do
    if ! systemctl is-active --quiet $service 2>/dev/null; then
        echo "$(date),SERVICE_FAILURE,$service" >> $DATA_FILE
        echo "$(date),SERVICE_FAILURE,$service" >> /var/log/predictive_alerts.log
    fi
done

# 5. å†å²æ•°æ®åˆ†æ
echo "5. Analyzing historical patterns..."

# åˆ†æè¿‡å»24å°æ—¶çš„æ•…éšœé¢‘ç‡
LAST_24_HOURS=$(date -d '24 hours ago' '+%Y-%m-%d %H:%M:%S')
ALERT_COUNT=$(grep -c "WARNING\|CRITICAL" /var/log/predictive_alerts.log | tail -24)

if [ "$ALERT_COUNT" -gt 10 ]; then
    echo "âš ï¸ Warning: High failure rate in the last 24 hours ($ALERT_COUNT alerts)"
    echo "$(date),HIGH_FAILURE_RATE,$ALERT_COUNT" >> /var/log/predictive_alerts.log
fi

# 6. ç”Ÿæˆé¢„æµ‹æŠ¥å‘Š
echo "6. Generating prediction report..."

cat > /tmp/prediction_report.txt << EOF
=== Predictive Analysis Report ===
Generated: $(date)

Key Findings:
1. Hardware Health: $(grep "HIGH_TEMP\|DISK_FAILURE_RISK" /var/log/predictive_alerts.log | wc -l) issues
2. Resource Usage: $(grep "HIGH_MEMORY\|HIGH_DISK" /var/log/predictive_alerts.log | wc -l) issues
3. Network Reliability: $(grep "NETWORK_FAILURE" /var/log/predictive_alerts.log | wc -l) failures
4. Service Reliability: $(grep "SERVICE_FAILURE" /var/log/predictive_alerts.log | wc -l) failures
5. Overall Failure Rate: $ALERT_COUNT alerts in 24 hours

Recommendations:
- Review hardware health for potential replacements
- Monitor resource usage trends
- Check network infrastructure
- Verify service configurations
- Consider preventive maintenance

Data collected in: $DATA_FILE
Alerts logged to: /var/log/predictive_alerts.log
EOF

echo ""
echo "=== Predictive Monitoring Complete ==="
echo "Report saved to: /tmp/prediction_report.txt"
echo "Data file: $DATA_FILE"
echo "Alerts file: /var/log/predictive_alerts.log"
echo ""

# æ˜¾ç¤ºå…³é”®è­¦å‘Š
if [ -f /var/log/predictive_alerts.log ]; then
    echo "Recent alerts:"
    tail -10 /var/log/predictive_alerts.log
fi
```

## ğŸ“š æœ€ä½³å®è·µ

### 13.6 æ•…éšœé¢„é˜²ç­–ç•¥

#### 13.6.1 å®šæœŸå¥åº·æ£€æŸ¥

**è‡ªåŠ¨åŒ–å¥åº·æ£€æŸ¥è„šæœ¬ï¼š**

```bash
#!/bin/bash
# /opt/troubleshooting/health_check.sh

echo "=== Daily Health Check ==="
echo "Date: $(date)"
echo ""

# 1. ç³»ç»Ÿèµ„æºæ£€æŸ¥
echo "1. System Resources:"
CPU_USAGE=$(top -bn1 | grep "Cpu(s)" | awk '{print 100 - $8}' | sed 's/%id,//')
MEM_USAGE=$(free | awk 'NR==2{printf "%.2f", $3*100/$2}')
DISK_USAGE=$(df / | awk 'NR==2{print $5}' | sed 's/%//')
LOAD_AVG=$(uptime | awk -F'load average:' '{print $2}' | awk '{print $1}' | sed 's/,//')

echo "  CPU Usage: $CPU_USAGE%"
echo "  Memory Usage: $MEM_USAGE%"
echo "  Disk Usage: $DISK_USAGE%"
echo "  Load Average: $LOAD_AVG"

# 2. æœåŠ¡çŠ¶æ€æ£€æŸ¥
echo ""
echo "2. Service Status:"
SERVICES=("sshd" "slurmctld" "slurmd" "nfs-server" "named")

for service in "${SERVICES[@]}"; do
    if systemctl is-active --quiet $service 2>/dev/null; then
        echo "  âœ“ $service: Running"
    else
        echo "  âœ— $service: Not running"
    fi
done

# 3. ç½‘ç»œè¿é€šæ€§æ£€æŸ¥
echo ""
echo "3. Network Connectivity:"
for node in compute-node-{01..05}; do
    if ping -c 1 -W 1 $node &> /dev/null; then
        echo "  âœ“ $node: OK"
    else
        echo "  âœ— $node: Failed"
    fi
done

# 4. å­˜å‚¨ç³»ç»Ÿæ£€æŸ¥
echo ""
echo "4. Storage Systems:"
if mount | grep -q lustre; then
    echo "  âœ“ LUSTRE: Mounted"
    lfs df -h | tail -1 | awk '{print "  Usage: " $5}'
else
    echo "  âœ— LUSTRE: Not mounted"
fi

# 5. SLURMé›†ç¾¤çŠ¶æ€
echo ""
echo "5. SLURM Cluster Status:"
if command -v sinfo &> /dev/null; then
    sinfo -h -o "%P %a %l %n %D" | head -5
else
    echo "  SLURM tools not available"
fi

# 6. ç”Ÿæˆå¥åº·æŠ¥å‘Š
echo ""
echo "6. Health Report:"
cat > /tmp/daily_health_report.txt << EOF
=== Daily Health Report ===
Date: $(date)
Hostname: $(hostname)

System Resources:
- CPU Usage: $CPU_USAGE%
- Memory Usage: $MEM_USAGE%
- Disk Usage: $DISK_USAGE%
- Load Average: $LOAD_AVG

Service Status:
$(systemctl is-active sshd slurmd slurmctld nfs-server named 2>/dev/null | \
    paste <(echo -e "sshd\nslurmd\nslurmctld\nnfs-server\nnamed") - | \
    awk '{print "  " $1 ": " $2}')

Network Status:
$(for node in compute-node-{01..05}; do
    if ping -c 1 -W 1 $node &> /dev/null; then
        echo "  $node: OK"
    else
        echo "  $node: Failed"
    fi
done)

Storage Status:
$(mount | grep -E "lustre|gpfs|nfs" | awk '{print "  " $3 ": " $5}')

SLURM Status:
$(sinfo -h -o "%P %a %l %n %D" 2>/dev/null | head -3)
EOF

echo "Health report saved to: /tmp/daily_health_report.txt"
echo "=== Health Check Complete ==="
```

#### 13.6.2 é¢„é˜²æ€§ç»´æŠ¤è®¡åˆ’

**ç»´æŠ¤è®¡åˆ’è„šæœ¬ï¼š**

```bash
#!/bin/bash
# /opt/troubleshooting/preventive_maintenance.sh

MAINTENANCE_TYPE=$1
HOSTNAME=$(hostname)

case $MAINTENANCE_TYPE in
    "daily")
        echo "=== Daily Preventive Maintenance ==="

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        find /tmp -type f -atime +7 -delete
        find /var/tmp -type f -atime +7 -delete

        # æ¸…ç†æ—¥å¿—æ–‡ä»¶
        find /var/log -name "*.log" -mtime +30 -delete
        find /var/log -name "*.gz" -mtime +90 -delete

        # æ£€æŸ¥ç£ç›˜ç©ºé—´
        df -h | awk '$5 > 80 {print "Warning: " $6 " is " $5 " full"}'

        # æ›´æ–°åŒ…ç¼“å­˜
        yum clean expire-cache
        ;;

    "weekly")
        echo "=== Weekly Preventive Maintenance ==="

        # ç¡¬ä»¶å¥åº·æ£€æŸ¥
        /opt/troubleshooting/hardware_check.sh

        # ç³»ç»Ÿæ›´æ–°æ£€æŸ¥
        yum check-update

        # å¤‡ä»½é‡è¦é…ç½®
        tar -czf /backup/config_backup_$(date +%Y%m%d).tar.gz \
            /etc/hosts \
            /etc/slurm/ \
            /etc/named.conf \
            /etc/exports

        # æ£€æŸ¥RAIDçŠ¶æ€
        if command -v mdadm &> /dev/null; then
            mdadm --detail /dev/md* 2>/dev/null | grep -E "State|Rebuild"
        fi
        ;;

    "monthly")
        echo "=== Monthly Preventive Maintenance ==="

        # å®Œæ•´ç³»ç»Ÿå¤‡ä»½
        echo "Creating system backup..."
        # å®é™…çš„å¤‡ä»½å‘½ä»¤ä¼šæ ¹æ®å¤‡ä»½ç­–ç•¥è°ƒæ•´

        # æ€§èƒ½åŸºå‡†æµ‹è¯•
        echo "Running performance benchmarks..."
        # è¿è¡ŒåŸºå‡†æµ‹è¯•

        # å®‰å…¨æ‰«æ
        echo "Performing security scan..."
        # è¿è¡Œå®‰å…¨æ‰«æå·¥å…·

        # æ›´æ–°åº”æ€¥å“åº”æ–‡æ¡£
        echo "Updating emergency procedures..."
        # æ£€æŸ¥å’Œæ›´æ–°æ–‡æ¡£
        ;;

    *)
        echo "Usage: $0 <daily|weekly|monthly>"
        echo "Example: $0 daily"
        exit 1
        ;;
esac

echo "Maintenance completed at: $(date)"
```

## ğŸ“ æœ¬ç« æ€»ç»“

æ•…éšœè¯Šæ–­ä¸æ’é™¤æ˜¯HPCè¿ç»´å·¥ç¨‹å¸ˆçš„æ ¸å¿ƒèƒ½åŠ›ï¼Œé€šè¿‡æœ¬ç« å­¦ä¹ ï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- âœ… è¯†åˆ«å’Œåˆ†ç±»HPCé›†ç¾¤çš„å¸¸è§æ•…éšœç±»å‹
- âœ… å»ºç«‹ç³»ç»ŸåŒ–çš„æ•…éšœè¯Šæ–­æµç¨‹
- âœ… ç†Ÿç»ƒä½¿ç”¨å„ç§æ•…éšœè¯Šæ–­å·¥å…·å’Œè„šæœ¬
- âœ… é…ç½®æœ‰æ•ˆçš„æ—¥å¿—åˆ†æå’Œç›‘æ§ç³»ç»Ÿ
- âœ… åˆ¶å®šå’Œæ‰§è¡Œåº”æ€¥å“åº”é¢„æ¡ˆ
- âœ… å®æ–½é¢„é˜²æ€§ç»´æŠ¤ç­–ç•¥

æ•…éšœå¤„ç†çš„å…³é”®åœ¨äº**å¿«é€Ÿå“åº”ã€å‡†ç¡®è¯Šæ–­ã€æœ‰æ•ˆè§£å†³ã€é¢„é˜²å¤å‘**ã€‚å»ºç«‹å®Œå–„çš„æ•…éšœå¤„ç†ä½“ç³»ï¼Œå¯ä»¥å¤§å¤§æé«˜HPCé›†ç¾¤çš„å¯ç”¨æ€§å’Œç”¨æˆ·æ»¡æ„åº¦ã€‚

## ğŸ“ ç»ƒä¹ é¢˜

1. **å®è·µé¢˜**ï¼šåœ¨æµ‹è¯•ç¯å¢ƒä¸­æ¨¡æ‹Ÿå„ç§æ•…éšœåœºæ™¯å¹¶ç»ƒä¹ è¯Šæ–­æµç¨‹
2. **é…ç½®é¢˜**ï¼šä¸ºä½ çš„é›†ç¾¤é…ç½®å®Œæ•´çš„ç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿ
3. **åˆ†æé¢˜**ï¼šåˆ†æä¸€å‘¨çš„ç³»ç»Ÿæ—¥å¿—ï¼Œè¯†åˆ«æ½œåœ¨é—®é¢˜
4. **è®¾è®¡é¢˜**ï¼šè®¾è®¡ä¸€ä¸ªåŒ…å«åº”æ€¥å“åº”æµç¨‹çš„æ•…éšœå¤„ç†æ‰‹å†Œ

## ğŸ”— å‚è€ƒèµ„æº

- [Linuxæ•…éšœæ’é™¤æŒ‡å—](https://www.kernel.org/doc/html/latest/admin-guide/bug-hunting.html)
- [SLURMæ•…éšœæ’é™¤](https://slurm.schedmd.com/troubleshoot.html)
- [LUSTREæ•…éšœæ’é™¤](https://wiki.lustre.org/Lustre_Operations_Guide_for_Lustre_2.12)
- [Nagiosæ•…éšœæ’é™¤](https://assets.nagios.com/downloads/nagioscore/docs/nagioscore/4/en/troubleshooting.html)