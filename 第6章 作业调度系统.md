# 第6章 作业调度系统

## 6.1 SLURM调度器详解

### SLURM基础概念

#### 核心组件

**SLURM架构**
```
┌─────────────────────────────────────────────────────────────┐
│                    用户界面层                                 │
│  ┌────────────┐  ┌────────────┐  ┌────────────┐             │
│  │   sbatch   │  │   srun     │  │   squeue   │             │
│  └────────────┘  └────────────┘  └────────────┘             │
└─────────────────────────────────────────────────────────────┘
┌─────────────────────────────────────────────────────────────┐
│                    控制层                                     │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │              SLURM Controller (slurmctld)               │ │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐  │ │
│  │  │  Scheduler │  │  Database│  │  Plugin │  │  REST   │  │ │
│  │  │  Module │  │  (slurmdbd)│ │  System │  │  API    │  │ │
│  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘  │ │
│  └─────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────┘
┌─────────────────────────────────────────────────────────────┐
│                    计算资源层                                 │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐        │
│  │  Login  │  │  Compute│  │  Login  │  │  Compute│        │
│  │  Node   │  │  Nodes  │  │  Node   │  │  Nodes  │        │
│  │  (1-2)  │  │  (100+) │  │  (1-2)  │  │  (100+) │        │
│  └─────────┘  └─────────┘  └─────────┘  └─────────┘        │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐        │
│  │  GPU    │  │  GPU    │  │  GPU    │  │  GPU    │        │
│  │  Nodes  │  │  Nodes  │  │  Nodes  │  │  Nodes  │        │
│  │  (10+)  │  │  (10+)  │  │  (10+)  │  │  (10+)  │        │
│  └─────────┘  └─────────┘  └─────────┘  └─────────┘        │
└─────────────────────────────────────────────────────────────┘
```

#### 基本概念

**作业（Job）**
- 用户提交的计算任务
- 包含资源需求和执行命令
- 有唯一的作业ID

**分区（Partition）**
- 计算资源的逻辑分组
- 可以有不同的配置策略
- 类似于PBS中的队列

**节点（Node）**
- 物理或虚拟计算资源
- 包含CPU、内存、GPU等资源
- 可以属于多个分区

### SLURM配置文件

#### 主配置文件（slurm.conf）

```bash
# SLURM配置文件示例
# /etc/slurm/slurm.conf

# 基本配置
ClusterName=hpc-cluster
ControlMachine=master-node
ControlAddr=192.168.1.10
SlurmctldPort=6817
SlurmdPort=6818

# 节点配置
NodeName=compute-[001-100] CPUs=64 RealMemory=524288 State=UNKNOWN
NodeName=gpu-[001-020] CPUs=32 RealMemory=262144 Gres=gpu:4 State=UNKNOWN
NodeName=bigmem-[001-005] CPUs=128 RealMemory=2097152 State=UNKNOWN

# 分区配置
PartitionName=compute Nodes=compute-[001-100] Default=YES MaxTime=72:00:00 State=UP
PartitionName=gpu Nodes=gpu-[001-020] Default=NO MaxTime=24:00:00 State=UP
PartitionName=debug Nodes=compute-[001-010] Default=NO MaxTime=02:00:00 State=UP

# QoS配置
QOS=normal,high,debug
QOSAccess=normal:compute,gpu
QOSAccess=high:compute
QOSAccess=debug:debug

# 调度配置
SchedulerType=sched/backfill
SchedulerParameters=bf_continue,bf_window=600
SelectType=select/cons_res
SelectTypeParameters=CR_Core_Memory

# 通信配置
SlurmctldTimeout=300
SlurmdTimeout=300
InactiveLimit=0
KillWait=30
Waittime=0

# 日志配置
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdLogFile=/var/log/slurm/slurmd.log
JobCompType=jobcomp/mysql
JobCompLoc=slurm_acct_db
```

#### 数据库配置（slurmdbd.conf）

```bash
# SLURM数据库配置
# /etc/slurm/slurmdbd.conf

# 数据库连接
AuthType=auth/munge
AuthInfo=/var/run/munge/munge.socket.2
DbdHost=localhost
DbdPort=6819
DbdUser=slurm
DbdPass=slurm_password
StorageHost=localhost
StorageLoc=slurm_acct_db
StoragePass=slurm_password
StorageUser=slurm
StorageType=accounting_storage/mysql

# 日志配置
LogFile=/var/log/slurm/slurmdbd.log
DebugLevel=info
```

### 作业提交与管理

#### 基本作业提交

```bash
# 1. 简单作业提交
sbatch job_script.sh

# 2. 交互式作业
salloc -N 1 -n 4 --time=01:00:00
srun --pty /bin/bash

# 3. 直接运行
srun -N 1 -n 4 --time=01:00:00 ./my_program

# 4. 批量作业脚本
#!/bin/bash
#SBATCH --job-name=test_job
#SBATCH --output=output_%j.log
#SBATCH --error=error_%j.log
#SBATCH --partition=compute
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=32
#SBATCH --time=02:00:00
#SBATCH --mail-type=ALL
#SBATCH --mail-user=user@hpc.edu.cn

# 加载模块
module load openmpi/4.1.4
module load gcc/11.2.0

# 运行程序
mpirun -np 64 ./my_mpi_program
```

#### 高级作业选项

```bash
# 1. 资源指定
#SBATCH --cpus-per-task=4        # 每个任务的CPU数
#SBATCH --mem=16G               # 内存限制
#SBATCH --gres=gpu:2            # GPU资源
#SBATCH --exclusive             # 独占节点

# 2. 时间和优先级
#SBATCH --time=24:00:00         # 运行时间
#SBATCH --priority=1000         # 优先级
#SBATCH --qos=high             # QoS级别

# 3. 调度约束
#SBATCH --constraint=fast      # 节点约束
#SBATCH --exclude=node001      # 排除节点
#SBATCH --nodelist=node002,node003  # 指定节点

# 4. 邮件通知
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=user@hpc.edu.cn

# 5. 依赖关系
#SBATCH --dependency=after:12345
#SBATCH --dependency=afterok:12345:12346
```

### 作业监控与管理

#### 作业状态查询

```bash
# 1. 查看作业队列
squeue
squeue -u username
squeue -p partition_name

# 2. 查看详细信息
squeue -l
squeue --format="%.18i %.9P %.8j %.8u %.2t %.10M %.6D %R"

# 3. 查看作业详情
scontrol show job 12345
scontrol show jobid -dd 12345

# 4. 查看节点状态
sinfo
sinfo -l
sinfo -N -l

# 5. 查看分区信息
sinfo -p partition_name
```

#### 作业管理操作

```bash
# 1. 取消作业
scancel 12345
scancel -u username
scancel -t pending

# 2. 挂起和恢复
scontrol suspend 12345
scontrol resume 12345

# 3. 重新排队
scontrol requeue 12345

# 4. 修改作业
scontrol update jobid=12345 TimeLimit=04:00:00
scontrol update jobid=12345 Priority=2000

# 5. 查看作业输出
scontrol read_file 12345
scontrol listpids 12345
```

### SLURM高级功能

#### 资源管理

```bash
# 1. 节点管理
scontrol update nodename=compute-001 state=drain reason="maintenance"
scontrol update nodename=compute-001 state=resume

# 2. 分区管理
scontrol update partitionname=compute MaxTime=48:00:00
scontrol create partition name=test nodes=node001,002

# 3. QoS管理
scontrol create qos name=premium
scontrol update qosname=premium Priority=10000

# 4. 账户管理
sacctmgr create account name=project1
sacctmgr create user name=user1 account=project1
```

#### 性能监控

```bash
# 1. 作业性能统计
sacct -j 12345 --format=JobID,JobName,Partition,AllocCPUS,Elapsed,MaxRSS,MaxVMSize
sacct -u username --format=User,JobID,Partition,Elapsed,CPUTime

# 2. 系统利用率
sreport cluster Allocated -t hour
sreport cluster Utilization -t hour

# 3. 资源使用统计
sreport license Utilization -t hour
sreport filesystem Utilization -t hour
```

#### 故障诊断

```bash
# 1. 检查服务状态
systemctl status slurmctld
systemctl status slurmd

# 2. 查看日志
tail -f /var/log/slurm/slurmctld.log
tail -f /var/log/slurm/slurmd.log

# 3. 检查配置
slurmd -C
scontrol show config

# 4. 测试连接
scontrol ping
```

## 6.2 PBS Pro使用指南

### PBS Pro架构

#### 核心组件

**PBS Pro架构图**
```
┌─────────────────────────────────────────────────────────────┐
│                    用户界面层                                 │
│  ┌────────────┐  ┌────────────┐  ┌────────────┐             │
│  │   qsub     │  │   qstat    │  │   qdel     │             │
│  └────────────┘  └────────────┘  └────────────┘             │
└─────────────────────────────────────────────────────────────┘
┌─────────────────────────────────────────────────────────────┐
│                    服务器层                                    │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │              PBS Server (pbs_server)                    │ │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐  │ │
│  │  │ Scheduler │  │  Database│  │  Plugin │  │  MOM    │  │ │
│  │  │  Module │  │  (pbs_sched)│ │  System │  │  (pbs_mom)│ │ │
│  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘  │ │
│  └─────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────┘
┌─────────────────────────────────────────────────────────────┐
│                    计算资源层                                 │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐        │
│  │  Login  │  │  Compute│  │  Login  │  │  Compute│        │
│  │  Node   │  │  Nodes  │  │  Node   │  │  Nodes  │        │
│  │  (1-2)  │  │  (100+) │  │  (1-2)  │  │  (100+) │        │
│  └─────────┘  └─────────┘  └─────────┘  └─────────┘        │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐        │
│  │  GPU    │  │  GPU    │  │  GPU    │  │  GPU    │        │
│  │  Nodes  │  │  Nodes  │  │  Nodes  │  │  Nodes  │        │
│  │  (10+)  │  │  (10+)  │  │  (10+)  │  │  (10+)  │        │
│  └─────────┘  └─────────┘  └─────────┘  └─────────┘        │
└─────────────────────────────────────────────────────────────┘
```

### PBS Pro配置

#### 主配置文件（server_priv/server_conf）

```bash
# PBS Server配置
# /var/spool/pbs/server_priv/server_conf

# 基本配置
$pbsserver master-node
$port 15001
$log_level 1
$restricted master-node
$usecp *:/home /home
$spool_directory /var/spool/pbs/spool
$lockfile /var/spool/pbs/server_priv/lock

# 调度器配置
$SchedulerAdd sched1
$sched_priv /var/spool/pbs/sched_priv/sched_config
$sched_log /var/spool/pbs/sched_logs

# 作业历史配置
$job_history_dir /var/spool/pbs/history
$job_history_enable True
```

#### 队列配置（server_priv/queues）

```bash
# 队列定义
create queue compute
set queue compute queue_type = Execution
set queue compute Priority = 100
set queue compute resources_default.nodes = 1
set queue compute resources_default.walltime = 72:00:00
set queue compute max_running = 100
set queue compute enabled = True
set queue compute started = True

create queue gpu
set queue gpu queue_type = Execution
set queue gpu Priority = 150
set queue gpu resources_default.nodes = 1:ppn=4:gpus=2
set queue gpu resources_default.walltime = 24:00:00
set queue gpu max_running = 20
set queue gpu enabled = True
set queue gpu started = True

create queue debug
set queue debug queue_type = Execution
set queue debug Priority = 50
set queue debug resources_default.nodes = 1
set queue debug resources_default.walltime = 02:00:00
set queue debug max_running = 5
set queue debug enabled = True
set queue debug started = True
```

#### 节点配置（server_priv/nodes）

```bash
# 节点定义
compute-001 np=64 gpus=0
compute-002 np=64 gpus=0
compute-003 np=64 gpus=0
...
gpu-001 np=32 gpus=4
gpu-002 np=32 gpus=4
gpu-003 np=32 gpus=4
...
bigmem-001 np=128 gpus=0
```

### PBS Pro作业管理

#### 作业提交

```bash
# 1. 简单作业脚本
#!/bin/bash
#PBS -N test_job
#PBS -o output.log
#PBS -e error.log
#PBS -q compute
#PBS -l nodes=2:ppn=32
#PBS -l walltime=02:00:00
#PBS -m abe
#PBS -M user@hpc.edu.cn

cd $PBS_O_WORKDIR
module load openmpi/4.1.4
mpirun -np 64 ./my_program

# 2. 交互式作业
qsub -I -q compute -l nodes=1:ppn=32 -l walltime=01:00:00

# 3. 批量作业
qsub job_script.sh

# 4. 高级选项
#PBS -l select=2:ncpus=32:mem=256gb
#PBS -l place=scatter
#PBS -W group_list=project1
#PBS -W depend=afterok:12345
```

#### 作业监控

```bash
# 1. 查看作业状态
qstat
qstat -u username
qstat -q queue_name
qstat -f job_id

# 2. 查看队列信息
qstat -Q
qstat -Qf queue_name

# 3. 查看节点状态
pbsnodes
pbsnodes -a
pbsnodes -l
pbsnodes node_name

# 4. 查看资源使用
qstat -B
qstat -G
```

#### 作业管理

```bash
# 1. 取消作业
qdel job_id
qdel -p job_id  # 暂停删除
qdel -w job_id  # 等待删除

# 2. 挂起和恢复
qsig -s SUSPEND job_id
qsig -s CONTINUE job_id

# 3. 修改作业
qalter job_id -l walltime=04:00:00
qalter job_id -p 100

# 4. 查看作业输出
qpeek job_id
qcat job_id
```

### PBS Pro高级功能

#### 资源管理

```bash
# 1. 节点管理
pbsnodes -o node_name  # 关闭节点
pbsnodes -c node_name  # 清除状态
pbsnodes -r node_name  # 恢复节点

# 2. 队列管理
qmgr -c "create queue new_queue"
qmgr -c "set queue compute max_running = 200"
qmgr -c "delete queue old_queue"

# 3. 服务器配置
qmgr -c "set server scheduler_iteration = 60"
qmgr -c "set server default_queue = compute"
qmgr -c "set server max_array_size = 1000"
```

#### 性能监控

```bash
# 1. 系统统计
qstat -Bf
qstat -Gf

# 2. 作业统计
qstat -f -x job_id
qstat -f -x -u username

# 3. 资源使用
pbsnodes -a -F json
```

## 6.3 LSF调度器配置

### LSF架构

#### 核心组件

**LSF架构图**
```
┌─────────────────────────────────────────────────────────────┐
│                    用户界面层                                 │
│  ┌────────────┐  ┌────────────┐  ┌────────────┐             │
│  │   bsub     │  │   bjobs    │  │   bkill    │             │
│  └────────────┘  └────────────┘  └────────────┘             │
└─────────────────────────────────────────────────────────────┘
┌─────────────────────────────────────────────────────────────┐
│                    管理层                                     │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │              LSF Master (lsbmaster)                     │ │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐  │ │
│  │  │ Scheduler │  │  Database│  │  Plugin │  │  Admin  │  │ │
│  │  │  Module │  │  (lsb_acct)│ │  System │  │  Tools  │  │ │
│  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘  │ │
│  └─────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────┘
┌─────────────────────────────────────────────────────────────┐
│                    计算资源层                                 │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐        │
│  │  Login  │  │  Compute│  │  Login  │  │  Compute│        │
│  │  Node   │  │  Nodes  │  │  Node   │  │  Nodes  │        │
│  │  (1-2)  │  │  (100+) │  │  (1-2)  │  │  (100+) │        │
│  └─────────┘  └─────────┘  └─────────┘  └─────────┘        │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐        │
│  │  GPU    │  │  GPU    │  │  GPU    │  │  GPU    │        │
│  │  Nodes  │  │  Nodes  │  │  Nodes  │  │  Nodes  │        │
│  │  (10+)  │  │  (10+)  │  │  (10+)  │  │  (10+)  │        │
│  └─────────┘  └─────────┘  └─────────┘  └─────────┘        │
└─────────────────────────────────────────────────────────────┘
```

### LSF配置文件

#### lsf.conf

```bash
# LSF主配置文件
# /opt/lsf/conf/lsf.conf

# 基本配置
LSF_CONFDIR="/opt/lsf/conf"
LSF_SERVERDIR="/opt/lsf/9.1/linux2.6-glibc2.3-x86_64/etc"
LSF_LIBDIR="/opt/lsf/9.1/linux2.6-glibc2.3-x86_64/lib"
LSF_BINDIR="/opt/lsf/9.1/linux2.6-glibc2.3-x86_64/bin"
LSF_LOGDIR="/opt/lsf/9.1/linux2.6-glibc2.3-x86_64/work/cluster1/logs"

# 服务配置
LSF_MASTER_LIST="master-node"
LSF_ENVDIR="/opt/lsf/conf"

# 网络配置
LSF_LIMPORT=6700
LSF_RES_PORT=6701
LSF_SBD_PORT=6702
```

#### cluster.conf

```bash
# 集群配置
Begin Cluster
HOSTNAME      model       type    server  swp   swp_interval
master-node   -           -       1       -     -
compute-001   x86_64      linux   1       -     -
compute-002   x86_64      linux   1       -     -
gpu-001       x86_64      linux   1       -     -
End Cluster
```

#### lsb.hosts

```bash
# 主机配置
Begin Host
HOST_NAME       model   type    server  swp   swp_interval
master-node     -       -       1       -     -
compute-001     x86_64  linux   1       -     -
compute-002     x86_64  linux   1       -     -
gpu-001         x86_64  linux   1       -     -
End Host
```

#### lsb.queues

```bash
# 队列配置
Begin Queue
QUEUE_NAME      PRIO    NICE    SLOT_LIMIT      RUN_WINDOWS
compute         50      10      -               24x7
gpu             100     5       -               24x7
debug           10      20      5               24x7
End Queue
```

### LSF作业管理

#### 作业提交

```bash
# 1. 简单作业
bsub -q compute -n 64 -W 2:00 ./my_program

# 2. 作业脚本
#!/bin/bash
#BSUB -J test_job
#BSUB -o output_%J.log
#BSUB -e error_%J.log
#BSUB -q compute
#BSUB -n 64
#BSUB -W 2:00
#BSUB -R "span[hosts=1]"
#BSUB -u user@hpc.edu.cn

module load openmpi/4.1.4
mpirun ./my_program

# 3. 高级选项
bsub -q gpu -n 4 -gpu "num=2" -R "rusage[mem=8GB]" ./gpu_program
bsub -w "ended(12345)" ./dependent_job
```

#### 作业监控

```bash
# 1. 查看作业状态
bjobs
bjobs -u username
bjobs -q queue_name
bjobs -l job_id

# 2. 查看队列状态
bqueues
bqueues -l queue_name

# 3. 查看主机状态
bhosts
bhosts -l hostname

# 4. 查看资源使用
bhosts -w
bqueues -l
```

#### 作业管理

```bash
# 1. 取消作业
bkill job_id
bkill -9 job_id
bkill -r job_id  # 重新排队

# 2. 挂起和恢复
bstop job_id
bresume job_id
bresume -a  # 恢复所有

# 3. 修改作业
bmod -q gpu job_id
bmod -W 4:00 job_id
```

## 6.4 调度策略优化

### 调度算法

#### 先来先服务（FCFS）
```bash
# 简单公平调度
# 优点：简单、公平
# 缺点：资源利用率低、响应时间长
```

#### 最短作业优先（SJF）
```bash
# 优化响应时间
# 优点：平均等待时间最短
# 缺点：可能造成饥饿
```

#### 优先级调度
```bash
# 基于优先级的调度
# 优点：灵活、可定制
# 缺点：需要合理设置优先级
```

#### 回填调度（Backfilling）
```bash
# SLURM回填配置
SchedulerParameters=bf_continue,bf_window=600

# 优点：提高资源利用率
# 缺点：实现复杂
```

### 资源分配策略

#### 节点分配策略

```bash
# 1. 打包策略（Pack）
# 尽量使用少的节点
#SBATCH --exclusive
#SBATCH --constraint=fast

# 2. 散布策略（Scatter）
# 分散到多个节点
#SBATCH --spread-job

# 3. 平衡策略（Balance）
# 平衡负载
#SBATCH --distribution=cyclic
```

#### CPU分配策略

```bash
# 1. 核心绑定
#SBATCH --cpu-bind=cores
#SBATCH --hint=compute_bound

# 2. 内存绑定
#SBATCH --hint=memory_bound

# 3. NUMA优化
numactl --membind=0 --cpunodebind=0 ./program
```

### 性能优化策略

#### 队列设计

```bash
# 1. 分级队列
# 短作业队列
PartitionName=short Nodes=compute-[001-020] MaxTime=02:00:00 State=UP

# 长作业队列
PartitionName=long Nodes=compute-[021-100] MaxTime=168:00:00 State=UP

# GPU队列
PartitionName=gpu Nodes=gpu-[001-020] MaxTime=24:00:00 State=UP
```

#### 资源预留

```bash
# 1. 维护预留
scontrol update nodename=compute-001 state=maintenance

# 2. 高优先级预留
scontrol create reservation name=urgent starttime=now duration=3600 users=root
```

#### 负载均衡

```bash
# 1. 动态负载均衡
# 配置负载均衡插件
SchedulerParameters=load_balance

# 2. 智能调度
# 基于历史数据的调度
SchedulerParameters=predictive
```

### 故障处理

#### 节点故障处理

```bash
# 1. 自动故障检测
scontrol update nodename=compute-001 state=drain reason="hardware_failure"

# 2. 作业重调度
# 配置自动重调度
RestartWaitTime=300
Resubmit=2
```

#### 网络故障处理

```bash
# 1. 网络分区处理
# 配置网络检测
HealthCheckInterval=60
HealthCheckProgram=/opt/slurm/bin/check_network.sh

# 2. 作业迁移
# 配置作业迁移
MigrateJobs=YES
```

## 本章小结

作业调度系统是HPC集群的核心组件，负责资源管理和作业调度。本章详细介绍了：

1. **SLURM调度器**：现代HPC主流调度器，功能丰富，配置灵活
2. **PBS Pro**：传统调度器，稳定可靠，企业级应用
3. **LSF调度器**：IBM产品，适用于大型企业环境
4. **调度策略优化**：提高资源利用率和作业吞吐量的方法

选择合适的调度器需要考虑集群规模、应用特点、管理复杂度等因素。在实际部署中，需要根据具体需求进行配置优化，确保系统的高效运行。