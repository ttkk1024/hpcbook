# 第10章 并行编程环境

## 10.1 MPI编程环境

### MPI基础概念

#### MPI架构概述

**MPI通信模型**
```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Process 0     │    │   Process 1     │    │   Process 2     │
│                 │    │                 │    │                 │
│  ┌───────────┐  │    │  ┌───────────┐  │    │  ┌───────────┐  │
│  │ Application│  │    │  │ Application│  │    │  │ Application│  │
│  └───────────┘  │    │  └───────────┘  │    │  └───────────┘  │
│                 │    │                 │    │                 │
│  ┌───────────┐  │    │  ┌───────────┐  │    │  ┌───────────┐  │
│  │ MPI Lib   │  │    │  │ MPI Lib   │  │    │  │ MPI Lib   │  │
│  └───────────┘  │    │  └───────────┘  │    │  └───────────┘  │
│                 │    │                 │    │                 │
│  ┌───────────┐  │    │  ┌───────────┐  │    │  ┌───────────┐  │
│  │ Transport │  │    │  │ Transport │  │    │  │ Transport │  │
│  │ Layer     │  │    │  │ Layer     │  │    │  │ Layer     │  │
│  └───────────┘  │    │  └───────────┘  │    │  └───────────┘  │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                       │                       │
         └───────────────────────┼───────────────────────┘
                                 │
                    ┌─────────────────┐
                    │  Network Layer  │
                    │  (TCP/IP, IB)   │
                    └─────────────────┘
```

**MPI通信类型**
- **点对点通信**：进程间直接通信
- **集合通信**：多进程协调通信
- **非阻塞通信**：异步通信机制
- **单边通信**：RMA (Remote Memory Access)

### MPI库安装与配置

#### OpenMPI安装

**源码编译安装**
```bash
# 1. 下载OpenMPI
wget https://download.open-mpi.org/release/open-mpi/v5.0/openmpi-5.0.1.tar.gz
tar -xzf openmpi-5.0.1.tar.gz
cd openmpi-5.0.1

# 2. 配置编译选项 (OpenMPI 5.0+, 移除过时选项)
./configure --prefix=/opt/openmpi-5.0.1 \
            --with-ucx=/opt/ucx \
            --with-ofi=/opt/libfabric \
            --with-cuda=/usr/local/cuda \
            --enable-debug

# 3. 编译安装
make -j$(nproc)
make install

# 4. 设置环境变量
echo 'export PATH=/opt/openmpi-5.0.1/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/opt/openmpi-5.0.1/lib:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc
```

**包管理器安装**
```bash
# CentOS/RHEL
yum install openmpi openmpi-devel
module load openmpi

# Ubuntu/Debian
apt-get install openmpi-bin openmpi-common libopenmpi-dev
```

#### Intel MPI安装

**Intel OneAPI MPI**
```bash
# 1. 安装Intel OneAPI
source /opt/intel/oneapi/setvars.sh

# 2. 验证MPI安装
mpiifort --version
mpiicc --version
mpic++ --version

# 3. 测试MPI
mpirun -n 2 hostname
```

#### MVAPICH2安装

**MVAPICH2编译安装**
```bash
# 1. 下载MVAPICH2
wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.7.tar.gz
tar -xzf mvapich2-2.3.7.tar.gz
cd mvapich2-2.3.7

# 2. 配置编译
./configure --prefix=/opt/mvapich2-2.3.7 \
            --enable-shared \
            --enable-threads=multiple \
            --with-device=ch3:mrail \
            --with-rdma=gen2 \
            --enable-cxx \
            --enable-fortran

# 3. 编译安装
make -j$(nproc)
make install
```

### MPI编程基础

#### 基本MPI程序

**Hello World程序**
```c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char *argv[]) {
    int rank, size;

    // 初始化MPI
    MPI_Init(&argc, &argv);

    // 获取进程信息
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 打印进程信息 (标准输出不保证顺序)
    printf("Hello from process %d of %d\n", rank, size);

    // 结束MPI
    MPI_Finalize();
    return 0;
}
```

**编译和运行**
```bash
# 编译
mpicc -o hello hello.c

# 运行
mpirun -n 4 ./hello
# 或
mpiexec -n 4 ./hello
```

#### MPI点对点通信

**基本通信函数**
```c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char *argv[]) {
    int rank, size;
    int data;
    MPI_Status status;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    if (rank == 0) {
        // 发送数据
        data = 42;
        MPI_Send(&data, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);
        printf("Process 0 sent data: %d\n", data);
    } else if (rank == 1) {
        // 接收数据
        MPI_Recv(&data, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);
        printf("Process 1 received data: %d\n", data);
    }

    MPI_Finalize();
    return 0;
}
```

**非阻塞通信**
```c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char *argv[]) {
    int rank, size;
    int send_data = 100, recv_data;
    MPI_Request request;
    MPI_Status status;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    if (rank == 0) {
        // 非阻塞发送
        MPI_Isend(&send_data, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, &request);
        printf("Process 0 started sending data: %d\n", send_data);

        // 等待发送完成
        MPI_Wait(&request, &status);
        printf("Process 0 send completed\n");
    } else if (rank == 1) {
        // 非阻塞接收
        MPI_Irecv(&recv_data, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &request);

        // 做其他工作
        printf("Process 1 doing other work...\n");

        // 等待接收完成
        MPI_Wait(&request, &status);
        printf("Process 1 received data: %d\n", recv_data);
    }

    MPI_Finalize();
    return 0;
}
```

#### MPI集合通信

**广播操作**
```c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char *argv[]) {
    int rank, size;
    int data;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    if (rank == 0) {
        data = 123;
        printf("Process 0 broadcasting data: %d\n", data);
    }

    // 广播
    MPI_Bcast(&data, 1, MPI_INT, 0, MPI_COMM_WORLD);

    printf("Process %d received data: %d\n", rank, data);

    MPI_Finalize();
    return 0;
}
```

**归约操作**
```c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char *argv[]) {
    int rank, size;
    int local_sum = rank * 10;
    int global_sum;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    printf("Process %d local sum: %d\n", rank, local_sum);

    // 求和归约
    MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("Global sum: %d\n", global_sum);
    }

    MPI_Finalize();
    return 0;
}
```

**全归约操作**
```c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char *argv[]) {
    int rank, size;
    int local_data = rank + 1;
    int global_sum;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 全归约，所有进程都得到结果
    MPI_Allreduce(&local_data, &global_sum, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);

    printf("Process %d: local_data=%d, global_sum=%d\n", rank, local_data, global_sum);

    MPI_Finalize();
    return 0;
}
```

### MPI高级特性

#### MPI进程拓扑

**进程拓扑创建**
```c
#include <mpi.h>
#include <stdio.h>
#include <math.h>

int main(int argc, char *argv[]) {
    int rank, size;
    int dims[2], periods[2], coords[2];
    int cart_rank;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 创建2D网格拓扑
    dims[0] = dims[1] = (int)sqrt(size);
    periods[0] = periods[1] = 0;  // 非周期性边界

    MPI_Comm cart_comm;
    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 0, &cart_comm);

    // 获取进程坐标
    MPI_Cart_coords(cart_comm, rank, 2, coords);
    printf("Process %d coordinates: (%d, %d)\n", rank, coords[0], coords[1]);

    // 获取邻居进程
    int left, right, up, down;
    MPI_Cart_shift(cart_comm, 0, 1, &up, &down);
    MPI_Cart_shift(cart_comm, 1, 1, &left, &right);

    printf("Process %d neighbors: up=%d, down=%d, left=%d, right=%d\n",
           rank, up, down, left, right);

    MPI_Comm_free(&cart_comm);
    MPI_Finalize();
    return 0;
}
```

#### MPI派生数据类型

**自定义数据类型**
```c
#include <mpi.h>
#include <stdio.h>

typedef struct {
    int id;
    float value;
    double timestamp;
} DataPacket;

int main(int argc, char *argv[]) {
    int rank, size;
    DataPacket send_data, recv_data;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 创建自定义数据类型
    int blocklengths[3] = {1, 1, 1};
    MPI_Datatype types[3] = {MPI_INT, MPI_FLOAT, MPI_DOUBLE};
    MPI_Aint displacements[3];

    DataPacket temp;
    MPI_Get_address(&temp.id, &displacements[0]);
    MPI_Get_address(&temp.value, &displacements[1]);
    MPI_Get_address(&temp.timestamp, &displacements[2]);

    // 相对位移
    MPI_Aint base;
    MPI_Get_address(&temp, &base);
    for (int i = 0; i < 3; i++) {
        displacements[i] -= base;
    }

    MPI_Datatype data_type;
    MPI_Type_create_struct(3, blocklengths, displacements, types, &data_type);
    MPI_Type_commit(&data_type);

    if (rank == 0) {
        send_data.id = 1;
        send_data.value = 3.14f;
        send_data.timestamp = 1234567890.0;

        MPI_Send(&send_data, 1, data_type, 1, 0, MPI_COMM_WORLD);
    } else if (rank == 1) {
        MPI_Recv(&recv_data, 1, data_type, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        printf("Received: id=%d, value=%f, timestamp=%f\n",
               recv_data.id, recv_data.value, recv_data.timestamp);
    }

    MPI_Type_free(&data_type);
    MPI_Finalize();
    return 0;
}
```

## 10.2 OpenMP多线程

### OpenMP基础概念

#### OpenMP执行模型

**线程层次结构**
```
┌─────────────────────────────────────┐
│            Master Thread            │
│  (主线程，线程ID = 0)               │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│             Fork Phase              │
│  创建工作线程                       │
└─────────────────────────────────────┘
                       │
┌─────────────┬─────────────┬─────────┐
│   Thread 0  │   Thread 1  │ Thread 2│
│  (主线程)   │  (工作线程) │ (工作线程)│
└─────────────┴─────────────┴─────────┘
                       │
┌─────────────────────────────────────┐
│            Join Phase               │
│  工作线程结束，主线程继续执行        │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│        Sequential Region            │
│  单线程执行                         │
└─────────────────────────────────────┘
```

### OpenMP编译器支持

#### 编译器配置

**GCC OpenMP支持**
```bash
# 1. 检查OpenMP支持
gcc -fopenmp -dM -E - < /dev/null | grep -i openmp

# 2. 编译OpenMP程序
gcc -fopenmp -O3 -o program program.c

# 3. 运行时控制
export OMP_NUM_THREADS=16
export OMP_PROC_BIND=TRUE
export OMP_PLACES=cores
```

**Intel编译器OpenMP**
```bash
# 1. 编译
icc -qopenmp -O3 -o program program.c

# 2. 运行时控制
export OMP_NUM_THREADS=16
export KMP_AFFINITY=scatter
```

#### OpenMP版本检查

```c
#include <omp.h>
#include <stdio.h>

int main() {
    printf("OpenMP version: %d\n", _OPENMP);
    printf("Number of processors: %d\n", omp_get_num_procs());
    printf("Max threads: %d\n", omp_get_max_threads());

    return 0;
}
```

### OpenMP指令详解

#### 并行区域指令

**基本并行区域**
```c
#include <omp.h>
#include <stdio.h>

int main() {
    #pragma omp parallel
    {
        int thread_id = omp_get_thread_num();
        int num_threads = omp_get_num_threads();
        printf("Hello from thread %d of %d\n", thread_id, num_threads);
    }

    return 0;
}
```

**线程数量控制**
```c
#include <omp.h>
#include <stdio.h>

int main() {
    #pragma omp parallel num_threads(4)
    {
        int thread_id = omp_get_thread_num();
        printf("Thread %d executing\n", thread_id);
    }

    return 0;
}
```

#### 工作共享指令

**for循环并行化**
```c
#include <omp.h>
#include <stdio.h>

int main() {
    int n = 100;
    int sum = 0;

    #pragma omp parallel for reduction(+:sum)
    for (int i = 0; i < n; i++) {
        sum += i;
    }

    printf("Sum: %d\n", sum);
    return 0;
}
```

**循环调度策略**
```c
#include <omp.h>
#include <stdio.h>

int main() {
    int n = 100;

    // 静态调度
    #pragma omp parallel for schedule(static, 10)
    for (int i = 0; i < n; i++) {
        printf("Thread %d: iteration %d\n", omp_get_thread_num(), i);
    }

    // 动态调度
    #pragma omp parallel for schedule(dynamic, 5)
    for (int i = 0; i < n; i++) {
        printf("Thread %d: iteration %d\n", omp_get_thread_num(), i);
    }

    // 运行时调度
    #pragma omp parallel for schedule(runtime)
    for (int i = 0; i < n; i++) {
        printf("Thread %d: iteration %d\n", omp_get_thread_num(), i);
    }

    return 0;
}
```

**sections指令**
```c
#include <omp.h>
#include <stdio.h>

void task1() { printf("Task 1 executed by thread %d\n", omp_get_thread_num()); }
void task2() { printf("Task 2 executed by thread %d\n", omp_get_thread_num()); }
void task3() { printf("Task 3 executed by thread %d\n", omp_get_thread_num()); }

int main() {
    #pragma omp parallel sections
    {
        #pragma omp section
        task1();

        #pragma omp section
        task2();

        #pragma omp section
        task3();
    }

    return 0;
}
```

#### 同步指令

**barrier同步**
```c
#include <omp.h>
#include <stdio.h>

int main() {
    #pragma omp parallel
    {
        int thread_id = omp_get_thread_num();

        printf("Thread %d before barrier\n", thread_id);

        #pragma omp barrier

        printf("Thread %d after barrier\n", thread_id);
    }

    return 0;
}
```

**critical临界区**
```c
#include <omp.h>
#include <stdio.h>

int main() {
    int shared_var = 0;

    #pragma omp parallel for
    for (int i = 0; i < 100; i++) {
        #pragma omp critical
        {
            shared_var++;
            printf("Thread %d: shared_var = %d\n", omp_get_thread_num(), shared_var);
        }
    }

    return 0;
}
```

**atomic原子操作**
```c
#include <omp.h>
#include <stdio.h>

int main() {
    int counter = 0;

    #pragma omp parallel for
    for (int i = 0; i < 1000000; i++) {
        #pragma omp atomic
        counter++;
    }

    printf("Final counter value: %d\n", counter);
    return 0;
}
```

#### 数据共享属性

**private和firstprivate**
```c
#include <omp.h>
#include <stdio.h>

int main() {
    int shared_var = 10;

    #pragma omp parallel private(shared_var)
    {
        shared_var = omp_get_thread_num();
        printf("Thread %d: private shared_var = %d\n", omp_get_thread_num(), shared_var);
    }

    printf("Original shared_var = %d\n", shared_var);

    return 0;
}
```

**shared和default**
```c
#include <omp.h>
#include <stdio.h>

int main() {
    int a = 1, b = 2, c = 3, d = 4;

    #pragma omp parallel shared(a, b) private(c) firstprivate(d)
    {
        // a, b: shared
        // c: private
        // d: firstprivate (initialized with original value)
        printf("Thread %d: a=%d, b=%d, c=%d, d=%d\n",
               omp_get_thread_num(), a, b, c, d);
    }

    return 0;
}
```

### OpenMP高级特性

#### 线程亲和性

**线程绑定控制**
```bash
# 1. 环境变量设置
export OMP_NUM_THREADS=16
export OMP_PROC_BIND=TRUE
export OMP_PLACES=cores

# 2. 绑定策略
export OMP_PROC_BIND=spread    # 分散绑定
export OMP_PROC_BIND=close     # 集中绑定
export OMP_PROC_BIND=master    # 主线程绑定

# 3. 位置指定
export OMP_PLACES=cores        # 按核心绑定
export OMP_PLACES=threads      # 按线程绑定
export OMP_PLACES=sockets      # 按插槽绑定
```

**NUMA感知绑定**
```c
#include <omp.h>
#include <stdio.h>
#include <numa.h>

int main() {
    #pragma omp parallel
    {
        int thread_id = omp_get_thread_num();
        int numa_node = numa_node_of_cpu(sched_getcpu());

        printf("Thread %d running on NUMA node %d\n", thread_id, numa_node);
    }

    return 0;
}
```

#### OpenMP任务并行

**task指令**
```c
#include <omp.h>
#include <stdio.h>

void recursive_task(int n) {
    if (n <= 1) return;

    #pragma omp task
    recursive_task(n/2);

    #pragma omp task
    recursive_task(n/2);

    #pragma omp taskwait
    printf("Task %d completed\n", n);
}

int main() {
    #pragma omp parallel
    {
        #pragma omp single
        recursive_task(16);
    }

    return 0;
}
```

**taskloop指令**
```c
#include <omp.h>
#include <stdio.h>

int fibonacci(int n) {
    if (n <= 1) return n;
    return fibonacci(n-1) + fibonacci(n-2);
}

int main() {
    int n = 20;
    int result;

    #pragma omp parallel
    {
        #pragma omp single
        {
            #pragma omp taskloop reduction(+:result)
            for (int i = 0; i < n; i++) {
                result += fibonacci(i);
            }
        }
    }

    printf("Fibonacci sum: %d\n", result);
    return 0;
}
```

## 10.3 CUDA GPU编程

### CUDA基础概念

#### CUDA架构概述

**CUDA内存层次结构**
```
┌─────────────────────────────────────┐
│           Host Memory (CPU)         │
│  (系统主内存，通过PCIe访问)         │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│           PCI Express Bus           │
│  (CPU和GPU之间的通信通道)           │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│          Global Memory              │
│  (GPU全局内存，容量大，延迟高)       │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│           L2 Cache                  │
│  (GPU二级缓存，统一缓存)            │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│           L1 Cache                  │
│  (每个SM的L1缓存)                   │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│         Shared Memory               │
│  (每个SM的共享内存，容量小，速度快)  │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│        Register File                │
│  (每个线程的寄存器，最快访问)        │
└─────────────────────────────────────┘
```

### CUDA编程模型

#### CUDA线程层次

**线程组织结构**
```cuda
// CUDA线程层次
// Grid -> Block -> Thread
// 三级层次结构

__global__ void kernel_function() {
    // blockIdx: 块索引 (grid level)
    // threadIdx: 线程索引 (block level)
    // blockDim: 块维度
    // gridDim: 网格维度

    int global_id = blockIdx.x * blockDim.x + threadIdx.x;
    int block_id = blockIdx.x;
    int thread_id = threadIdx.x;
}
```

**线程配置示例**
```cuda
// 1. 一维配置
dim3 blockSize(256);
dim3 gridSize((N + blockSize.x - 1) / blockSize.x);
kernel_function<<<gridSize, blockSize>>>();

// 2. 二维配置
dim3 blockSize(16, 16);
dim3 gridSize((width + 15) / 16, (height + 15) / 16);
kernel_function<<<gridSize, blockSize>>>();

// 3. 三维配置
dim3 blockSize(8, 8, 8);
dim3 gridSize((N + 7) / 8, (M + 7) / 8, (P + 7) / 8);
kernel_function<<<gridSize, blockSize>>>();
```

#### CUDA内存管理

**内存分配函数**
```cuda
// 1. 主机内存分配
float *h_data;
h_data = (float*)malloc(size * sizeof(float));

// 2. 设备内存分配
float *d_data;
cudaMalloc(&d_data, size * sizeof(float));

// 3. 统一内存分配
float *u_data;
cudaMallocManaged(&u_data, size * sizeof(float));

// 4. 零拷贝内存
float *pinned_data;
cudaHostAlloc(&pinned_data, size * sizeof(float), cudaHostAllocMapped);
```

**内存拷贝函数**
```cuda
// 1. 主机到设备
cudaMemcpy(d_data, h_data, size * sizeof(float), cudaMemcpyHostToDevice);

// 2. 设备到主机
cudaMemcpy(h_data, d_data, size * sizeof(float), cudaMemcpyDeviceToHost);

// 3. 设备到设备
cudaMemcpy(d_dest, d_src, size * sizeof(float), cudaMemcpyDeviceToDevice);

// 4. 异步拷贝
cudaMemcpyAsync(d_data, h_data, size * sizeof(float),
                cudaMemcpyHostToDevice, stream);
```

### CUDA内核编程

#### 基本内核示例

**向量加法内核**
```cuda
#include <cuda_runtime.h>
#include <stdio.h>

// CUDA内核函数
__global__ void vector_add(float *a, float *b, float *c, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        c[idx] = a[idx] + b[idx];
    }
}

int main() {
    const int N = 1024 * 1024;
    const int size = N * sizeof(float);

    // 分配主机内存
    float *h_a = (float*)malloc(size);
    float *h_b = (float*)malloc(size);
    float *h_c = (float*)malloc(size);

    // 初始化数据
    for (int i = 0; i < N; i++) {
        h_a[i] = i;
        h_b[i] = i * 2;
    }

    // 分配设备内存
    float *d_a, *d_b, *d_c;
    cudaMalloc(&d_a, size);
    cudaMalloc(&d_b, size);
    cudaMalloc(&d_c, size);

    // 拷贝数据到设备
    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);

    // 配置执行参数
    int blockSize = 256;
    int gridSize = (N + blockSize - 1) / blockSize;

    // 启动内核
    vector_add<<<gridSize, blockSize>>>(d_a, d_b, d_c, N);

    // 等待内核完成
    cudaDeviceSynchronize();

    // 拷贝结果回主机
    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);

    // 验证结果
    for (int i = 0; i < 10; i++) {
        printf("h_c[%d] = %f\n", i, h_c[i]);
    }

    // 释放内存
    free(h_a); free(h_b); free(h_c);
    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);

    return 0;
}
```

#### 内存访问优化

**共享内存使用**
```cuda
__global__ void matrix_multiply_shared(float *A, float *B, float *C,
                                      int N, int M, int K) {
    // 共享内存声明
    __shared__ float shared_A[16][16];
    __shared__ float shared_B[16][16];

    int tx = threadIdx.x;
    int ty = threadIdx.y;
    int row = blockIdx.y * 16 + ty;
    int col = blockIdx.x * 16 + tx;

    float sum = 0.0f;

    // 分块计算
    for (int k = 0; k < (M + 15) / 16; k++) {
        // 加载数据到共享内存
        if (row < N && k * 16 + tx < M) {
            shared_A[ty][tx] = A[row * M + k * 16 + tx];
        } else {
            shared_A[ty][tx] = 0.0f;
        }

        if (k * 16 + ty < M && col < K) {
            shared_B[ty][tx] = B[(k * 16 + ty) * K + col];
        } else {
            shared_B[ty][tx] = 0.0f;
        }

        __syncthreads();

        // 计算部分和
        for (int i = 0; i < 16; i++) {
            sum += shared_A[ty][i] * shared_B[i][tx];
        }

        __syncthreads();
    }

    // 存储结果
    if (row < N && col < K) {
        C[row * K + col] = sum;
    }
}

#### CUDA 高级特性 (Cooperative Groups & C++20)

**Cooperative Groups (协作组)**
Cooperative Groups 提供了比 `__syncthreads()` 更灵活的线程同步机制。

```cuda
#include <cooperative_groups.h>
using namespace cooperative_groups;

__global__ void cg_kernel(int *count) {
    thread_block g = this_thread_block();

    // 组内同步
    g.sync();

    if (g.thread_rank() == 0) {
        printf("Block leader: %d\n", blockIdx.x);
    }
}
```

**C++20 Barrier (从 CUDA 11.x 开始支持)**
```cuda
#include <cuda/barrier>
#include <cuda/std/chrono>

 __global__ void barrier_example(int* data, int N) {
    #pragma nv_diag_suppress 177
    __shared__ cuda::barrier<cuda::thread_scope_block> bar;
    
    if (threadIdx.x == 0) {
        init(&bar, blockDim.x);
    }
    __syncthreads();

    // 异步等待
    cuda::memcpy_async(shared, global, size, bar);
    bar.arrive_and_wait();
 }
``````

**纹理内存使用**
```cuda
// 纹理内存声明
texture<float, cudaTextureType1D, cudaReadModeElementType> tex_A;
texture<float, cudaTextureType1D, cudaReadModeElementType> tex_B;

__global__ void interpolation_kernel(float *result, int N) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < N) {
        float x = idx / (float)(N - 1);
        float a_val = tex1D(tex_A, x);
        float b_val = tex1D(tex_B, x);
        result[idx] = a_val * 0.5f + b_val * 0.5f;
    }
}

int main() {
    // 绑定纹理
    cudaBindTexture(NULL, tex_A, d_A, N * sizeof(float));
    cudaBindTexture(NULL, tex_B, d_B, N * sizeof(float));

    // 执行内核
    dim3 blockSize(256);
    dim3 gridSize((N + blockSize.x - 1) / blockSize.x);
    interpolation_kernel<<<gridSize, blockSize>>>(d_result, N);

    // 解绑纹理
    cudaUnbindTexture(tex_A);
    cudaUnbindTexture(tex_B);

    return 0;
}
```

### 10.4 OpenACC加速计算 (逐渐向 OpenMP Offloading 迁移)

尽管 OpenACC 依然被 PGI/NVIDIA 编译器支持，但社区趋势正在转向 **OpenMP 5.x Offloading**。建议新项目优先考虑 OpenMP Target Offloading。

#### OpenACC 基础指令 (与 OpenMP Target 对照)

| OpenACC | OpenMP 5.x Target |
| :--- | :--- |
| `#pragma acc parallel loop` | `#pragma omp target teams distribute parallel for` |
| `#pragma acc data copyin(a)` | `#pragma omp target data map(to:a)` |
| `#pragma acc enter data` | `#pragma omp target enter data` |

### OpenACC指令详解

#### 计算指令

**parallel指令**
```c
#include <openacc.h>
#include <stdio.h>

int main() {
    int n = 1000;
    float *a = (float*)malloc(n * sizeof(float));
    float *b = (float*)malloc(n * sizeof(float));
    float *c = (float*)malloc(n * sizeof(float));

    // 初始化数据
    for (int i = 0; i < n; i++) {
        a[i] = i;
        b[i] = i * 2;
    }

    // 并行计算区域
    #pragma acc parallel loop
    for (int i = 0; i < n; i++) {
        c[i] = a[i] + b[i];
    }

    // 输出结果
    for (int i = 0; i < 10; i++) {
        printf("c[%d] = %f\n", i, c[i]);
    }

    free(a); free(b); free(c);
    return 0;
}
```

**kernels指令**
```c
#include <openacc.h>
#include <stdio.h>

int main() {
    int n = 1000;
    float *a = (float*)malloc(n * sizeof(float));
    float *b = (float*)malloc(n * sizeof(float));
    float *c = (float*)malloc(n * sizeof(float));

    // 初始化数据
    for (int i = 0; i < n; i++) {
        a[i] = i;
        b[i] = i * 2;
    }

    // 内核区域，编译器自动分析并行性
    #pragma acc kernels
    {
        for (int i = 0; i < n; i++) {
            a[i] = a[i] * 2;
        }

        for (int i = 0; i < n; i++) {
            b[i] = b[i] + 1;
        }

        for (int i = 0; i < n; i++) {
            c[i] = a[i] + b[i];
        }
    }

    free(a); free(b); free(c);
    return 0;
}
```

#### 数据管理指令

**数据传输指令**
```c
#include <openacc.h>
#include <stdio.h>

int main() {
    int n = 1000;
    float *a = (float*)malloc(n * sizeof(float));
    float *b = (float*)malloc(n * sizeof(float));
    float *c = (float*)malloc(n * sizeof(float));

    // 初始化数据
    for (int i = 0; i < n; i++) {
        a[i] = i;
        b[i] = i * 2;
    }

    // 数据传输和计算
    #pragma acc data copyin(a[0:n], b[0:n]) copyout(c[0:n])
    {
        #pragma acc parallel loop
        for (int i = 0; i < n; i++) {
            c[i] = a[i] + b[i];
        }
    }

    free(a); free(b); free(c);
    return 0;
}
```

**数据区域管理**
```c
#include <openacc.h>
#include <stdio.h>

int main() {
    int n = 1000;
    float *a = (float*)malloc(n * sizeof(float));
    float *b = (float*)malloc(n * sizeof(float));
    float *c = (float*)malloc(n * sizeof(float));

    // 创建数据区域
    #pragma acc enter data create(a[0:n], b[0:n], c[0:n])

    // 初始化数据
    for (int i = 0; i < n; i++) {
        a[i] = i;
        b[i] = i * 2;
    }
    #pragma acc update device(a[0:n], b[0:n])

    // 并行计算
    #pragma acc parallel loop present(a, b, c)
    for (int i = 0; i < n; i++) {
        c[i] = a[i] + b[i];
    }

    // 获取结果
    #pragma acc update host(c[0:n])

    // 销毁数据区域
    #pragma acc exit data delete(a, b, c)

    free(a); free(b); free(c);
    return 0;
}
```

#### 循环指令

**loop指令优化**
```c
#include <openacc.h>
#include <stdio.h>

int main() {
    int n = 1000;
    float *a = (float*)malloc(n * sizeof(float));
    float *b = (float*)malloc(n * sizeof(float));
    float *c = (float*)malloc(n * sizeof(float));

    // 初始化数据
    for (int i = 0; i < n; i++) {
        a[i] = i;
        b[i] = i * 2;
    }

    // 循环优化
    #pragma acc parallel loop gang vector
    for (int i = 0; i < n; i++) {
        c[i] = a[i] + b[i];
    }

    // 嵌套循环优化
    #pragma acc parallel loop gang
    for (int i = 0; i < 100; i++) {
        #pragma acc loop vector
        for (int j = 0; j < 100; j++) {
            a[i*100 + j] = i + j;
        }
    }

    free(a); free(b); free(c);
    return 0;
}
```

### OpenACC性能优化

#### 编译器优化选项

**PGI编译器选项**
```bash
# 1. 基本编译
pgcc -acc -o program program.c

# 2. 性能分析
pgcc -acc -Minfo=accel -o program program.c

# 3. 优化级别
pgcc -acc -O3 -o program program.c

# 4. GPU信息
pgcc -acc -gpu=info -o program program.c
```

**GCC编译器选项**
```bash
# 1. 基本编译
gcc -fopenacc -o program program.c

# 2. 性能信息
gcc -fopenacc -fopt-info-omp -o program program.c

# 3. 优化级别
gcc -fopenacc -O3 -o program program.c
```

#### 性能分析工具

**NVIDIA Nsight**
```bash
# 1. 基本分析
nsys profile ./program

# 2. OpenACC分析
nsys profile --trace=openacc ./program

# 3. 性能报告
nsys stats report1.nsys-rep
```

**PGI性能分析**
```bash
# 1. 编译时信息
pgcc -acc -Minfo=accel program.c

# 2. 运行时分析
pgprof ./program
```

## 10.5 SYCL异构计算编程

### SYCL基础概念

#### SYCL架构概述

**SYCL执行模型**
```
┌─────────────────────────────────────┐
│           Host (CPU)                │
│  ┌─────────────────────────────────┐ │
│  │    SYCL Runtime                 │ │
│  │  (管理设备和内存)                │ │
│  └─────────────────────────────────┘ │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│        Device Selection             │
│  (CPU, GPU, FPGA, Accelerator)      │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│         Compute Device              │
│  ┌─────────────────────────────────┐ │
│  │      Kernels                    │ │
│  │  (并行计算任务)                  │ │
│  └─────────────────────────────────┘ │
└─────────────────────────────────────┘
```

**SYCL内存模型**
- **Host Memory**：主机内存，CPU可直接访问
- **Device Memory**：设备内存，GPU/FPGA等设备访问
- **Unified Memory**：统一内存，自动管理数据迁移
- **Buffer/Accessors**：SYCL内存管理抽象

### SYCL实现框架

#### Intel oneAPI DPC++

**安装配置**
```bash
# 1. 下载Intel oneAPI
wget https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB
sudo apt-key add GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB

# 2. 添加仓库
echo "deb https://apt.repos.intel.com/oneapi all main" | sudo tee /etc/apt/sources.list.d/oneAPI.list

# 3. 安装DPC++
sudo apt-get update
sudo apt-get install intel-basekit

# 4. 设置环境
source /opt/intel/oneapi/setvars.sh
```

**基础编译**
```bash
# 编译SYCL程序
dpcpp -o program program.cpp

# 启用特定设备
dpcpp -fsycl-targets=nvptx64-nvidia-cuda program.cpp
dpcpp -fsycl-targets=spir64_fpga-unknown-unknown program.cpp
```

#### Khronos SYCL

**安装配置**
```bash
# 1. 安装ComputeCpp
wget https://developer.codeplay.com/products/computecpp/latest/computecpp-ubuntu-20.04-2.7.0.tar.gz
tar -xzf computecpp-ubuntu-20.04-2.7.0.tar.gz
cd ComputeCpp-CE-2.7.0-Ubuntu-20.04-1/

# 2. 设置环境
export COMPUTECPP_DIR=/path/to/ComputeCpp-CE-2.7.0-Ubuntu-20.04-1
export PATH=$COMPUTECPP_DIR/bin:$PATH
export LD_LIBRARY_PATH=$COMPUTECPP_DIR/lib:$LD_LIBRARY_PATH
```

### SYCL编程基础

#### 基本SYCL程序结构

**Hello World示例**
```cpp
#include <CL/sycl.hpp>
#include <iostream>

using namespace sycl;

int main() {
    // 1. 创建队列
    queue q;

    // 2. 获取设备信息
    auto device = q.get_device();
    std::cout << "Running on: " << device.get_info<info::device::name>() << std::endl;

    // 3. 创建缓冲区
    const int N = 1024;
    buffer<int, 1> buffer_a(range<1>(N));
    buffer<int, 1> buffer_b(range<1>(N));
    buffer<int, 1> buffer_c(range<1>(N));

    // 4. 初始化数据
    {
        auto a = buffer_a.get_access<access::mode::write>();
        auto b = buffer_b.get_access<access::mode::write>();

        for (int i = 0; i < N; i++) {
            a[i] = i;
            b[i] = i * 2;
        }
    }

    // 5. 提交内核
    q.submit([&](handler& h) {
        // 获取访问器
        auto accessor_a = buffer_a.get_access<access::mode::read>(h);
        auto accessor_b = buffer_b.get_access<access::mode::read>(h);
        auto accessor_c = buffer_c.get_access<access::mode::write>(h);

        // 并行内核
        h.parallel_for(range<1>(N), [=](id<1> idx) {
            accessor_c[idx] = accessor_a[idx] + accessor_b[idx];
        });
    });

    // 6. 获取结果
    {
        auto c = buffer_c.get_access<access::mode::read>();
        std::cout << "Result[0] = " << c[0] << std::endl;
        std::cout << "Result[100] = " << c[100] << std::endl;
    }

    return 0;
}
```

#### SYCL内存管理

**Buffer和Accessor**
```cpp
#include <CL/sycl.hpp>

using namespace sycl;

int main() {
    queue q;

    const int N = 1024;

    // 1. 创建buffer
    buffer<float, 1> data(range<1>(N));

    // 2. 使用accessor访问数据
    q.submit([&](handler& h) {
        // 只读访问器
        auto read_accessor = data.get_access<access::mode::read>(h);

        // 只写访问器
        auto write_accessor = data.get_access<access::mode::write>(h);

        // 读写访问器
        auto read_write_accessor = data.get_access<access::mode::read_write>(h);

        h.parallel_for(range<1>(N), [=](id<1> idx) {
            write_accessor[idx] = read_accessor[idx] * 2.0f;
        });
    });

    return 0;
}
```

**Unified Shared Memory (USM)**
```cpp
#include <CL/sycl.hpp>

using namespace sycl;

int main() {
    queue q;

    const int N = 1024;

    // 1. 分配USM内存
    float* data = malloc_shared<float>(N, q);

    // 2. 初始化数据
    for (int i = 0; i < N; i++) {
        data[i] = i;
    }

    // 3. 在设备上执行
    q.submit([&](handler& h) {
        h.parallel_for(range<1>(N), [=](id<1> idx) {
            data[idx] *= 2.0f;
        });
    });

    // 4. 主机上使用结果
    std::cout << "Result[0] = " << data[0] << std::endl;

    // 5. 释放内存
    free(data, q);

    return 0;
}
```

#### SYCL并行执行

**并行for循环**
```cpp
#include <CL/sycl.hpp>

using namespace sycl;

int main() {
    queue q;

    const int N = 1024;
    buffer<int, 1> result(range<1>(N));

    // 1. 一维并行
    q.submit([&](handler& h) {
        auto acc = result.get_access<access::mode::write>(h);
        h.parallel_for(range<1>(N), [=](id<1> idx) {
            acc[idx] = idx[0] * idx[0];
        });
    });

    // 2. 二维并行
    const int M = 32, K = 32;
    buffer<int, 2> matrix(range<2>(M, K));

    q.submit([&](handler& h) {
        auto acc = matrix.get_access<access::mode::write>(h);
        h.parallel_for(range<2>(M, K), [=](id<2> idx) {
            acc[idx] = idx[0] * idx[1];
        });
    });

    // 3. 三维并行
    const int X = 16, Y = 16, Z = 16;
    buffer<int, 3> cube(range<3>(X, Y, Z));

    q.submit([&](handler& h) {
        auto acc = cube.get_access<access::mode::write>(h);
        h.parallel_for(range<3>(X, Y, Z), [=](id<3> idx) {
            acc[idx] = idx[0] + idx[1] + idx[2];
        });
    });

    return 0;
}
```

**工作组和子组**
```cpp
#include <CL/sycl.hpp>

using namespace sycl;

int main() {
    queue q;

    const int N = 1024;
    buffer<float, 1> data(range<1>(N));

    q.submit([&](handler& h) {
        auto acc = data.get_access<access::mode::write>(h);

        h.parallel_for_work_group(range<1>(N), range<1>(256),
        [=](group<1> group) {
            // 工作组级别的操作
            group.parallel_for_work_item([&](id<1> idx) {
                // 工作项级别的操作
                acc[idx] = idx[0] * 2.0f;
            });
        });
    });

    return 0;
}
```

### SYCL高级特性

#### 设备选择和查询

**设备查询**
```cpp
#include <CL/sycl.hpp>

using namespace sycl;

int main() {
    // 1. 获取所有可用平台
    std::vector<platform> platforms = platform::get_platforms();

    for (const auto& platform : platforms) {
        std::cout << "Platform: " << platform.get_info<info::platform::name>() << std::endl;

        // 2. 获取平台上的设备
        std::vector<device> devices = platform.get_devices(device_type::all);

        for (const auto& device : devices) {
            std::cout << "  Device: " << device.get_info<info::device::name>() << std::endl;
            std::cout << "  Type: " << device.get_info<info::device::device_type>() << std::endl;
            std::cout << "  Compute Units: " << device.get_info<info::device::max_compute_units>() << std::endl;
            std::cout << "  Global Memory: " << device.get_info<info::device::global_mem_size>() / (1024*1024) << " MB" << std::endl;
        }
    }

    // 3. 创建特定设备的队列
    gpu_selector gpu;
    cpu_selector cpu;
    default_selector def;

    queue gpu_queue(gpu);
    queue cpu_queue(cpu);

    return 0;
}
```

**设备选择策略**
```cpp
// 1. 基于性能的设备选择
struct performance_selector {
    device operator()(const device& dev) const {
        if (dev.is_gpu()) {
            return dev;
        }
        return {};
    }
};

// 2. 基于内存的设备选择
struct memory_selector {
    device operator()(const device& dev) const {
        size_t mem_size = dev.get_info<info::device::global_mem_size>();
        if (mem_size > 4ULL * 1024 * 1024 * 1024) { // 4GB
            return dev;
        }
        return {};
    }
};

// 3. 使用自定义选择器
queue q(performance_selector{});
```

#### 异步执行和事件管理

**事件依赖**
```cpp
#include <CL/sycl.hpp>

using namespace sycl;

int main() {
    queue q;

    const int N = 1024;
    buffer<int, 1> a(range<1>(N));
    buffer<int, 1> b(range<1>(N));
    buffer<int, 1> c(range<1>(N));

    // 1. 第一个任务
    event e1 = q.submit([&](handler& h) {
        auto acc_a = a.get_access<access::mode::write>(h);
        h.parallel_for(range<1>(N), [=](id<1> idx) {
            acc_a[idx] = idx[0] * 2;
        });
    });

    // 2. 第二个任务（依赖于第一个）
    event e2 = q.submit([&](handler& h) {
        h.depends_on(e1);
        auto acc_b = b.get_access<access::mode::write>(h);
        h.parallel_for(range<1>(N), [=](id<1> idx) {
            acc_b[idx] = idx[0] * 3;
        });
    });

    // 3. 第三个任务（依赖于前两个）
    event e3 = q.submit([&](handler& h) {
        h.depends_on({e1, e2});
        auto acc_a = a.get_access<access::mode::read>(h);
        auto acc_b = b.get_access<access::mode::read>(h);
        auto acc_c = c.get_access<access::mode::write>(h);

        h.parallel_for(range<1>(N), [=](id<1> idx) {
            acc_c[idx] = acc_a[idx] + acc_b[idx];
        });
    });

    // 4. 等待所有任务完成
    e3.wait();

    return 0;
}
```

**流式执行**
```cpp
#include <CL/sycl.hpp>

using namespace sycl;

int main() {
    queue q;

    const int N = 1024;
    const int num_chunks = 4;
    const int chunk_size = N / num_chunks;

    std::vector<buffer<int, 1>> buffers(num_chunks);

    // 创建多个缓冲区
    for (int i = 0; i < num_chunks; i++) {
        buffers[i] = buffer<int, 1>(range<1>(chunk_size));
    }

    std::vector<event> events;

    // 流式处理
    for (int i = 0; i < num_chunks; i++) {
        event e = q.submit([&](handler& h) {
            auto acc = buffers[i].get_access<access::mode::write>(h);
            h.parallel_for(range<1>(chunk_size), [=](id<1> idx) {
                acc[idx] = (i * chunk_size + idx[0]) * 2;
            });
        });
        events.push_back(e);
    }

    // 等待所有任务完成
    for (auto& e : events) {
        e.wait();
    }

    return 0;
}
```

#### 性能优化

**内存访问优化**
```cpp
#include <CL/sycl.hpp>

using namespace sycl;

// 1. 使用局部内存优化
int main() {
    queue q;

    const int N = 1024;
    const int block_size = 256;

    buffer<float, 1> input(range<1>(N));
    buffer<float, 1> output(range<1>(N));

    q.submit([&](handler& h) {
        auto input_acc = input.get_access<access::mode::read>(h);
        auto output_acc = output.get_access<access::mode::write>(h);

        // 声明局部内存
        local_accessor<float, 1> local_mem(range<1>(block_size), h);

        h.parallel_for_work_group(range<1>(N), range<1>(block_size),
        [=](group<1> group) {
            // 加载数据到局部内存
            group.parallel_for_work_item([&](id<1> global_id) {
                size_t local_id = group.leader() + global_id[0];
                if (local_id < N) {
                    local_mem[global_id[0]] = input_acc[local_id];
                }
            });

            // 同步工作组
            group.barrier();

            // 处理局部内存中的数据
            group.parallel_for_work_item([&](id<1> global_id) {
                size_t local_id = group.leader() + global_id[0];
                if (local_id < N) {
                    // 访问相邻元素（使用局部内存减少全局内存访问）
                    float sum = local_mem[global_id[0]];
                    if (global_id[0] > 0) {
                        sum += local_mem[global_id[0] - 1];
                    }
                    if (global_id[0] < block_size - 1) {
                        sum += local_mem[global_id[0] + 1];
                    }
                    output_acc[local_id] = sum / 3.0f;
                }
            });
        });
    });

    return 0;
}
```

**向量化操作**
```cpp
#include <CL/sycl.hpp>

using namespace sycl;

int main() {
    queue q;

    const int N = 1024;
    buffer<float4, 1> input(range<1>(N/4));  // 使用float4向量类型
    buffer<float4, 1> output(range<1>(N/4));

    q.submit([&](handler& h) {
        auto input_acc = input.get_access<access::mode::read>(h);
        auto output_acc = output.get_access<access::mode::write>(h);

        h.parallel_for(range<1>(N/4), [=](id<1> idx) {
            // 向量化的操作
            float4 vec = input_acc[idx];
            float4 result;

            // SIMD操作
            result.x() = vec.x() * 2.0f;
            result.y() = vec.y() * 2.0f;
            result.z() = vec.z() * 2.0f;
            result.w() = vec.w() * 2.0f;

            output_acc[idx] = result;
        });
    });

    return 0;
}
```

### SYCL与其他框架对比

#### SYCL vs CUDA
```cpp
// CUDA版本
__global__ void vector_add_cuda(float* a, float* b, float* c, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        c[idx] = a[idx] + b[idx];
    }
}

// SYCL版本
void vector_add_sycl(queue& q, float* a, float* b, float* c, int n) {
    buffer<float, 1> buf_a(a, range<1>(n));
    buffer<float, 1> buf_b(b, range<1>(n));
    buffer<float, 1> buf_c(c, range<1>(n));

    q.submit([&](handler& h) {
        auto acc_a = buf_a.get_access<access::mode::read>(h);
        auto acc_b = buf_b.get_access<access::mode::read>(h);
        auto acc_c = buf_c.get_access<access::mode::write>(h);

        h.parallel_for(range<1>(n), [=](id<1> idx) {
            acc_c[idx] = acc_a[idx] + acc_b[idx];
        });
    });
}
```

#### SYCL vs OpenCL
```cpp
// OpenCL版本
cl_program program = clCreateProgramWithSource(context, 1, &source, NULL, NULL);
clBuildProgram(program, 1, &device, NULL, NULL, NULL);
cl_kernel kernel = clCreateKernel(program, "vector_add", NULL);

// SYCL版本（更简洁）
q.submit([&](handler& h) {
    h.parallel_for(range<1>(n), [=](id<1> idx) {
        // 内核代码
    });
});
```

### SYCL性能分析

#### 编译器优化选项
```bash
# Intel DPC++编译器
dpcpp -O3 -fsycl-targets=spir64_gen -Xsycl-target-backend "-O3" program.cpp

# AMD HIP-SYCL编译器
hipSYCL-syclcc -O3 --hipsycl-targets=hip:gfx906 program.cpp

# Codeplay ComputeCpp
computecpp_compiler -O3 -sycl-driver program.cpp
```

#### 性能分析工具
```cpp
#include <CL/sycl.hpp>
#include <chrono>
#include <iostream>

using namespace sycl;

int main() {
    queue q;

    const int N = 1000000;

    buffer<float, 1> a(range<1>(N));
    buffer<float, 1> b(range<1>(N));
    buffer<float, 1> c(range<1>(N));

    // 性能计时
    auto start = std::chrono::high_resolution_clock::now();

    event e = q.submit([&](handler& h) {
        auto acc_a = a.get_access<access::mode::read>(h);
        auto acc_b = b.get_access<access::mode::read>(h);
        auto acc_c = c.get_access<access::mode::write>(h);

        h.parallel_for(range<1>(N), [=](id<1> idx) {
            acc_c[idx] = acc_a[idx] + acc_b[idx];
        });
    });

    e.wait();

    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);

    std::cout << "Execution time: " << duration.count() << " microseconds" << std::endl;

    return 0;
}
```

## 本章小结

并行编程环境是HPC应用开发的核心。本章详细介绍了：

1. **MPI编程环境**：OpenMPI、Intel MPI、MVAPICH2的安装配置，MPI编程基础和高级特性
2. **OpenMP多线程**：编译器支持、指令详解、线程管理和任务并行
3. **CUDA GPU编程**：CUDA架构、内存管理、内核编程和性能优化
4. **OpenACC加速计算**：执行模型、指令详解和性能优化
5. **SYCL异构计算**：跨平台异构编程模型、内存管理、设备选择和性能优化

掌握这些并行编程技术有助于：
- 开发高效的并行应用程序
- 充分利用多核CPU和GPU资源
- 构建跨平台的异构计算应用
- 优化计算密集型应用性能
- 构建可扩展的并行计算系统

在实际应用中，需要根据具体的硬件平台、应用特性和性能需求，选择合适的并行编程模型和技术。现代HPC应用往往需要结合多种并行编程模型，以充分利用异构计算资源的性能优势。