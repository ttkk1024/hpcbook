# 第10章 并行编程环境

## 10.1 MPI编程环境

### MPI基础概念

#### MPI架构概述

**MPI通信模型**
```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Process 0     │    │   Process 1     │    │   Process 2     │
│                 │    │                 │    │                 │
│  ┌───────────┐  │    │  ┌───────────┐  │    │  ┌───────────┐  │
│  │ Application│  │    │  │ Application│  │    │  │ Application│  │
│  └───────────┘  │    │  └───────────┘  │    │  └───────────┘  │
│                 │    │                 │    │                 │
│  ┌───────────┐  │    │  ┌───────────┐  │    │  ┌───────────┐  │
│  │ MPI Lib   │  │    │  │ MPI Lib   │  │    │  │ MPI Lib   │  │
│  └───────────┘  │    │  └───────────┘  │    │  └───────────┘  │
│                 │    │                 │    │                 │
│  ┌───────────┐  │    │  ┌───────────┐  │    │  ┌───────────┐  │
│  │ Transport │  │    │  │ Transport │  │    │  │ Transport │  │
│  │ Layer     │  │    │  │ Layer     │  │    │  │ Layer     │  │
│  └───────────┘  │    │  └───────────┘  │    │  └───────────┘  │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                       │                       │
         └───────────────────────┼───────────────────────┘
                                 │
                    ┌─────────────────┐
                    │  Network Layer  │
                    │  (TCP/IP, IB)   │
                    └─────────────────┘
```

**MPI通信类型**
- **点对点通信**：进程间直接通信
- **集合通信**：多进程协调通信
- **非阻塞通信**：异步通信机制
- **单边通信**：RMA (Remote Memory Access)

### MPI库安装与配置

#### OpenMPI安装

**源码编译安装**
```bash
# 1. 下载OpenMPI
wget https://download.open-mpi.org/release/open-mpi/v5.0/openmpi-5.0.1.tar.gz
tar -xzf openmpi-5.0.1.tar.gz
cd openmpi-5.0.1

# 2. 配置编译选项
./configure --prefix=/opt/openmpi-5.0.1 \
            --enable-mpi-cxx \
            --enable-mpi-fortran \
            --enable-mpi-thread-multiple \
            --with-ucx=/opt/ucx \
            --with-ofi=/opt/libfabric \
            --enable-debug

# 3. 编译安装
make -j$(nproc)
make install

# 4. 设置环境变量
echo 'export PATH=/opt/openmpi-5.0.1/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/opt/openmpi-5.0.1/lib:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc
```

**包管理器安装**
```bash
# CentOS/RHEL
yum install openmpi openmpi-devel
module load openmpi

# Ubuntu/Debian
apt-get install openmpi-bin openmpi-common libopenmpi-dev
```

#### Intel MPI安装

**Intel OneAPI MPI**
```bash
# 1. 安装Intel OneAPI
source /opt/intel/oneapi/setvars.sh

# 2. 验证MPI安装
mpiifort --version
mpiicc --version
mpic++ --version

# 3. 测试MPI
mpirun -n 2 hostname
```

#### MVAPICH2安装

**MVAPICH2编译安装**
```bash
# 1. 下载MVAPICH2
wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.7.tar.gz
tar -xzf mvapich2-2.3.7.tar.gz
cd mvapich2-2.3.7

# 2. 配置编译
./configure --prefix=/opt/mvapich2-2.3.7 \
            --enable-shared \
            --enable-threads=multiple \
            --with-device=ch3:mrail \
            --with-rdma=gen2 \
            --enable-cxx \
            --enable-fortran

# 3. 编译安装
make -j$(nproc)
make install
```

### MPI编程基础

#### 基本MPI程序

**Hello World程序**
```c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char *argv[]) {
    int rank, size;

    // 初始化MPI
    MPI_Init(&argc, &argv);

    // 获取进程信息
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 打印进程信息
    printf("Hello from process %d of %d\n", rank, size);

    // 结束MPI
    MPI_Finalize();
    return 0;
}
```

**编译和运行**
```bash
# 编译
mpicc -o hello hello.c

# 运行
mpirun -n 4 ./hello
# 或
mpiexec -n 4 ./hello
```

#### MPI点对点通信

**基本通信函数**
```c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char *argv[]) {
    int rank, size;
    int data;
    MPI_Status status;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    if (rank == 0) {
        // 发送数据
        data = 42;
        MPI_Send(&data, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);
        printf("Process 0 sent data: %d\n", data);
    } else if (rank == 1) {
        // 接收数据
        MPI_Recv(&data, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);
        printf("Process 1 received data: %d\n", data);
    }

    MPI_Finalize();
    return 0;
}
```

**非阻塞通信**
```c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char *argv[]) {
    int rank, size;
    int send_data = 100, recv_data;
    MPI_Request request;
    MPI_Status status;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    if (rank == 0) {
        // 非阻塞发送
        MPI_Isend(&send_data, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, &request);
        printf("Process 0 started sending data: %d\n", send_data);

        // 等待发送完成
        MPI_Wait(&request, &status);
        printf("Process 0 send completed\n");
    } else if (rank == 1) {
        // 非阻塞接收
        MPI_Irecv(&recv_data, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &request);

        // 做其他工作
        printf("Process 1 doing other work...\n");

        // 等待接收完成
        MPI_Wait(&request, &status);
        printf("Process 1 received data: %d\n", recv_data);
    }

    MPI_Finalize();
    return 0;
}
```

#### MPI集合通信

**广播操作**
```c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char *argv[]) {
    int rank, size;
    int data;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    if (rank == 0) {
        data = 123;
        printf("Process 0 broadcasting data: %d\n", data);
    }

    // 广播
    MPI_Bcast(&data, 1, MPI_INT, 0, MPI_COMM_WORLD);

    printf("Process %d received data: %d\n", rank, data);

    MPI_Finalize();
    return 0;
}
```

**归约操作**
```c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char *argv[]) {
    int rank, size;
    int local_sum = rank * 10;
    int global_sum;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    printf("Process %d local sum: %d\n", rank, local_sum);

    // 求和归约
    MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("Global sum: %d\n", global_sum);
    }

    MPI_Finalize();
    return 0;
}
```

**全归约操作**
```c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char *argv[]) {
    int rank, size;
    int local_data = rank + 1;
    int global_sum;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 全归约，所有进程都得到结果
    MPI_Allreduce(&local_data, &global_sum, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);

    printf("Process %d: local_data=%d, global_sum=%d\n", rank, local_data, global_sum);

    MPI_Finalize();
    return 0;
}
```

### MPI高级特性

#### MPI进程拓扑

**进程拓扑创建**
```c
#include <mpi.h>
#include <stdio.h>
#include <math.h>

int main(int argc, char *argv[]) {
    int rank, size;
    int dims[2], periods[2], coords[2];
    int cart_rank;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 创建2D网格拓扑
    dims[0] = dims[1] = (int)sqrt(size);
    periods[0] = periods[1] = 0;  // 非周期性边界

    MPI_Comm cart_comm;
    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 0, &cart_comm);

    // 获取进程坐标
    MPI_Cart_coords(cart_comm, rank, 2, coords);
    printf("Process %d coordinates: (%d, %d)\n", rank, coords[0], coords[1]);

    // 获取邻居进程
    int left, right, up, down;
    MPI_Cart_shift(cart_comm, 0, 1, &up, &down);
    MPI_Cart_shift(cart_comm, 1, 1, &left, &right);

    printf("Process %d neighbors: up=%d, down=%d, left=%d, right=%d\n",
           rank, up, down, left, right);

    MPI_Comm_free(&cart_comm);
    MPI_Finalize();
    return 0;
}
```

#### MPI派生数据类型

**自定义数据类型**
```c
#include <mpi.h>
#include <stdio.h>

typedef struct {
    int id;
    float value;
    double timestamp;
} DataPacket;

int main(int argc, char *argv[]) {
    int rank, size;
    DataPacket send_data, recv_data;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 创建自定义数据类型
    int blocklengths[3] = {1, 1, 1};
    MPI_Datatype types[3] = {MPI_INT, MPI_FLOAT, MPI_DOUBLE};
    MPI_Aint displacements[3];

    DataPacket temp;
    MPI_Get_address(&temp.id, &displacements[0]);
    MPI_Get_address(&temp.value, &displacements[1]);
    MPI_Get_address(&temp.timestamp, &displacements[2]);

    // 相对位移
    MPI_Aint base;
    MPI_Get_address(&temp, &base);
    for (int i = 0; i < 3; i++) {
        displacements[i] -= base;
    }

    MPI_Datatype data_type;
    MPI_Type_create_struct(3, blocklengths, displacements, types, &data_type);
    MPI_Type_commit(&data_type);

    if (rank == 0) {
        send_data.id = 1;
        send_data.value = 3.14f;
        send_data.timestamp = 1234567890.0;

        MPI_Send(&send_data, 1, data_type, 1, 0, MPI_COMM_WORLD);
    } else if (rank == 1) {
        MPI_Recv(&recv_data, 1, data_type, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        printf("Received: id=%d, value=%f, timestamp=%f\n",
               recv_data.id, recv_data.value, recv_data.timestamp);
    }

    MPI_Type_free(&data_type);
    MPI_Finalize();
    return 0;
}
```

## 10.2 OpenMP多线程

### OpenMP基础概念

#### OpenMP执行模型

**线程层次结构**
```
┌─────────────────────────────────────┐
│            Master Thread            │
│  (主线程，线程ID = 0)               │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│             Fork Phase              │
│  创建工作线程                       │
└─────────────────────────────────────┘
                       │
┌─────────────┬─────────────┬─────────┐
│   Thread 0  │   Thread 1  │ Thread 2│
│  (主线程)   │  (工作线程) │ (工作线程)│
└─────────────┴─────────────┴─────────┘
                       │
┌─────────────────────────────────────┐
│            Join Phase               │
│  工作线程结束，主线程继续执行        │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│        Sequential Region            │
│  单线程执行                         │
└─────────────────────────────────────┘
```

### OpenMP编译器支持

#### 编译器配置

**GCC OpenMP支持**
```bash
# 1. 检查OpenMP支持
gcc -fopenmp -dM -E - < /dev/null | grep -i openmp

# 2. 编译OpenMP程序
gcc -fopenmp -O3 -o program program.c

# 3. 运行时控制
export OMP_NUM_THREADS=16
export OMP_PROC_BIND=TRUE
export OMP_PLACES=cores
```

**Intel编译器OpenMP**
```bash
# 1. 编译
icc -qopenmp -O3 -o program program.c

# 2. 运行时控制
export OMP_NUM_THREADS=16
export KMP_AFFINITY=scatter
```

#### OpenMP版本检查

```c
#include <omp.h>
#include <stdio.h>

int main() {
    printf("OpenMP version: %d\n", _OPENMP);
    printf("Number of processors: %d\n", omp_get_num_procs());
    printf("Max threads: %d\n", omp_get_max_threads());

    return 0;
}
```

### OpenMP指令详解

#### 并行区域指令

**基本并行区域**
```c
#include <omp.h>
#include <stdio.h>

int main() {
    #pragma omp parallel
    {
        int thread_id = omp_get_thread_num();
        int num_threads = omp_get_num_threads();
        printf("Hello from thread %d of %d\n", thread_id, num_threads);
    }

    return 0;
}
```

**线程数量控制**
```c
#include <omp.h>
#include <stdio.h>

int main() {
    #pragma omp parallel num_threads(4)
    {
        int thread_id = omp_get_thread_num();
        printf("Thread %d executing\n", thread_id);
    }

    return 0;
}
```

#### 工作共享指令

**for循环并行化**
```c
#include <omp.h>
#include <stdio.h>

int main() {
    int n = 100;
    int sum = 0;

    #pragma omp parallel for reduction(+:sum)
    for (int i = 0; i < n; i++) {
        sum += i;
    }

    printf("Sum: %d\n", sum);
    return 0;
}
```

**循环调度策略**
```c
#include <omp.h>
#include <stdio.h>

int main() {
    int n = 100;

    // 静态调度
    #pragma omp parallel for schedule(static, 10)
    for (int i = 0; i < n; i++) {
        printf("Thread %d: iteration %d\n", omp_get_thread_num(), i);
    }

    // 动态调度
    #pragma omp parallel for schedule(dynamic, 5)
    for (int i = 0; i < n; i++) {
        printf("Thread %d: iteration %d\n", omp_get_thread_num(), i);
    }

    // 运行时调度
    #pragma omp parallel for schedule(runtime)
    for (int i = 0; i < n; i++) {
        printf("Thread %d: iteration %d\n", omp_get_thread_num(), i);
    }

    return 0;
}
```

**sections指令**
```c
#include <omp.h>
#include <stdio.h>

void task1() { printf("Task 1 executed by thread %d\n", omp_get_thread_num()); }
void task2() { printf("Task 2 executed by thread %d\n", omp_get_thread_num()); }
void task3() { printf("Task 3 executed by thread %d\n", omp_get_thread_num()); }

int main() {
    #pragma omp parallel sections
    {
        #pragma omp section
        task1();

        #pragma omp section
        task2();

        #pragma omp section
        task3();
    }

    return 0;
}
```

#### 同步指令

**barrier同步**
```c
#include <omp.h>
#include <stdio.h>

int main() {
    #pragma omp parallel
    {
        int thread_id = omp_get_thread_num();

        printf("Thread %d before barrier\n", thread_id);

        #pragma omp barrier

        printf("Thread %d after barrier\n", thread_id);
    }

    return 0;
}
```

**critical临界区**
```c
#include <omp.h>
#include <stdio.h>

int main() {
    int shared_var = 0;

    #pragma omp parallel for
    for (int i = 0; i < 100; i++) {
        #pragma omp critical
        {
            shared_var++;
            printf("Thread %d: shared_var = %d\n", omp_get_thread_num(), shared_var);
        }
    }

    return 0;
}
```

**atomic原子操作**
```c
#include <omp.h>
#include <stdio.h>

int main() {
    int counter = 0;

    #pragma omp parallel for
    for (int i = 0; i < 1000000; i++) {
        #pragma omp atomic
        counter++;
    }

    printf("Final counter value: %d\n", counter);
    return 0;
}
```

#### 数据共享属性

**private和firstprivate**
```c
#include <omp.h>
#include <stdio.h>

int main() {
    int shared_var = 10;

    #pragma omp parallel private(shared_var)
    {
        shared_var = omp_get_thread_num();
        printf("Thread %d: private shared_var = %d\n", omp_get_thread_num(), shared_var);
    }

    printf("Original shared_var = %d\n", shared_var);

    return 0;
}
```

**shared和default**
```c
#include <omp.h>
#include <stdio.h>

int main() {
    int a = 1, b = 2, c = 3, d = 4;

    #pragma omp parallel shared(a, b) private(c) firstprivate(d)
    {
        // a, b: shared
        // c: private
        // d: firstprivate (initialized with original value)
        printf("Thread %d: a=%d, b=%d, c=%d, d=%d\n",
               omp_get_thread_num(), a, b, c, d);
    }

    return 0;
}
```

### OpenMP高级特性

#### 线程亲和性

**线程绑定控制**
```bash
# 1. 环境变量设置
export OMP_NUM_THREADS=16
export OMP_PROC_BIND=TRUE
export OMP_PLACES=cores

# 2. 绑定策略
export OMP_PROC_BIND=spread    # 分散绑定
export OMP_PROC_BIND=close     # 集中绑定
export OMP_PROC_BIND=master    # 主线程绑定

# 3. 位置指定
export OMP_PLACES=cores        # 按核心绑定
export OMP_PLACES=threads      # 按线程绑定
export OMP_PLACES=sockets      # 按插槽绑定
```

**NUMA感知绑定**
```c
#include <omp.h>
#include <stdio.h>
#include <numa.h>

int main() {
    #pragma omp parallel
    {
        int thread_id = omp_get_thread_num();
        int numa_node = numa_node_of_cpu(sched_getcpu());

        printf("Thread %d running on NUMA node %d\n", thread_id, numa_node);
    }

    return 0;
}
```

#### OpenMP任务并行

**task指令**
```c
#include <omp.h>
#include <stdio.h>

void recursive_task(int n) {
    if (n <= 1) return;

    #pragma omp task
    recursive_task(n/2);

    #pragma omp task
    recursive_task(n/2);

    #pragma omp taskwait
    printf("Task %d completed\n", n);
}

int main() {
    #pragma omp parallel
    {
        #pragma omp single
        recursive_task(16);
    }

    return 0;
}
```

**taskloop指令**
```c
#include <omp.h>
#include <stdio.h>

int fibonacci(int n) {
    if (n <= 1) return n;
    return fibonacci(n-1) + fibonacci(n-2);
}

int main() {
    int n = 20;
    int result;

    #pragma omp parallel
    {
        #pragma omp single
        {
            #pragma omp taskloop reduction(+:result)
            for (int i = 0; i < n; i++) {
                result += fibonacci(i);
            }
        }
    }

    printf("Fibonacci sum: %d\n", result);
    return 0;
}
```

## 10.3 CUDA GPU编程

### CUDA基础概念

#### CUDA架构概述

**CUDA内存层次结构**
```
┌─────────────────────────────────────┐
│           Host Memory (CPU)         │
│  (系统主内存，通过PCIe访问)         │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│           PCI Express Bus           │
│  (CPU和GPU之间的通信通道)           │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│          Global Memory              │
│  (GPU全局内存，容量大，延迟高)       │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│           L2 Cache                  │
│  (GPU二级缓存，统一缓存)            │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│           L1 Cache                  │
│  (每个SM的L1缓存)                   │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│         Shared Memory               │
│  (每个SM的共享内存，容量小，速度快)  │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│        Register File                │
│  (每个线程的寄存器，最快访问)        │
└─────────────────────────────────────┘
```

### CUDA编程模型

#### CUDA线程层次

**线程组织结构**
```cuda
// CUDA线程层次
// Grid -> Block -> Thread
// 三级层次结构

__global__ void kernel_function() {
    // blockIdx: 块索引 (grid level)
    // threadIdx: 线程索引 (block level)
    // blockDim: 块维度
    // gridDim: 网格维度

    int global_id = blockIdx.x * blockDim.x + threadIdx.x;
    int block_id = blockIdx.x;
    int thread_id = threadIdx.x;
}
```

**线程配置示例**
```cuda
// 1. 一维配置
dim3 blockSize(256);
dim3 gridSize((N + blockSize.x - 1) / blockSize.x);
kernel_function<<<gridSize, blockSize>>>();

// 2. 二维配置
dim3 blockSize(16, 16);
dim3 gridSize((width + 15) / 16, (height + 15) / 16);
kernel_function<<<gridSize, blockSize>>>();

// 3. 三维配置
dim3 blockSize(8, 8, 8);
dim3 gridSize((N + 7) / 8, (M + 7) / 8, (P + 7) / 8);
kernel_function<<<gridSize, blockSize>>>();
```

#### CUDA内存管理

**内存分配函数**
```cuda
// 1. 主机内存分配
float *h_data;
h_data = (float*)malloc(size * sizeof(float));

// 2. 设备内存分配
float *d_data;
cudaMalloc(&d_data, size * sizeof(float));

// 3. 统一内存分配
float *u_data;
cudaMallocManaged(&u_data, size * sizeof(float));

// 4. 零拷贝内存
float *pinned_data;
cudaHostAlloc(&pinned_data, size * sizeof(float), cudaHostAllocMapped);
```

**内存拷贝函数**
```cuda
// 1. 主机到设备
cudaMemcpy(d_data, h_data, size * sizeof(float), cudaMemcpyHostToDevice);

// 2. 设备到主机
cudaMemcpy(h_data, d_data, size * sizeof(float), cudaMemcpyDeviceToHost);

// 3. 设备到设备
cudaMemcpy(d_dest, d_src, size * sizeof(float), cudaMemcpyDeviceToDevice);

// 4. 异步拷贝
cudaMemcpyAsync(d_data, h_data, size * sizeof(float),
                cudaMemcpyHostToDevice, stream);
```

### CUDA内核编程

#### 基本内核示例

**向量加法内核**
```cuda
#include <cuda_runtime.h>
#include <stdio.h>

// CUDA内核函数
__global__ void vector_add(float *a, float *b, float *c, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        c[idx] = a[idx] + b[idx];
    }
}

int main() {
    const int N = 1024 * 1024;
    const int size = N * sizeof(float);

    // 分配主机内存
    float *h_a = (float*)malloc(size);
    float *h_b = (float*)malloc(size);
    float *h_c = (float*)malloc(size);

    // 初始化数据
    for (int i = 0; i < N; i++) {
        h_a[i] = i;
        h_b[i] = i * 2;
    }

    // 分配设备内存
    float *d_a, *d_b, *d_c;
    cudaMalloc(&d_a, size);
    cudaMalloc(&d_b, size);
    cudaMalloc(&d_c, size);

    // 拷贝数据到设备
    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);

    // 配置执行参数
    int blockSize = 256;
    int gridSize = (N + blockSize - 1) / blockSize;

    // 启动内核
    vector_add<<<gridSize, blockSize>>>(d_a, d_b, d_c, N);

    // 等待内核完成
    cudaDeviceSynchronize();

    // 拷贝结果回主机
    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);

    // 验证结果
    for (int i = 0; i < 10; i++) {
        printf("h_c[%d] = %f\n", i, h_c[i]);
    }

    // 释放内存
    free(h_a); free(h_b); free(h_c);
    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);

    return 0;
}
```

#### 内存访问优化

**共享内存使用**
```cuda
__global__ void matrix_multiply_shared(float *A, float *B, float *C,
                                      int N, int M, int K) {
    // 共享内存声明
    __shared__ float shared_A[16][16];
    __shared__ float shared_B[16][16];

    int tx = threadIdx.x;
    int ty = threadIdx.y;
    int row = blockIdx.y * 16 + ty;
    int col = blockIdx.x * 16 + tx;

    float sum = 0.0f;

    // 分块计算
    for (int k = 0; k < (M + 15) / 16; k++) {
        // 加载数据到共享内存
        if (row < N && k * 16 + tx < M) {
            shared_A[ty][tx] = A[row * M + k * 16 + tx];
        } else {
            shared_A[ty][tx] = 0.0f;
        }

        if (k * 16 + ty < M && col < K) {
            shared_B[ty][tx] = B[(k * 16 + ty) * K + col];
        } else {
            shared_B[ty][tx] = 0.0f;
        }

        __syncthreads();

        // 计算部分和
        for (int i = 0; i < 16; i++) {
            sum += shared_A[ty][i] * shared_B[i][tx];
        }

        __syncthreads();
    }

    // 存储结果
    if (row < N && col < K) {
        C[row * K + col] = sum;
    }
}
```

**纹理内存使用**
```cuda
// 纹理内存声明
texture<float, cudaTextureType1D, cudaReadModeElementType> tex_A;
texture<float, cudaTextureType1D, cudaReadModeElementType> tex_B;

__global__ void interpolation_kernel(float *result, int N) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < N) {
        float x = idx / (float)(N - 1);
        float a_val = tex1D(tex_A, x);
        float b_val = tex1D(tex_B, x);
        result[idx] = a_val * 0.5f + b_val * 0.5f;
    }
}

int main() {
    // 绑定纹理
    cudaBindTexture(NULL, tex_A, d_A, N * sizeof(float));
    cudaBindTexture(NULL, tex_B, d_B, N * sizeof(float));

    // 执行内核
    dim3 blockSize(256);
    dim3 gridSize((N + blockSize.x - 1) / blockSize.x);
    interpolation_kernel<<<gridSize, blockSize>>>(d_result, N);

    // 解绑纹理
    cudaUnbindTexture(tex_A);
    cudaUnbindTexture(tex_B);

    return 0;
}
```

## 10.4 OpenACC加速计算

### OpenACC基础概念

#### OpenACC执行模型

**加速器模型**
```
┌─────────────────────────────────────┐
│           Host (CPU)                │
│  ┌─────────────────────────────────┐ │
│  │        Host Code                │ │
│  │  (串行或OpenMP并行)             │ │
│  └─────────────────────────────────┘ │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│           Data Transfer             │
│  (数据在主机和设备间传输)           │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│         Accelerator (GPU)           │
│  ┌─────────────────────────────────┐ │
│  │      Device Code                │ │
│  │  (OpenACC并行区域)              │ │
│  └─────────────────────────────────┘ │
└─────────────────────────────────────┘
```

### OpenACC指令详解

#### 计算指令

**parallel指令**
```c
#include <openacc.h>
#include <stdio.h>

int main() {
    int n = 1000;
    float *a = (float*)malloc(n * sizeof(float));
    float *b = (float*)malloc(n * sizeof(float));
    float *c = (float*)malloc(n * sizeof(float));

    // 初始化数据
    for (int i = 0; i < n; i++) {
        a[i] = i;
        b[i] = i * 2;
    }

    // 并行计算区域
    #pragma acc parallel loop
    for (int i = 0; i < n; i++) {
        c[i] = a[i] + b[i];
    }

    // 输出结果
    for (int i = 0; i < 10; i++) {
        printf("c[%d] = %f\n", i, c[i]);
    }

    free(a); free(b); free(c);
    return 0;
}
```

**kernels指令**
```c
#include <openacc.h>
#include <stdio.h>

int main() {
    int n = 1000;
    float *a = (float*)malloc(n * sizeof(float));
    float *b = (float*)malloc(n * sizeof(float));
    float *c = (float*)malloc(n * sizeof(float));

    // 初始化数据
    for (int i = 0; i < n; i++) {
        a[i] = i;
        b[i] = i * 2;
    }

    // 内核区域，编译器自动分析并行性
    #pragma acc kernels
    {
        for (int i = 0; i < n; i++) {
            a[i] = a[i] * 2;
        }

        for (int i = 0; i < n; i++) {
            b[i] = b[i] + 1;
        }

        for (int i = 0; i < n; i++) {
            c[i] = a[i] + b[i];
        }
    }

    free(a); free(b); free(c);
    return 0;
}
```

#### 数据管理指令

**数据传输指令**
```c
#include <openacc.h>
#include <stdio.h>

int main() {
    int n = 1000;
    float *a = (float*)malloc(n * sizeof(float));
    float *b = (float*)malloc(n * sizeof(float));
    float *c = (float*)malloc(n * sizeof(float));

    // 初始化数据
    for (int i = 0; i < n; i++) {
        a[i] = i;
        b[i] = i * 2;
    }

    // 数据传输和计算
    #pragma acc data copyin(a[0:n], b[0:n]) copyout(c[0:n])
    {
        #pragma acc parallel loop
        for (int i = 0; i < n; i++) {
            c[i] = a[i] + b[i];
        }
    }

    free(a); free(b); free(c);
    return 0;
}
```

**数据区域管理**
```c
#include <openacc.h>
#include <stdio.h>

int main() {
    int n = 1000;
    float *a = (float*)malloc(n * sizeof(float));
    float *b = (float*)malloc(n * sizeof(float));
    float *c = (float*)malloc(n * sizeof(float));

    // 创建数据区域
    #pragma acc enter data create(a[0:n], b[0:n], c[0:n])

    // 初始化数据
    for (int i = 0; i < n; i++) {
        a[i] = i;
        b[i] = i * 2;
    }
    #pragma acc update device(a[0:n], b[0:n])

    // 并行计算
    #pragma acc parallel loop present(a, b, c)
    for (int i = 0; i < n; i++) {
        c[i] = a[i] + b[i];
    }

    // 获取结果
    #pragma acc update host(c[0:n])

    // 销毁数据区域
    #pragma acc exit data delete(a, b, c)

    free(a); free(b); free(c);
    return 0;
}
```

#### 循环指令

**loop指令优化**
```c
#include <openacc.h>
#include <stdio.h>

int main() {
    int n = 1000;
    float *a = (float*)malloc(n * sizeof(float));
    float *b = (float*)malloc(n * sizeof(float));
    float *c = (float*)malloc(n * sizeof(float));

    // 初始化数据
    for (int i = 0; i < n; i++) {
        a[i] = i;
        b[i] = i * 2;
    }

    // 循环优化
    #pragma acc parallel loop gang vector
    for (int i = 0; i < n; i++) {
        c[i] = a[i] + b[i];
    }

    // 嵌套循环优化
    #pragma acc parallel loop gang
    for (int i = 0; i < 100; i++) {
        #pragma acc loop vector
        for (int j = 0; j < 100; j++) {
            a[i*100 + j] = i + j;
        }
    }

    free(a); free(b); free(c);
    return 0;
}
```

### OpenACC性能优化

#### 编译器优化选项

**PGI编译器选项**
```bash
# 1. 基本编译
pgcc -acc -o program program.c

# 2. 性能分析
pgcc -acc -Minfo=accel -o program program.c

# 3. 优化级别
pgcc -acc -O3 -o program program.c

# 4. GPU信息
pgcc -acc -gpu=info -o program program.c
```

**GCC编译器选项**
```bash
# 1. 基本编译
gcc -fopenacc -o program program.c

# 2. 性能信息
gcc -fopenacc -fopt-info-omp -o program program.c

# 3. 优化级别
gcc -fopenacc -O3 -o program program.c
```

#### 性能分析工具

**NVIDIA Nsight**
```bash
# 1. 基本分析
nsys profile ./program

# 2. OpenACC分析
nsys profile --trace=openacc ./program

# 3. 性能报告
nsys stats report1.nsys-rep
```

**PGI性能分析**
```bash
# 1. 编译时信息
pgcc -acc -Minfo=accel program.c

# 2. 运行时分析
pgprof ./program
```

## 本章小结

并行编程环境是HPC应用开发的核心。本章详细介绍了：

1. **MPI编程环境**：OpenMPI、Intel MPI、MVAPICH2的安装配置，MPI编程基础和高级特性
2. **OpenMP多线程**：编译器支持、指令详解、线程管理和任务并行
3. **CUDA GPU编程**：CUDA架构、内存管理、内核编程和性能优化
4. **OpenACC加速计算**：执行模型、指令详解和性能优化

掌握这些并行编程技术有助于：
- 开发高效的并行应用程序
- 充分利用多核CPU和GPU资源
- 优化计算密集型应用性能
- 构建可扩展的并行计算系统

在实际应用中，需要根据具体的硬件平台、应用特性和性能需求，选择合适的并行编程模型和技术。