# 第8章 存储系统管理

## 8.1 并行文件系统

### Lustre文件系统

#### Lustre架构概述

**Lustre组件结构**
```
┌─────────────────────────────────────────────────────────────┐
│                    Lustre文件系统                           │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐        │
│  │  Client │  │  Client │  │  Client │  │  Client │        │
│  │  (OSC)  │  │  (OSC)  │  │  (OSC)  │  │  (OSC)  │        │
│  └─────────┘  └─────────┘  └─────────┘  └─────────┘        │
│         │            │            │            │           │
│         └────────────┼────────────┼────────────┘           │
│                      │            │                        │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │                    Metadata Layer                       │ │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐  │ │
│  │  │   MDT   │  │   MDT   │  │   MDT   │  │   MGS   │  │ │
│  │  │  (MDT)  │  │  (MDT)  │  │  (MDT)  │  │  (MGS)  │  │ │
│  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘  │ │
│  │         │            │            │            │      │ │
│  │         └────────────┼────────────┼────────────┘      │ │
│  │                      │            │                   │ │
│  └─────────────────────────────────────────────────────────┘ │
│                      │            │                        │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │                     Object Layer                        │ │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐  │ │
│  │  │   OST   │  │   OST   │  │   OST   │  │   OST   │  │ │
│  │  │  (OST)  │  │  (OST)  │  │  (OST)  │  │  (OST)  │  │ │
│  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘  │ │
│  │         │            │            │            │      │ │
│  │         └────────────┼────────────┼────────────┘      │ │
│  │                      │            │                   │ │
│  └─────────────────────────────────────────────────────────┘ │
│                      │            │                        │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │                    Storage Layer                        │ │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐  │ │
│  │  │  Disk   │  │  Disk   │  │  Disk   │  │  Disk   │  │ │
│  │  │  (HDD)  │  │  (HDD)  │  │  (SSD)  │  │  (SSD)  │  │ │
│  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘  │ │
│  └─────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────┘
```

**Lustre核心组件**
- **MGS (Management Server)**：管理系统配置和元数据
- **MDT (Metadata Target)**：存储文件系统元数据
- **OST (Object Storage Target)**：存储文件数据对象
- **Client**：客户端访问文件系统

#### Lustre安装与配置

**环境准备**
```bash
# 1. 系统要求
# 所有节点需要相同的操作系统版本
# 内核版本要求 (通常需要Lustre内核模块)

# 2. 网络配置
# 确保所有节点间网络连通性
# 推荐使用InfiniBand或高速以太网

# 3. 存储设备准备
# 为MDT和OST准备专用存储设备
fdisk -l /dev/sdb /dev/sdc /dev/sdd
```

**Lustre软件安装**
```bash
# 1. 下载Lustre源码
wget https://downloads.whamcloud.com/public/lustre/lustre-2.15.0/el9/client/SRPMS/lustre-2.15.0-1.el9.src.rpm
rpm -ivh lustre-2.15.0-1.el9.src.rpm

# 2. 编译安装
cd /root/rpmbuild/SPECS
rpmbuild -bb --with servers lustre.spec
rpmbuild -bb --without servers lustre.spec

# 3. 安装服务端
rpm -ivh /root/rpmbuild/RPMS/x86_64/lustre-*.rpm
rpm -ivh /root/rpmbuild/RPMS/x86_64/lustre-osd-ldiskfs-*.rpm

# 4. 安装客户端
rpm -ivh /root/rpmbuild/RPMS/x86_64/lustre-client-*.rpm
```

**MGS配置**
```bash
# 1. 格式化MGS设备
mkfs.lustre --fsname=hpc-lustre --mgs /dev/sdb1

# 2. 挂载MGS
mkdir -p /mnt/mgs
mount -t lustre /dev/sdb1 /mnt/mgs

# 3. 启动MGS服务
lctl format /dev/sdb1
lctl start mgs
```

**MDT配置**
```bash
# 1. 格式化MDT设备
mkfs.lustre --fsname=hpc-lustre --mgsnode=mgs-server@tcp --mdt /dev/sdc1

# 2. 挂载MDT
mkdir -p /mnt/mdt
mount -t lustre /dev/sdc1 /mnt/mdt

# 3. 启动MDT服务
lctl start mdt
```

**OST配置**
```bash
# 1. 格式化OST设备
mkfs.lustre --fsname=hpc-lustre --mgsnode=mgs-server@tcp --ost /dev/sdd1

# 2. 挂载OST
mkdir -p /mnt/ost
mount -t lustre /dev/sdd1 /mnt/ost

# 3. 启动OST服务
lctl start ost
```

**客户端配置**
```bash
# 1. 挂载Lustre文件系统
mkdir -p /mnt/lustre
mount -t lustre mgs-server@tcp:/hpc-lustre /mnt/lustre

# 2. 验证挂载
lfs df -h /mnt/lustre
lfs getstripe /mnt/lustre

# 3. 添加到fstab
echo "mgs-server@tcp:/hpc-lustre /mnt/lustre lustre defaults 0 0" >> /etc/fstab
```

#### Lustre性能优化

**条带化配置**
```bash
# 1. 查看当前条带化设置
lfs getstripe /mnt/lustre/file

# 2. 设置条带化参数
# 条带数: 数据分布的OST数量
# 条带大小: 每个OST上的数据块大小
lfs setstripe -c 4 -S 1M /mnt/lustre/directory

# 3. 不同文件类型的条带化策略
# 大文件: 更多条带，更大条带大小
lfs setstripe -c -1 -S 4M /mnt/lustre/big_files/

# 小文件: 较少条带，较小条带大小
lfs setstripe -c 1 -S 64k /mnt/lustre/small_files/

# 元数据密集型: 优化MDT访问
lfs setstripe -i 0 -c 1 /mnt/lustre/metadata/
```

**缓存优化**
```bash
# 1. 客户端缓存配置
echo 1 > /proc/fs/lustre/osc/*/max_dirty_mb
echo 50 > /proc/fs/lustre/osc/*/max_rpcs_in_flight

# 2. 服务器端缓存配置
echo 1000 > /proc/fs/lustre/obdfilter/*/read_cache_enable
echo 1000 > /proc/fs/lustre/obdfilter/*/write_cache_enable

# 3. 网络缓存优化
echo 134217728 > /proc/sys/net/core/rmem_max
echo 134217728 > /proc/sys/net/core/wmem_max
```

**网络优化**
```bash
# 1. InfiniBand优化
echo 65520 > /sys/class/infiniband/ib0/mtu
echo 1024 > /sys/class/infiniband/ib0/ports/1/gid_attrs/roce/queue_pairs

# 2. TCP优化
echo 'net.core.rmem_max = 134217728' >> /etc/sysctl.conf
echo 'net.core.wmem_max = 134217728' >> /etc/sysctl.conf
sysctl -p
```

### GPFS文件系统

#### GPFS架构概述

**GPFS组件结构**
```
┌─────────────────────────────────────────────────────────────┐
│                      GPFS文件系统                           │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐        │
│  │  Client │  │  Client │  │  Client │  │  Client │        │
│  │  (NSD)  │  │  (NSD)  │  │  (NSD)  │  │  (NSD)  │        │
│  └─────────┘  └─────────┘  └─────────┘  └─────────┘        │
│         │            │            │            │           │
│         └────────────┼────────────┼────────────┘           │
│                      │            │                        │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │                    Management                           │ │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐  │ │
│  │  │  Admin  │  │  Admin  │  │  Admin  │  │  Admin  │  │ │
│  │  │  Node   │  │  Node   │  │  Node   │  │  Node   │  │ │
│  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘  │ │
│  │         │            │            │            │      │ │
│  │         └────────────┼────────────┼────────────┘      │ │
│  │                      │            │                   │ │
│  └─────────────────────────────────────────────────────────┘ │
│                      │            │                        │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │                     Storage Layer                       │ │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐  │ │
│  │  │  Disk   │  │  Disk   │  │  Disk   │  │  Disk   │  │ │
│  │  │  (HDD)  │  │  (HDD)  │  │  (SSD)  │  │  (SSD)  │  │ │
│  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘  │ │
│  └─────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────┘
```

**GPFS核心组件**
- **NSD (Network Shared Disk)**：网络共享磁盘服务
- **Admin Node**：管理节点，负责文件系统管理
- **Client Node**：客户端节点，访问文件系统
- **Shared Storage**：共享存储设备

#### GPFS安装与配置

**环境准备**
```bash
# 1. 系统要求
# 所有节点需要相同的GPFS版本
# 共享存储设备配置

# 2. 网络配置
# 配置高速网络连接
# 确保节点间SSH免密登录

# 3. 存储设备准备
# 配置共享存储 (SAN, NAS, etc.)
# 确保所有节点都能访问存储设备
```

**GPFS软件安装**
```bash
# 1. 下载GPFS软件
wget https://public.dhe.ibm.com/software/server/cluster/filesystem/gpfs/gpfs_install-5.1.3-0.x86_64.rpm
rpm -ivh gpfs_install-5.1.3-0.x86_64.rpm

# 2. 安装GPFS
/opt/ibm/gpfs_install/install_gpfs -a

# 3. 验证安装
mmgetstate -a
```

**GPFS集群配置**
```bash
# 1. 创建集群
mmcrcluster -N node1,node2,node3 -p node1 -s node2

# 2. 启动集群
mmstartup -a

# 3. 验证集群状态
mmgetstate -a
```

**存储设备配置**
```bash
# 1. 创建NSD配置文件
cat > nsd.config << EOF
%nsd
device /dev/sdb
servers node1,node2
usage dataAndMetadata
failureGroup 1
pool system
EOF

# 2. 创建NSD
mmcrnsd -F nsd.config

# 3. 验证NSD状态
mmlsnsd
```

**文件系统创建**
```bash
# 1. 创建文件系统
mmcrfs hpc-fs -F nsd.config -B 1m -r 2 -i 32768

# 2. 挂载文件系统
mcmount hpc-fs

# 3. 验证文件系统
mmlsfs hpc-fs
```

#### GPFS性能优化

**条带化配置**
```bash
# 1. 查看当前条带化设置
mmlsfs hpc-fs -B

# 2. 修改条带化大小
mmchfs hpc-fs -B 2m

# 3. 设置副本数
mmchfs hpc-fs -r 3

# 4. 设置inode大小
mmchfs hpc-fs -i 65536
```

**缓存优化**
```bash
# 1. 客户端缓存配置
echo 1000 > /proc/fs/gpfs/hpc-fs/maxNumThreads
echo 2048 > /proc/fs/gpfs/hpc-fs/maxCacheSize

# 2. 服务器端缓存配置
mmchconfig maxMB=1024
mmchconfig pagePoolMB=512
```

**网络优化**
```bash
# 1. GPFS网络配置
mmchconfig tcpWindowSize=1048576
mmchconfig tcpSendBufSize=1048576
mmchconfig tcpRecvBufSize=1048576

# 2. 系统网络优化
echo 134217728 > /proc/sys/net/core/rmem_max
echo 134217728 > /proc/sys/net/core/wmem_max
```

### BeeGFS文件系统

#### BeeGFS架构概述

**BeeGFS组件结构**
```
┌─────────────────────────────────────────────────────────────┐
│                     BeeGFS文件系统                          │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐        │
│  │  Client │  │  Client │  │  Client │  │  Client │        │
│  │  (Client)│ │  (Client)│ │  (Client)│ │  (Client)│        │
│  └─────────┘  └─────────┘  └─────────┘  └─────────┘        │
│         │            │            │            │           │
│         └────────────┼────────────┼────────────┘           │
│                      │            │                        │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │                     Metadata Layer                      │ │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐  │ │
│  │  │Metadata │  │Metadata │  │Metadata │  │Metadata │  │ │
│  │  │ Server  │  │ Server  │  │ Server  │  │ Server  │  │ │
│  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘  │ │
│  │         │            │            │            │      │ │
│  │         └────────────┼────────────┼────────────┘      │ │
│  │                      │            │                   │ │
│  └─────────────────────────────────────────────────────────┘ │
│                      │            │                        │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │                      Storage Layer                      │ │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐  │ │
│  │  │ Storage │  │ Storage │  │ Storage │  │ Storage │  │ │
│  │  │ Server  │  │ Server  │  │ Server  │  │ Server  │  │ │
│  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘  │ │
│  │         │            │            │            │      │ │
│  │         └────────────┼────────────┼────────────┘      │ │
│  │                      │            │                   │ │
│  └─────────────────────────────────────────────────────────┘ │
│                      │            │                        │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │                    Storage Layer                        │ │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐  │ │
│  │  │  Disk   │  │  Disk   │  │  Disk   │  │  Disk   │  │ │
│  │  │  (HDD)  │  │  (HDD)  │  │  (SSD)  │  │  (SSD)  │  │ │
│  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘  │ │
│  └─────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────┘
```

**BeeGFS核心组件**
- **Client**: 客户端组件，提供文件系统接口
- **Metadata Server**: 元数据服务器，管理文件系统元数据
- **Storage Server**: 存储服务器，管理实际数据存储
- **Management Daemon**: 管理守护进程，负责集群管理

#### BeeGFS安装与配置

**环境准备**
```bash
# 1. 系统要求
# 所有节点需要相同的BeeGFS版本
# 高速网络连接 (推荐InfiniBand)

# 2. 网络配置
# 配置高速网络
# 确保节点间SSH免密登录
```

**BeeGFS软件安装**
```bash
# 1. 添加BeeGFS仓库
wget -O- http://www.beegfs.com/release/beegfs_7.2.1/gpg/DEB-GPG-KEY-beegfs | apt-key add -
echo "deb http://www.beegfs.com/release/beegfs_7.2.1/dists/beegfs-deb10 ./" > /etc/apt/sources.list.d/beegfs.list

# 2. 安装BeeGFS
apt-get update
apt-get install beegfs-client beegfs-server beegfs-utils

# 3. 验证安装
systemctl status beegfs-client
systemctl status beegfs-server
```

**BeeGFS配置**
```bash
# 1. 配置Management Daemon
cat > /etc/beegfs/beegfs-mgmtd.conf << EOF
# Management Daemon Configuration
sysMgmtdHost = mgmt-server
connAuthFile = /etc/beegfs/beegfs-auth.conf
EOF

# 2. 配置Metadata Server
cat > /etc/beegfs/beegfs-meta.conf << EOF
# Metadata Server Configuration
sysMgmtdHost = mgmt-server
storeMetaDirectory = /data/beegfs/meta
EOF

# 3. 配置Storage Server
cat > /etc/beegfs/beegfs-storage.conf << EOF
# Storage Server Configuration
sysMgmtdHost = mgmt-server
storeStorageDirectory = /data/beegfs/storage
EOF

# 4. 配置Client
cat > /etc/beegfs/beegfs-client.conf << EOF
# Client Configuration
sysMgmtdHost = mgmt-server
connClientPortUDP = 8004
connClientPortTCP = 8004
EOF
```

**BeeGFS服务启动**
```bash
# 1. 启动Management Daemon
systemctl enable beegfs-mgmtd
systemctl start beegfs-mgmtd

# 2. 启动Metadata Server
systemctl enable beegfs-meta
systemctl start beegfs-meta

# 3. 启动Storage Server
systemctl enable beegfs-storage
systemctl start beegfs-storage

# 4. 启动Client
systemctl enable beegfs-client
systemctl start beegfs-client
```

**文件系统挂载**
```bash
# 1. 创建挂载点
mkdir -p /mnt/beegfs

# 2. 挂载文件系统
mount -t beegfs -o beegfsClient=beegfs-client.conf mgmt-server:/ /mnt/beegfs

# 3. 验证挂载
df -h /mnt/beegfs
beegfs-ctl --listnodes
```

#### BeeGFS性能优化

**条带化配置**
```bash
# 1. 设置条带化策略
beegfs-ctl --setpattern --chunksize=1M --numtargets=4 /mnt/beegfs/data

# 2. 查看条带化信息
beegfs-ctl --getpattern /mnt/beegfs/data

# 3. 不同文件类型的条带化策略
# 大文件: 更多目标，更大块大小
beegfs-ctl --setpattern --chunksize=4M --numtargets=8 /mnt/beegfs/big_files/

# 小文件: 较少目标，较小块大小
beegfs-ctl --setpattern --chunksize=64k --numtargets=2 /mnt/beegfs/small_files/
```

**缓存优化**
```bash
# 1. 客户端缓存配置
echo 2048 > /proc/fs/beegfs/client/maxNumThreads
echo 4096 > /proc/fs/beegfs/client/maxCacheSize

# 2. 服务器端缓存配置
echo 1024 > /proc/fs/beegfs/storage/maxCacheSize
echo 512 > /proc/fs/beegfs/meta/maxCacheSize
```

**网络优化**
```bash
# 1. BeeGFS网络配置
echo 1048576 > /proc/fs/beegfs/client/tcpWindowSize
echo 1048576 > /proc/fs/beegfs/client/tcpSendBufSize
echo 1048576 > /proc/fs/beegfs/client/tcpRecvBufSize

# 2. 系统网络优化
echo 134217728 > /proc/sys/net/core/rmem_max
echo 134217728 > /proc/sys/net/core/wmem_max
```

## 8.2 存储性能优化

### 存储性能指标

#### 性能参数详解

**IOPS (Input/Output Operations Per Second)**
```bash
# IOPS计算公式
IOPS = 1000ms / (平均寻道时间 + 平均延迟时间 + 传输时间)

# 不同存储设备的IOPS
HDD (7200 RPM):     75-100 IOPS
HDD (15000 RPM):    175-200 IOPS
SATA SSD:          10,000-100,000 IOPS
NVMe SSD:         100,000-1,000,000+ IOPS
```

**吞吐量 (Throughput)**
```bash
# 吞吐量计算公式
吞吐量 = IOPS × 平均I/O大小

# 不同存储设备的吞吐量
HDD:                100-250 MB/s
SATA SSD:           500-5,500 MB/s
NVMe SSD:          3,000-14,000 MB/s
```

**延迟 (Latency)**
```bash
# 延迟组成
总延迟 = 控制器延迟 + 队列延迟 + 传输延迟 + 寻道延迟 + 旋转延迟

# 不同存储设备的延迟
HDD:                5-10 ms
SATA SSD:           0.1 ms
NVMe SSD:           0.02-0.05 ms
```

### 存储性能测试

#### FIO基准测试

**FIO测试配置**
```bash
# 1. 顺序读取测试
fio --name=seq_read --ioengine=libaio --rw=read --bs=1m --size=1G --numjobs=4 --runtime=60 --time_based --filename=/dev/sdb --direct=1 --iodepth=64

# 2. 顺序写入测试
fio --name=seq_write --ioengine=libaio --rw=write --bs=1m --size=1G --numjobs=4 --runtime=60 --time_based --filename=/dev/sdb --direct=1 --iodepth=64

# 3. 随机读取测试
fio --name=rand_read --ioengine=libaio --rw=randread --bs=4k --size=1G --numjobs=16 --runtime=60 --time_based --filename=/dev/sdb --direct=1 --iodepth=256

# 4. 随机写入测试
fio --name=rand_write --ioengine=libaio --rw=randwrite --bs=4k --size=1G --numjobs=16 --runtime=60 --time_based --filename=/dev/sdb --direct=1 --iodepth=256

# 5. 混合读写测试
fio --name=mixed --ioengine=libaio --rw=randrw --rwmixread=70 --bs=4k --size=1G --numjobs=8 --runtime=60 --time_based --filename=/dev/sdb --direct=1 --iodepth=128
```

**FIO结果分析**
```bash
# 结果解读
# read: IOPS=12345, BW=48.2MB/s (50.5MB/s)(2964MB/61506msec)
# write: IOPS=6789, BW=26.5MB/s (27.8MB/s)(1632MB/61506msec)
# lat (usec): min=42, max=15680, avg=123.45, stdev=45.67

# 性能评估
# IOPS: 每秒I/O操作数，反映随机访问性能
# BW: 带宽，反映顺序访问性能
# lat: 延迟，反映响应速度
# stdev: 标准差，反映延迟稳定性
```

#### 并行文件系统测试

**Lustre性能测试**
```bash
# 1. IOR测试
ior -a POSIX -b 1g -t 1m -o /mnt/lustre/testfile -w -r

# 2. MDTEST测试
mdtest -d /mnt/lustre/testdir -i 10 -I 1000 -n 100 -u

# 3. 自定义测试脚本
#!/bin/bash
# lustre-performance-test.sh

TEST_DIR="/mnt/lustre/perf_test"
FILE_SIZE="1G"
BLOCK_SIZE="1M"

# 创建测试目录
mkdir -p $TEST_DIR

# 顺序写入测试
echo "Sequential Write Test:"
time dd if=/dev/zero of=$TEST_DIR/seq_write_test bs=$BLOCK_SIZE count=$((1024*1024/$BLOCK_SIZE)) oflag=direct

# 顺序读取测试
echo "Sequential Read Test:"
time dd if=$TEST_DIR/seq_write_test of=/dev/null bs=$BLOCK_SIZE iflag=direct

# 随机I/O测试
echo "Random I/O Test:"
fio --name=rand_test --ioengine=libaio --rw=randrw --bs=4k --size=$FILE_SIZE --numjobs=16 --runtime=60 --time_based --filename=$TEST_DIR/rand_test --direct=1

# 清理测试文件
rm -f $TEST_DIR/*
```

**GPFS性能测试**
```bash
# 1. GPFS特定工具
mmfsdump -f /tmp/gpfs_dump
mmfsstat -f /tmp/gpfs_stat

# 2. 标准工具测试
# 使用IOR、MDTEST等工具
ior -a POSIX -b 1g -t 1m -o /mnt/gpfs/testfile -w -r

# 3. GPFS性能监控
mmfsadm dumpstats
mmfsadm dumpconfig
```

### 存储缓存策略

#### 缓存层次结构

**存储缓存架构**
```
┌─────────────────────────────────────┐
│           应用程序缓存               │
│  (应用程序内部缓存机制)              │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│            操作系统缓存              │
│  (Page Cache, Buffer Cache)         │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│            存储控制器缓存            │
│  (RAID卡缓存, HBA缓存)              │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│            存储设备缓存              │
│  (SSD缓存, HDD缓存)                 │
└─────────────────────────────────────┘
```

#### 缓存配置优化

**操作系统缓存优化**
```bash
# 1. Page Cache配置
echo 15 > /proc/sys/vm/dirty_ratio        # 脏页比例上限
echo 5 > /proc/sys/vm/dirty_background_ratio  # 后台写入比例
echo 300 > /proc/sys/vm/dirty_expire_centisecs  # 脏页过期时间
echo 500 > /proc/sys/vm/dirty_writeback_centisecs  # 写回间隔

# 2. 文件系统缓存
# XFS配置
mount -o noatime,logbsize=256k,largeio /dev/sdb1 /data

# ext4配置
mount -o noatime,data=writeback,barrier=0 /dev/sdb1 /data

# 3. 内存管理
echo 1 > /proc/sys/vm/overcommit_memory
echo 80 > /proc/sys/vm/swappiness
```

**RAID控制器缓存优化**
```bash
# 1. 查看RAID控制器信息
megacli -AdpAllInfo -aALL
storcli /c0 show

# 2. 配置写缓存策略
megacli -LDSetProp -WB -Lall -aALL
storcli /c0/e0/s0 set wrcache=WB

# 3. 配置读缓存策略
megacli -LDSetProp -RA -Lall -aALL
storcli /c0/e0/s0 set rdcache=RA
```

**SSD缓存配置**
```bash
# 1. 启用TRIM支持
echo 'ACTION=="add|change", KERNEL=="sd*[!0-9]", ATTR{queue/discard_granularity}=="512", ATTR{queue/discard_max_bytes}=="4294967295", RUN+="/sbin/blockdev --setra 512 /dev/$name"' > /etc/udev/rules.d/80-ssd.rules

# 2. 优化I/O调度器
echo mq-deadline > /sys/block/sdb/queue/scheduler

# 3. 配置I/O队列深度
echo 1024 > /sys/block/sdb/queue/nr_requests
echo 128 > /sys/block/sdb/queue/nr_requests
```

#### 缓存策略选择

**不同工作负载的缓存策略**
```bash
# 1. 读密集型工作负载
# 增大读缓存，启用预读
echo 80 > /proc/sys/vm/dirty_ratio
echo 20 > /proc/sys/vm/dirty_background_ratio
blockdev --setra 1024 /dev/sdb

# 2. 写密集型工作负载
# 增大写缓存，优化写入策略
echo 10 > /proc/sys/vm/dirty_ratio
echo 5 > /proc/sys/vm/dirty_background_ratio
echo 3000 > /proc/sys/vm/dirty_expire_centisecs

# 3. 混合工作负载
# 平衡读写缓存
echo 40 > /proc/sys/vm/dirty_ratio
echo 10 > /proc/sys/vm/dirty_background_ratio
```

## 8.3 数据备份与恢复

### 备份策略设计

#### 备份类型

**完全备份 (Full Backup)**
```bash
# 1. 完全备份脚本
#!/bin/bash
# full_backup.sh

BACKUP_DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backup/full/$BACKUP_DATE"
SOURCE_DIR="/data"
EXCLUDE_FILE="/etc/backup.exclude"

# 创建备份目录
mkdir -p $BACKUP_DIR

# 执行完全备份
tar --exclude-from=$EXCLUDE_FILE -czf "$BACKUP_DIR/full_backup_$BACKUP_DATE.tar.gz" $SOURCE_DIR

# 计算校验和
sha256sum "$BACKUP_DIR/full_backup_$BACKUP_DATE.tar.gz" > "$BACKUP_DIR/checksum.sha256"

# 压缩备份
gzip "$BACKUP_DIR/full_backup_$BACKUP_DATE.tar.gz"

echo "Full backup completed: $BACKUP_DIR"
```

**增量备份 (Incremental Backup)**
```bash
# 1. 增量备份脚本
#!/bin/bash
# incremental_backup.sh

BACKUP_DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backup/incremental/$BACKUP_DATE"
SOURCE_DIR="/data"
LAST_BACKUP="/backup/incremental/last_backup.timestamp"

# 创建备份目录
mkdir -p $BACKUP_DIR

# 执行增量备份
find $SOURCE_DIR -newer $LAST_BACKUP -type f -exec tar -rf "$BACKUP_DIR/incremental_$BACKUP_DATE.tar" {} \;

# 更新时间戳
touch $LAST_BACKUP

echo "Incremental backup completed: $BACKUP_DIR"
```

**差异备份 (Differential Backup)**
```bash
# 1. 差异备份脚本
#!/bin/bash
# differential_backup.sh

BACKUP_DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backup/differential/$BACKUP_DATE"
SOURCE_DIR="/data"
BASE_BACKUP="/backup/full/base_backup.timestamp"

# 创建备份目录
mkdir -p $BACKUP_DIR

# 执行差异备份
find $SOURCE_DIR -newer $BASE_BACKUP -type f -exec tar -rf "$BACKUP_DIR/differential_$BACKUP_DATE.tar" {} \;

echo "Differential backup completed: $BACKUP_DIR"
```

#### 备份调度

**定时备份配置**
```bash
# 1. crontab配置
# 每周日2点执行完全备份
0 2 * * 0 /usr/local/bin/full_backup.sh

# 每天2点执行增量备份
0 2 * * * /usr/local/bin/incremental_backup.sh

# 每周六2点执行差异备份
0 2 * * 6 /usr/local/bin/differential_backup.sh

# 2. systemd定时器
# /etc/systemd/system/backup-full.timer
[Unit]
Description=Full Backup Timer
Requires=backup-full.service

[Timer]
OnCalendar=weekly
Persistent=true

[Install]
WantedBy=timers.target

# /etc/systemd/system/backup-full.service
[Unit]
Description=Full Backup Service
Requires=backup-full.timer

[Service]
Type=oneshot
ExecStart=/usr/local/bin/full_backup.sh
```

### 数据恢复策略

#### 恢复流程

**灾难恢复计划**
```bash
#!/bin/bash
# disaster_recovery.sh

# 1. 环境检查
echo "=== Disaster Recovery Start ==="
echo "Recovery Time: $(date)"

# 2. 硬件检查
echo "=== Hardware Check ==="
lspci | grep -i raid
lsblk
df -h

# 3. 网络检查
echo "=== Network Check ==="
ping -c 3 backup-server
mount -t nfs backup-server:/backup /mnt/backup

# 4. 数据恢复
echo "=== Data Recovery ==="
# 恢复最近的完全备份
LATEST_FULL=$(ls -t /mnt/backup/full/ | head -1)
tar -xzf "/mnt/backup/full/$LATEST_FULL" -C /

# 恢复所有增量备份
for inc in $(ls -t /mnt/backup/incremental/); do
    tar -xf "/mnt/backup/incremental/$inc" -C /
done

# 5. 系统验证
echo "=== System Validation ==="
df -h
ls -la /data
service status

echo "=== Disaster Recovery Complete ==="
```

#### 恢复测试

**定期恢复测试**
```bash
#!/bin/bash
# recovery_test.sh

# 1. 创建测试环境
TEST_DIR="/tmp/recovery_test"
mkdir -p $TEST_DIR

# 2. 恢复测试数据
echo "=== Recovery Test Start ==="
LATEST_BACKUP=$(ls -t /backup/full/ | head -1)
tar -xzf "/backup/full/$LATEST_BACKUP" -C $TEST_DIR

# 3. 数据完整性验证
echo "=== Data Integrity Check ==="
cd $TEST_DIR
find . -name "*.tar.gz" -exec sha256sum -c {}.sha256 \;

# 4. 应用功能测试
echo "=== Application Test ==="
# 启动测试应用
# 执行基本功能测试

# 5. 性能测试
echo "=== Performance Test ==="
# 测试I/O性能
# 测试应用响应时间

# 6. 清理测试环境
rm -rf $TEST_DIR

echo "=== Recovery Test Complete ==="
```

### 备份监控与管理

#### 备份监控

**备份状态监控**
```bash
#!/bin/bash
# backup_monitor.sh

# 1. 检查备份文件
BACKUP_DIR="/backup"
BACKUP_AGE_LIMIT=7  # 天

echo "=== Backup Status Monitor ==="

# 检查完全备份
LATEST_FULL=$(ls -t $BACKUP_DIR/full/ 2>/dev/null | head -1)
if [ -z "$LATEST_FULL" ]; then
    echo "WARNING: No full backup found!"
else
    FULL_AGE=$(stat -c %Y $BACKUP_DIR/full/$LATEST_FULL)
    CURRENT_TIME=$(date +%s)
    AGE_DAYS=$(( (CURRENT_TIME - FULL_AGE) / 86400 ))

    if [ $AGE_DAYS -gt $BACKUP_AGE_LIMIT ]; then
        echo "WARNING: Full backup is $AGE_DAYS days old!"
    else
        echo "OK: Full backup is $AGE_DAYS days old"
    fi
fi

# 检查增量备份
LATEST_INC=$(ls -t $BACKUP_DIR/incremental/ 2>/dev/null | head -1)
if [ -z "$LATEST_INC" ]; then
    echo "WARNING: No incremental backup found!"
else
    INC_AGE=$(stat -c %Y $BACKUP_DIR/incremental/$LATEST_INC)
    CURRENT_TIME=$(date +%s)
    AGE_DAYS=$(( (CURRENT_TIME - INC_AGE) / 86400 ))

    if [ $AGE_DAYS -gt 1 ]; then
        echo "WARNING: Incremental backup is $AGE_DAYS days old!"
    else
        echo "OK: Incremental backup is $AGE_DAYS days old"
    fi
fi

# 2. 发送监控报告
echo "=== Sending Report ==="
# 发送邮件通知
echo "Backup monitoring completed at $(date)" | mail -s "Backup Status Report" admin@hpc.edu.cn
```

**备份性能监控**
```bash
#!/bin/bash
# backup_performance.sh

# 1. 监控备份性能
BACKUP_LOG="/var/log/backup.log"

echo "=== Backup Performance Monitor ===" >> $BACKUP_LOG
echo "Timestamp: $(date)" >> $BACKUP_LOG

# 监控I/O性能
iostat -x 1 10 >> $BACKUP_LOG

# 监控网络性能
iftop -t -s 60 >> $BACKUP_LOG

# 监控磁盘空间
df -h >> $BACKUP_LOG

# 2. 生成性能报告
awk '
/Backup started/ { start_time = $0 }
/Backup completed/ { end_time = $0; print start_time; print end_time }
' $BACKUP_LOG > /tmp/backup_summary.log

echo "=== Performance Report Generated ==="
```

## 本章小结

存储系统管理是HPC运维的核心技能之一。本章详细介绍了：

1. **并行文件系统**：Lustre、GPFS、BeeGFS的架构、安装、配置和优化
2. **存储性能优化**：性能指标、基准测试、缓存策略
3. **数据备份与恢复**：备份策略、恢复流程、监控管理

掌握这些知识和技术有助于：
- 选择合适的并行文件系统
- 优化存储系统性能
- 建立完善的数据保护机制
- 确保数据的高可用性和可靠性

在实际工作中，需要根据具体的存储需求、预算限制和性能要求，选择合适的存储解决方案，并进行针对性的优化和管理。