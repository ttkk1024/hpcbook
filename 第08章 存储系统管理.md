# 第8章 存储系统管理

## 8.1 并行文件系统

### Lustre文件系统

#### Lustre架构概述

**Lustre组件结构**
```
┌─────────────────────────────────────────────────────────────┐
│                    Lustre文件系统                           │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐        │
│  │  Client │  │  Client │  │  Client │  │  Client │        │
│  │  (OSC)  │  │  (OSC)  │  │  (OSC)  │  │  (OSC)  │        │
│  └─────────┘  └─────────┘  └─────────┘  └─────────┘        │
│         │            │            │            │           │
│         └────────────┼────────────┼────────────┘           │
│                      │            │                        │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │                    Metadata Layer                       │ │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐  │ │
│  │  │   MDT   │  │   MDT   │  │   MDT   │  │   MGS   │  │ │
│  │  │  (MDT)  │  │  (MDT)  │  │  (MDT)  │  │  (MGS)  │  │ │
│  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘  │ │
│  │         │            │            │            │      │ │
│  │         └────────────┼────────────┼────────────┘      │ │
│  │                      │            │                   │ │
│  └─────────────────────────────────────────────────────────┘ │
│                      │            │                        │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │                     Object Layer                        │ │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐  │ │
│  │  │   OST   │  │   OST   │  │   OST   │  │   OST   │  │ │
│  │  │  (OST)  │  │  (OST)  │  │  (OST)  │  │  (OST)  │  │ │
│  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘  │ │
│  │         │            │            │            │      │ │
│  │         └────────────┼────────────┼────────────┘      │ │
│  │                      │            │                   │ │
│  └─────────────────────────────────────────────────────────┘ │
│                      │            │                        │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │                    Storage Layer                        │ │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐  │ │
│  │  │  Disk   │  │  Disk   │  │  Disk   │  │  Disk   │  │ │
│  │  │  (HDD)  │  │  (HDD)  │  │  (SSD)  │  │  (SSD)  │  │ │
│  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘  │ │
│  └─────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────┘
```

**Lustre核心组件**
- **MGS (Management Server)**：管理系统配置和元数据
- **MDT (Metadata Target)**：存储文件系统元数据
- **OST (Object Storage Target)**：存储文件数据对象
- **Client**：客户端访问文件系统

#### Lustre安装与配置

**环境准备**
```bash
# 1. 系统要求
# 所有节点需要相同的操作系统版本
# 内核版本要求 (通常需要Lustre内核模块)

# 2. 网络配置
# 确保所有节点间网络连通性
# 推荐使用InfiniBand或高速以太网

# 3. 存储设备准备
# 为MDT和OST准备专用存储设备
fdisk -l /dev/sdb /dev/sdc /dev/sdd
```

**Lustre软件安装**
```bash
# 1. 下载Lustre源码
wget https://downloads.whamcloud.com/public/lustre/lustre-2.15.0/el9/client/SRPMS/lustre-2.15.0-1.el9.src.rpm
rpm -ivh lustre-2.15.0-1.el9.src.rpm

# 2. 编译安装
cd /root/rpmbuild/SPECS
rpmbuild -bb --with servers lustre.spec
rpmbuild -bb --without servers lustre.spec

# 3. 安装服务端
rpm -ivh /root/rpmbuild/RPMS/x86_64/lustre-*.rpm
rpm -ivh /root/rpmbuild/RPMS/x86_64/lustre-osd-ldiskfs-*.rpm

# 4. 安装客户端
rpm -ivh /root/rpmbuild/RPMS/x86_64/lustre-client-*.rpm
```

**MGS配置**
```bash
# 1. 格式化MGS设备
mkfs.lustre --fsname=hpc-lustre --mgs /dev/sdb1

# 2. 挂载MGS
mkdir -p /mnt/mgs
mount -t lustre /dev/sdb1 /mnt/mgs

# 3. 启动MGS服务
lctl format /dev/sdb1
lctl start mgs
```

**MDT配置**
```bash
# 1. 格式化MDT设备
mkfs.lustre --fsname=hpc-lustre --mgsnode=mgs-server@tcp --mdt /dev/sdc1

# 2. 挂载MDT
mkdir -p /mnt/mdt
mount -t lustre /dev/sdc1 /mnt/mdt

# 3. 启动MDT服务
lctl start mdt
```

**OST配置**
```bash
# 1. 格式化OST设备
mkfs.lustre --fsname=hpc-lustre --mgsnode=mgs-server@tcp --ost /dev/sdd1

# 2. 挂载OST
mkdir -p /mnt/ost
mount -t lustre /dev/sdd1 /mnt/ost

# 3. 启动OST服务
lctl start ost
```

**客户端配置**
```bash
# 1. 挂载Lustre文件系统
mkdir -p /mnt/lustre
mount -t lustre mgs-server@tcp:/hpc-lustre /mnt/lustre

# 2. 验证挂载
lfs df -h /mnt/lustre
# (New) 客户端持久化缓存 (PCC) 配置 [Lustre 2.12+]
lctl set_param -n llite.*.pcc_add_attach_id="1000"

# 3. 添加到fstab
echo "mgs-server@tcp:/hpc-lustre /mnt/lustre lustre defaults,_netdev 0 0" >> /etc/fstab
```

#### Lustre性能优化

**条带化配置**
```bash
# 1. 查看当前条带化设置
lfs getstripe /mnt/lustre/file

# 2. 设置条带化参数
# 条带数: 数据分布的OST数量
# 条带大小: 每个OST上的数据块大小
lfs setstripe -c 4 -S 1M /mnt/lustre/directory

# 3. 不同文件类型的条带化策略
# 大文件: 更多条带，更大条带大小
lfs setstripe -c -1 -S 4M /mnt/lustre/big_files/

# 小文件: 较少条带，较小条带大小
lfs setstripe -c 1 -S 64k /mnt/lustre/small_files/

# 元数据密集型: 优化MDT访问
lfs setstripe -i 0 -c 1 /mnt/lustre/metadata/
```

**缓存优化**
```bash
# 1. 客户端缓存配置
echo 1 > /proc/fs/lustre/osc/*/max_dirty_mb
echo 50 > /proc/fs/lustre/osc/*/max_rpcs_in_flight

# 2. 服务器端缓存配置
echo 1000 > /proc/fs/lustre/obdfilter/*/read_cache_enable
echo 1000 > /proc/fs/lustre/obdfilter/*/write_cache_enable

# 3. 网络缓存优化
echo 134217728 > /proc/sys/net/core/rmem_max
echo 134217728 > /proc/sys/net/core/wmem_max
```

**网络优化**
```bash
# 1. InfiniBand优化
echo 65520 > /sys/class/infiniband/ib0/mtu
echo 1024 > /sys/class/infiniband/ib0/ports/1/gid_attrs/roce/queue_pairs

# 2. TCP优化
echo 'net.core.rmem_max = 134217728' >> /etc/sysctl.conf
echo 'net.core.wmem_max = 134217728' >> /etc/sysctl.conf
sysctl -p
```

## 8.2 新一代对象存储 (DAOS)

DAOS (Distributed Asynchronous Object Storage) 是 Intel开源的基于NVMe和SCM (Storage Class Memory) 构建的高性能用户态对象存储系统，专为 Exascale 和 AI 场景设计。

### DAOS 架构与优势

#### 即使架构
- **零拷贝与操作系统旁路**：直接在用户态操作 NVMe 设备 (SPDK) 和 网络设备 (Libfabric)，绕过内核开销。
- **SCM + NVMe 分层**：
  - **SCM (傲腾/PMem)**：存储元数据 (Metadata) 和极小I/O，提供超低延迟。
  - **NVMe SSD**：存储大块数据 (Bulk Data)。

#### 核心优势
- **微秒级延迟**：相比传统并行文件系统降低 1-2 个数量级。
- **高吞吐**：充分发挥 PCIe Gen4/5 NVMe 带宽。
- **非阻塞事务**：支持大规模并发读写，无锁设计。

### DAOS 部署与管理

**服务端部署**
```bash
# DAOS Server Config (daos_server.yml)
engines:
  - targets: 16
    first_core: 0
    nr_xs_helpers: 4
    fabric_iface: ib0
    fabric_iface_port: 31416
    log_mask: INFO
    storage:
      - class: dcpm
        scm_mount: /mnt/daos0
        scm_list: [/dev/pmem0]
      - class: nvme
        bdev_list: ["0000:81:00.0", "0000:82:00.0"]

# 启动服务
systemctl start daos_server
```

**管理操作**
```bash
# 1. 格式化存储
dmg storage format

# 2. 创建存储池 (Pool)
dmg pool create --size=10TB --tier-ratio=6% my_pool

# 3. 创建容器 (Container) - 类似于文件系统命名空间
daos container create --type POSIX my_pool my_container

# 4. 挂载 (通过DFUSE)
dfuse --pool=my_pool --container=my_container -m /mnt/daos
```

**性能调优建议**
- **网络层**: 确保 `libfabric` 使用正确的 Provider (如 `rxm` 或 `cxi`)。
- **CPU绑定**: 在 `daos_server.yml` 中精确绑定 NVMe 所在的 NUMA 节点 CPU 核心。


## 8.2 存储性能优化

### 存储性能指标

#### 性能参数详解

**IOPS (Input/Output Operations Per Second)**
```bash
# IOPS计算公式
IOPS = 1000ms / (平均寻道时间 + 平均延迟时间 + 传输时间)

# 不同存储设备的IOPS (2025 参考值)
SATA SSD:          50,000 - 100,000 IOPS
NVMe Gen4 SSD:     400,000 - 1,000,000+ IOPS
NVMe Gen5 SSD:     2,000,000+ IOPS
```

**吞吐量 (Throughput)**
```bash
# 吞吐量计算公式
吞吐量 = IOPS × 平均I/O大小

# 不同存储设备的吞吐量 (2025 参考值)
HDD (7.2k RPM):     150 - 250 MB/s
SATA SSD:           500 - 560 MB/s
NVMe Gen4 SSD:      5,000 - 7,500 MB/s
NVMe Gen5 SSD:      12,000 - 14,000+ MB/s
```

**延迟 (Latency)**
```bash
# 延迟组成
总延迟 = 控制器延迟 + 队列延迟 + 传输延迟 + 介质处理延迟

# 不同存储设备的延迟
NVMe NAND SSD:      ~20 - 100 μs
NVMe Optane/SCM:    ~5 - 10 μs
```

### 存储性能测试

#### FIO基准测试

**FIO测试配置 (NVMe优化)**
```bash
# 1. 顺序读取测试 (io_uring 引擎)
fio --name=seq_read --ioengine=io_uring --rw=read --bs=1m --size=10G --numjobs=1 --runtime=60 --direct=1 --iodepth=128 --filename=/dev/nvme0n1

# 2. 顺序写入测试
fio --name=seq_write --ioengine=libaio --rw=write --bs=1m --size=1G --numjobs=4 --runtime=60 --time_based --filename=/dev/sdb --direct=1 --iodepth=64

# 3. 随机读取测试
fio --name=rand_read --ioengine=libaio --rw=randread --bs=4k --size=1G --numjobs=16 --runtime=60 --time_based --filename=/dev/sdb --direct=1 --iodepth=256

# 4. 随机写入测试
fio --name=rand_write --ioengine=libaio --rw=randwrite --bs=4k --size=1G --numjobs=16 --runtime=60 --time_based --filename=/dev/sdb --direct=1 --iodepth=256

# 5. 混合读写测试
fio --name=mixed --ioengine=libaio --rw=randrw --rwmixread=70 --bs=4k --size=1G --numjobs=8 --runtime=60 --time_based --filename=/dev/sdb --direct=1 --iodepth=128
```

**FIO结果分析**
```bash
# 结果解读
# read: IOPS=12345, BW=48.2MB/s (50.5MB/s)(2964MB/61506msec)
# write: IOPS=6789, BW=26.5MB/s (27.8MB/s)(1632MB/61506msec)
# lat (usec): min=42, max=15680, avg=123.45, stdev=45.67

# 性能评估
# IOPS: 每秒I/O操作数，反映随机访问性能
# BW: 带宽，反映顺序访问性能
# lat: 延迟，反映响应速度
# stdev: 标准差，反映延迟稳定性
```

#### 并行文件系统测试

**Lustre性能测试**
```bash
# 1. IOR测试
ior -a POSIX -b 1g -t 1m -o /mnt/lustre/testfile -w -r

# 2. MDTEST测试
mdtest -d /mnt/lustre/testdir -i 10 -I 1000 -n 100 -u

# 3. 自定义测试脚本
#!/bin/bash
# lustre-performance-test.sh

TEST_DIR="/mnt/lustre/perf_test"
FILE_SIZE="1G"
BLOCK_SIZE="1M"

# 创建测试目录
mkdir -p $TEST_DIR

# 顺序写入测试
echo "Sequential Write Test:"
time dd if=/dev/zero of=$TEST_DIR/seq_write_test bs=$BLOCK_SIZE count=$((1024*1024/$BLOCK_SIZE)) oflag=direct

# 顺序读取测试
echo "Sequential Read Test:"
time dd if=$TEST_DIR/seq_write_test of=/dev/null bs=$BLOCK_SIZE iflag=direct

# 随机I/O测试
echo "Random I/O Test:"
fio --name=rand_test --ioengine=libaio --rw=randrw --bs=4k --size=$FILE_SIZE --numjobs=16 --runtime=60 --time_based --filename=$TEST_DIR/rand_test --direct=1

# 清理测试文件
rm -f $TEST_DIR/*
```

**GPFS性能测试**
```bash
# 1. GPFS特定工具
mmfsdump -f /tmp/gpfs_dump
mmfsstat -f /tmp/gpfs_stat

# 2. 标准工具测试
# 使用IOR、MDTEST等工具
ior -a POSIX -b 1g -t 1m -o /mnt/gpfs/testfile -w -r

# 3. GPFS性能监控
mmfsadm dumpstats
mmfsadm dumpconfig
```

### 存储缓存策略

#### 缓存层次结构

**存储缓存架构**
```
┌─────────────────────────────────────┐
│           应用程序缓存               │
│  (应用程序内部缓存机制)              │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│            操作系统缓存              │
│  (Page Cache, Buffer Cache)         │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│            存储控制器缓存            │
│  (RAID卡缓存, HBA缓存)              │
└─────────────────────────────────────┘
                       │
┌─────────────────────────────────────┐
│            存储设备缓存              │
│  (SSD缓存, HDD缓存)                 │
└─────────────────────────────────────┘
```

#### 缓存配置优化

**操作系统缓存优化**
```bash
# 1. Page Cache配置
echo 15 > /proc/sys/vm/dirty_ratio        # 脏页比例上限
echo 5 > /proc/sys/vm/dirty_background_ratio  # 后台写入比例
echo 300 > /proc/sys/vm/dirty_expire_centisecs  # 脏页过期时间
echo 500 > /proc/sys/vm/dirty_writeback_centisecs  # 写回间隔

# 2. 文件系统缓存
# XFS配置
mount -o noatime,logbsize=256k,largeio /dev/sdb1 /data

# ext4配置
mount -o noatime,data=writeback,barrier=0 /dev/sdb1 /data

# 3. 内存管理
echo 1 > /proc/sys/vm/overcommit_memory
echo 80 > /proc/sys/vm/swappiness
```

**RAID控制器缓存优化**
```bash
# 1. 查看RAID控制器信息
megacli -AdpAllInfo -aALL
storcli /c0 show

# 2. 配置写缓存策略
megacli -LDSetProp -WB -Lall -aALL
storcli /c0/e0/s0 set wrcache=WB

# 3. 配置读缓存策略
megacli -LDSetProp -RA -Lall -aALL
storcli /c0/e0/s0 set rdcache=RA
```

**SSD缓存配置**
```bash
# 1. 启用TRIM支持
echo 'ACTION=="add|change", KERNEL=="sd*[!0-9]", ATTR{queue/discard_granularity}=="512", ATTR{queue/discard_max_bytes}=="4294967295", RUN+="/sbin/blockdev --setra 512 /dev/$name"' > /etc/udev/rules.d/80-ssd.rules

# 2. 优化I/O调度器
echo mq-deadline > /sys/block/sdb/queue/scheduler

# 3. 配置I/O队列深度
echo 1024 > /sys/block/sdb/queue/nr_requests
echo 128 > /sys/block/sdb/queue/nr_requests
```

#### 缓存策略选择

**不同工作负载的缓存策略**
```bash
# 1. 读密集型工作负载
# 增大读缓存，启用预读
echo 80 > /proc/sys/vm/dirty_ratio
echo 20 > /proc/sys/vm/dirty_background_ratio
blockdev --setra 1024 /dev/sdb

# 2. 写密集型工作负载
# 增大写缓存，优化写入策略
echo 10 > /proc/sys/vm/dirty_ratio
echo 5 > /proc/sys/vm/dirty_background_ratio
echo 3000 > /proc/sys/vm/dirty_expire_centisecs

# 3. 混合工作负载
# 平衡读写缓存
echo 40 > /proc/sys/vm/dirty_ratio
echo 10 > /proc/sys/vm/dirty_background_ratio
```

## 8.3 数据备份与恢复

### 备份策略设计

#### 备份类型

**完全备份 (Full Backup)**
```bash
# 1. 完全备份脚本
#!/bin/bash
# full_backup.sh

BACKUP_DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backup/full/$BACKUP_DATE"
SOURCE_DIR="/data"
EXCLUDE_FILE="/etc/backup.exclude"

# 创建备份目录
mkdir -p $BACKUP_DIR

# 执行完全备份
tar --exclude-from=$EXCLUDE_FILE -czf "$BACKUP_DIR/full_backup_$BACKUP_DATE.tar.gz" $SOURCE_DIR

# 计算校验和
sha256sum "$BACKUP_DIR/full_backup_$BACKUP_DATE.tar.gz" > "$BACKUP_DIR/checksum.sha256"

# 压缩备份
gzip "$BACKUP_DIR/full_backup_$BACKUP_DATE.tar.gz"

echo "Full backup completed: $BACKUP_DIR"
```

**增量备份 (Incremental Backup)**
```bash
# 1. 增量备份脚本
#!/bin/bash
# incremental_backup.sh

BACKUP_DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backup/incremental/$BACKUP_DATE"
SOURCE_DIR="/data"
LAST_BACKUP="/backup/incremental/last_backup.timestamp"

# 创建备份目录
mkdir -p $BACKUP_DIR

# 执行增量备份
find $SOURCE_DIR -newer $LAST_BACKUP -type f -exec tar -rf "$BACKUP_DIR/incremental_$BACKUP_DATE.tar" {} \;

# 更新时间戳
touch $LAST_BACKUP

echo "Incremental backup completed: $BACKUP_DIR"
```

**差异备份 (Differential Backup)**
```bash
# 1. 差异备份脚本
#!/bin/bash
# differential_backup.sh

BACKUP_DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backup/differential/$BACKUP_DATE"
SOURCE_DIR="/data"
BASE_BACKUP="/backup/full/base_backup.timestamp"

# 创建备份目录
mkdir -p $BACKUP_DIR

# 执行差异备份
find $SOURCE_DIR -newer $BASE_BACKUP -type f -exec tar -rf "$BACKUP_DIR/differential_$BACKUP_DATE.tar" {} \;

echo "Differential backup completed: $BACKUP_DIR"
```

#### 备份调度

**定时备份配置**
```bash
# 1. crontab配置
# 每周日2点执行完全备份
0 2 * * 0 /usr/local/bin/full_backup.sh

# 每天2点执行增量备份
0 2 * * * /usr/local/bin/incremental_backup.sh

# 每周六2点执行差异备份
0 2 * * 6 /usr/local/bin/differential_backup.sh

# 2. systemd定时器
# /etc/systemd/system/backup-full.timer
[Unit]
Description=Full Backup Timer
Requires=backup-full.service

[Timer]
OnCalendar=weekly
Persistent=true

[Install]
WantedBy=timers.target

# /etc/systemd/system/backup-full.service
[Unit]
Description=Full Backup Service
Requires=backup-full.timer

[Service]
Type=oneshot
ExecStart=/usr/local/bin/full_backup.sh
```

### 数据恢复策略

#### 恢复流程

**灾难恢复计划**
```bash
#!/bin/bash
# disaster_recovery.sh

# 1. 环境检查
echo "=== Disaster Recovery Start ==="
echo "Recovery Time: $(date)"

# 2. 硬件检查
echo "=== Hardware Check ==="
lspci | grep -i raid
lsblk
df -h

# 3. 网络检查
echo "=== Network Check ==="
ping -c 3 backup-server
mount -t nfs backup-server:/backup /mnt/backup

# 4. 数据恢复
echo "=== Data Recovery ==="
# 恢复最近的完全备份
LATEST_FULL=$(ls -t /mnt/backup/full/ | head -1)
tar -xzf "/mnt/backup/full/$LATEST_FULL" -C /

# 恢复所有增量备份
for inc in $(ls -t /mnt/backup/incremental/); do
    tar -xf "/mnt/backup/incremental/$inc" -C /
done

# 5. 系统验证
echo "=== System Validation ==="
df -h
ls -la /data
service status

echo "=== Disaster Recovery Complete ==="
```

#### 恢复测试

**定期恢复测试**
```bash
#!/bin/bash
# recovery_test.sh

# 1. 创建测试环境
TEST_DIR="/tmp/recovery_test"
mkdir -p $TEST_DIR

# 2. 恢复测试数据
echo "=== Recovery Test Start ==="
LATEST_BACKUP=$(ls -t /backup/full/ | head -1)
tar -xzf "/backup/full/$LATEST_BACKUP" -C $TEST_DIR

# 3. 数据完整性验证
echo "=== Data Integrity Check ==="
cd $TEST_DIR
find . -name "*.tar.gz" -exec sha256sum -c {}.sha256 \;

# 4. 应用功能测试
echo "=== Application Test ==="
# 启动测试应用
# 执行基本功能测试

# 5. 性能测试
echo "=== Performance Test ==="
# 测试I/O性能
# 测试应用响应时间

# 6. 清理测试环境
rm -rf $TEST_DIR

echo "=== Recovery Test Complete ==="
```

### 备份监控与管理

#### 备份监控

**备份状态监控**
```bash
#!/bin/bash
# backup_monitor.sh

# 1. 检查备份文件
BACKUP_DIR="/backup"
BACKUP_AGE_LIMIT=7  # 天

echo "=== Backup Status Monitor ==="

# 检查完全备份
LATEST_FULL=$(ls -t $BACKUP_DIR/full/ 2>/dev/null | head -1)
if [ -z "$LATEST_FULL" ]; then
    echo "WARNING: No full backup found!"
else
    FULL_AGE=$(stat -c %Y $BACKUP_DIR/full/$LATEST_FULL)
    CURRENT_TIME=$(date +%s)
    AGE_DAYS=$(( (CURRENT_TIME - FULL_AGE) / 86400 ))

    if [ $AGE_DAYS -gt $BACKUP_AGE_LIMIT ]; then
        echo "WARNING: Full backup is $AGE_DAYS days old!"
    else
        echo "OK: Full backup is $AGE_DAYS days old"
    fi
fi

# 检查增量备份
LATEST_INC=$(ls -t $BACKUP_DIR/incremental/ 2>/dev/null | head -1)
if [ -z "$LATEST_INC" ]; then
    echo "WARNING: No incremental backup found!"
else
    INC_AGE=$(stat -c %Y $BACKUP_DIR/incremental/$LATEST_INC)
    CURRENT_TIME=$(date +%s)
    AGE_DAYS=$(( (CURRENT_TIME - INC_AGE) / 86400 ))

    if [ $AGE_DAYS -gt 1 ]; then
        echo "WARNING: Incremental backup is $AGE_DAYS days old!"
    else
        echo "OK: Incremental backup is $AGE_DAYS days old"
    fi
fi

# 2. 发送监控报告
echo "=== Sending Report ==="
# 发送邮件通知
echo "Backup monitoring completed at $(date)" | mail -s "Backup Status Report" admin@hpc.edu.cn
```

**备份性能监控**
```bash
#!/bin/bash
# backup_performance.sh

# 1. 监控备份性能
BACKUP_LOG="/var/log/backup.log"

echo "=== Backup Performance Monitor ===" >> $BACKUP_LOG
echo "Timestamp: $(date)" >> $BACKUP_LOG

# 监控I/O性能
iostat -x 1 10 >> $BACKUP_LOG

# 监控网络性能
iftop -t -s 60 >> $BACKUP_LOG

# 监控磁盘空间
df -h >> $BACKUP_LOG

# 2. 生成性能报告
awk '
/Backup started/ { start_time = $0 }
/Backup completed/ { end_time = $0; print start_time; print end_time }
' $BACKUP_LOG > /tmp/backup_summary.log

echo "=== Performance Report Generated ==="
```

## 本章小结

存储系统管理是HPC运维的核心技能之一。本章详细介绍了：

1. **并行文件系统**：Lustre、GPFS、BeeGFS的架构、安装、配置和优化
2. **存储性能优化**：性能指标、基准测试、缓存策略
3. **数据备份与恢复**：备份策略、恢复流程、监控管理

掌握这些知识和技术有助于：
- 选择合适的并行文件系统
- 优化存储系统性能
- 建立完善的数据保护机制
- 确保数据的高可用性和可靠性

在实际工作中，需要根据具体的存储需求、预算限制和性能要求，选择合适的存储解决方案，并进行针对性的优化和管理。